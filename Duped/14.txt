
14
Improving Accuracy
Once upon a time, not so long ago, it was a reliable and well-established
empirical fact that humans were invariably poor lie detectors. Accuracy rates
were statistically better than chance, but unimpressive on the face. Further,
these poor accuracy findings had been amazingly persistent. One might wish
it otherwise, but beliefs to the contrary were, for a long time, simply counterfactual.
A few years ago, however, things began to change. The once solid accuracy ceiling began to crumble. My lab started to see accuracy rates in the mid-
sixties. Those findings replicated. Then we obtained and replicated findings in
the seventies. And it was not just our lab. Others, too, were reporting levels of
accuracy that departed substantially from old and reliable slightly-better-than-
chance conclusions. One of my most recent series of experiments reported
and replicated accuracy over 90%.1
This chapter tells the story of how the once-upon-a-time fact of poor accuracy (summarized in chapters 3 and 13) gave way to much higher accuracy results. As an introduction to my new and improved approach to lie detection,
I begin with an odd but true story of suspected deception at a deception research conference.
Hee Sun Park and I were attending a boutique legal-criminal psychology
conference in the Netherlands. At one session a leading deception researcher
spoke. During his talk, the speaker said that he had never said such and such
a thing.2 The claim he disavowed was one that I closely associated with him
and his work. I was confident that I did not mishear his denial. He made the
claim more than once. Hee Sun heard him say it too. I was confused because
I was quite confident in my understanding of his work. I had recently read
works authored by him that made the claim that he was now saying he never
made. I was worried because I credit that claim to him in my work. I did so

254

CHAPTER FOURTEEN

earlier in this book. I routinely cite him (as do others) as a leading advocate of
that position. Could I have been mis-citing him? That would be awful.
I felt like one of the subjects in the Asch conformity experiment.3 I knew
which line actually matched the standard, but here was someone saying something that seemed objectively false. I wondered, Did I mishear? No, I didn’t
think so. Had I previously misread? Maybe, but I didn’t think so. How very
odd. What was going on?
When I got back to my hotel room, I fired up my laptop. I pulled up an article authored by this guy and quickly found the quote I thought I had remembered reading. I was not mistaken. There was the claim in print, as clear as
day. I reread it carefully to be sure, and made sure I was taking it in context.
The earlier denial was objectively false, just as I had thought when I heard it.
Had we the audience attending the talk just been lied to? Why would some
one lie about something so easily fact-checked? Stranger still, this guy continues to make the same original claim in print. His disavowal apparently didn’t
last long. Very odd indeed.
Before moving on, here is a little fantasy of mine, an imagined interaction
existing only in my head until now. I ask this guy, “Did I hear you correctly say
blah-blah-blah?” I see if he commits to saying it. If he does, I pull out highlighted text and confront him with his own quote and ask for an explanation. I
never did this because it would be hugely face-threatening. Even when I know I
am being lied to, I typically don’t call the liars out unless I have a good reason.
Why be a jerk? But if that interaction had occurred, it likely would have been
informative in ways very relevant to the content of this chapter.
Simply knowing that a statement is false is not sufficient to conclude that
the statement is a lie. Maybe it was an honest mistake, a misunderstanding,
a delusion, or just plain bullshit.4 Later in the chapter, I’ll make the case for a
four-step inferential process in evidence-based lie detection that goes well beyond just knowing ground truth. Lie or not, however, the statement certainly
would have misled anyone who accepted its literal meaning. But ascertaining
whether it is a lie or not, or deceptive or not, isn’t where I am going with all
this. The point I want to make here with this strange story is how very different
this experience was from the task faced by research subjects in the typical deception detection lab experiment yielding slightly-better-than-chance accuracy.
I’m listening to an academic talk. The thought that a speaker might be l ying
usually does not enter my consciousness in professional research presentations. There is no researcher there asking me to make a truth–lie assessment.
What kicks me out of my truth-default is that something is said that contradicts my prior knowledge. Prior knowledge is of little relevance in most lab

IMPROVING ACCURACY

255

experiments. So, what do I then do once I am wary? I after-the-fact fact-check
the suspicious statement. There is no need for me to judge the statement right
away. In contrast, there is no opportunity to fact-check suspected lies in the
lab, and truth–lie judgments must be made on the spot. Further, cues and demeanor played no role in my story. In the lab, that’s all subjects have to go on.
I did not seek to question the guy, but I could have had I been so inclined. I
had the opportunity. I don’t know how that would have played out, but that
sort of thing isn’t even an option in most lab experiments.
My odd-occurrence-at-the-psychology-conference story sets up my next story,
which occurred more than a decade earlier. Back in chapter 12 when introducing the veracity effect and base-rates, I mentioned that the chapter could have
been titled “How Hee Sun Park Changed Deception Research, Part One.” Well,
here is how Hee Sun Park changed deception research, Part Two. If I had to
pick one, I’d say this is even more of a game changer than her veracity effect
or base-rate ideas. This is what first put me on the path to eventually breaking the accuracy ceiling.
TDT STUDY THIRTY-S EVEN: HOW PEOPLE REALLY DETECT LIES
The idea for TDT study thirty-seven occurred to Hee Sun Park5 about the same
time that she came up with the Park–Levine Probability Model. She was a doctoral student at the time, maybe halfway through her program of study. One
day she told me her latest study idea. She knew it was a little outside the box.
She anticipated my usual initial skepticism. Unbeknownst to me, she had already collected pretest data demonstrating her point. That way she could skip
the extensive explanation and persuasion and shut me up with some hard evidence. That has been known to work on me.
She made the point to me that I just made with the conference story. She
told me, lab experiments have people watch others and make real-time judgments based only on at-the-time observed behaviors (i.e., cues and demeanor).
This, she said, bears little resemblance to how people really detect lies outside
the lab in real life. She said she could prove it.
For initial evidence, she gave one hundred students at UCSB a short, open-
ended survey. She asked them to describe a recent time that they had discovered being lied to. How had they found out? Try this yourself. When was the
last time someone lied to you? How did you find out that the lie was a lie? Ask
someone else. See what they say.
Lots of prior research (covered in chapter 2) had asked subjects how they
could tell when someone was lying to them. Answers usually involved nonverbal cues like gaze aversion. Hee Sun asked the question just a little differ-

256

CHAPTER FOURTEEN

ently than the prior research. Instead of asking hypothetically or in terms of
generalities, she ask for a specific recollection of an instance where the subject had detected a real lie. This subtle difference in the way the question was
asked yielded huge differences in the sorts of answers obtained.
Reading the accounts in Hee Sun’s pretest data, two things immediately
popped out. There was no need for statistics or formal analysis. First, many if
not most of the recalled lies were discovered well after the fact. Second, at-the-
time behavioral displays (cues and demeanor) had little to do with it. Instead,
most of the discovered lies were revealed either through comparing what was
said to some external evidence or because the liar later told the truth, revealing the lie for what it was. The recalled lie detection stories were much more
similar to my odd convention story than to the task presented in deception
detection experiments.
Hee Sun explained to me that it was no wonder people perform poorly at
lie detection experiments: that’s not how people really detect lies. This became
the title of our eventual article “How People Really Detect Lies” (or the “How
Lies” study, for short). Typical deception detection experiments are the creation of cue theorists and make good sense from within that perspective. Researchers have been imposing their view of lie detection on subjects without
ever considering how people typically discover lies outside the lab.
Hee Sun’s pretest was effective in persuading me that her idea was worth
pursuing. But her initial data collection was more of a proof-of-concept thing
than the stuff of an A-level peer-reviewed journal article. Graduate students
can use all the first-authored articles in A-level peer-reviewed journals they
can get. It was one heck of an idea that had the potential to turn lie detection
research on its head. So I enlisted Steve McCornack and Kelly Morrison and
my graduate student at the time, Merissa Ferrara, and together with Hee Sun,
we set out to redo Hee Sun’s little study in a way that would get her idea into
a top-flight journal.
We used Hee Sun’s pretest data to create a coding system that we could then
apply to new data. We worked to refine Hee Sun’s survey questions. Then we
administered the refined survey to 202 MSU students. We trained two coders and gave them the data to sort. The data came back just as Hee Sun had
predicted.
There was much more nuance to the coding and the results than is needed
here. The short version can be summarized in four bullet points.
• At the time verbal and nonverbal behaviors were listed as the sole
basis for lie detection only 2% of the time.

IMPROVING ACCURACY

257

• Only 15% of the lies were detected immediately. A majority of lies
were uncovered a week or more later.
• Over 80% of detected lies involved some sort of external evidence
(information from informants, physical evidence, or prior factual
knowledge).
• In more than one-third of the detected lies, the liar confessed to telling the lie. In about 14% of instances, a confession was the only
thing that revealed the lie.
These results were absolutely stunning. Literally hundreds of prior experiments were devoted exclusively to a lie detection method that is used only 2%
of the time. Sweeping claims about poor lie detection seem to be based on a
very thin slice of the lie detection pie.
Our findings have now been replicated twice. Jaume Masip recently replicated the How Lies study in Spain with police officers and civilians.6 As with
the research reviewed in chapter 2, they found that when asked about beliefs,
both officers and civilians in Spain believed in the utility of behavior cues in
spotting lies. But when asked to recall a successfully detected lie, evidence
ruled the day, just as it had in our results.
The second replication is an unpublished study Hee Sun did with her student Austin Lee using data collected in Korea. Confessions were less common
in the Korean data, but otherwise results were pretty similar. Fewer than 6%
of the lies were detected on the sole basis of cues or demeanor. More than half
of the lies were detected using only evidence. Again, the vast majority of lies
were detected after the fact.
Hee Sun’s idea and TDT study thirty-seven form the How People Really Detect Lies module in TDT.
How People Really Detect Lies—Outside the deception lab, in everyday
life, most lies are detected after the fact based on either confessions or
the discovery of some evidence showing that what was said was false.
Few lies are detected in real time based only on the passive observation
of sender nonverbal behavior.
Study thirty-seven as well as the Spanish and Korean replications also provide evidence for TDT propositions ten and twelve. There will be more evidence for proposition twelve as this chapter progresses.

258

CHAPTER FOURTEEN

TDT Proposition Ten. Triggers and deception judgments need not occur at the time of the deception. Many deceptions are suspected and detected well after the fact.
TDT Proposition Twelve. Deception is most accurately detected through
either (a) subsequent confession by the deceiver or (b) comparison of the
contextualized communication content to some external evidence or preexisting knowledge.
CONTENT VS. CUES
If cues are not very useful in lie detection, then perhaps there are better
alternatives. Hee Sun’s idea about how people really detect lies got me started
down a line of thinking that would eventually lead me to break out of slightly-
better-than-chance accuracy and produce some much improved accuracy findings. Central to the evolution in my thinking is a distinction between cues and
content. By communication content I mean the substance of what is said. Communication content is contrasted with cues and demeanor, which involve how
something is said and how a person is behaving when they say something. My
key insight is that if you want to know if someone is honest or not, your best
bet is to listen carefully and critically to what is said (communication content)
and don’t get distracted or unduly influenced by cues and demeanor. As shown
in the last chapter, reliance on cues and demeanor pushes accuracy down toward chance. So, don’t go there. On this, I stake out a position almost opposite to Ekman’s. Ekman argues that “one reason why most people do so poorly
in judging deceit is that they rely too much upon what people say and ignore
the discrepancies between the expressive behaviors and what is said.”7 I say
people rely too little on what is said and rely too much on expressive behaviors.
UNPACKING COMMUNICATION CONTENT:
CORRESPONDENCE AND COHERENCE INFORMATION
Pete Blair, Torsten Reimer, and I wrote a paper focusing on two classic ways
communication content might be used to assess truth.8 We call these correspondence and coherence, and together they form a TDT module by that name.
Correspondence refers to the extent to which what is said (communication
content) matches (i.e., corresponds with) some known facts and evidence (i.e.,
known ground truth). My previous conference example where I fact-checked
a claim is an example of using correspondence information. Statements that
check out and align with the facts merit belief and acceptance. Statements that
fail to correspond with what is known are considered to be objectively false.

IMPROVING ACCURACY

259

Correspondence is basically an evidence-based lie detection. It is much like
empirical science. True statements are data consistent. That which does not
correspond with the data is deemed false and rejected.
Coherence refers to logical consistency. Another way we could use communication content to detect lies is to listen for contradictions and inconsistencies.
Logically, we know that both A and not-A cannot both be true at the same time.
The idea is that honest messages do not contradict other honest messages.
When people change their stories or make logically inconsistent claims, perhaps they are lying, or at the very least they should not be trusted or believed.
Two European psychologists, Pär Anders Granhag and Leif Strömwall, did a
series of intriguing experiments a number of years ago exploring logical consistency as an indicator of deception.9 They found little evidence that liars are less
consistent in their stories than honest people. There are even some cases where
liars are actually more consistent. The clear conclusion from their research is
that coherence information is not especially useful as a lie detection strategy.
My opinion is that coherence is best used along the lines of what Ekman
calls a “hot spot.” If a person contradicts an earlier statement, make note, but
don’t jump to a “gotcha, you’re lying” conclusion. Honest people are often not
completely coherent in what they say. Everyday talk just doesn’t meet the standards of sound logic.
Correspondence and Coherence—Correspondence and coherence are
two types of consistency information that may be used in deception detection. Correspondence has to do with comparing what is said to known
facts and evidence. It is fact-checking. Coherence involves the logical consistency of communication. Generally speaking, correspondence is more
useful than coherence in deception detection.
EVIDENCE-B ASED LIE DETECTION
In my opinion, an evidence-based lie approach is the most preferred method of
lie detection. Let’s be clear, though. It’s not perfect. Sometimes evidence gets
tainted or biased. Sometimes evidence is just incomplete. Sometimes alternative explanations exist but aren’t apparent. Evidence isn’t perfect and inferences based on evidence aren’t either. But if statements can be fact-checked,
fact-checking is the best bet for discovering the truth and assessing honesty.
Further, in real-life lie detection, the concern usually isn’t merely the recognition of a deceptive attempt. The goal is usually knowing the truth of the matter. Evidence is our best bet for getting to the truth behind deception.
I have encountered much resistance to my view that evidence-based lie de-

260

CHAPTER FOURTEEN

tection is preferable to cue-based approaches, especially from psychologists.
Some of this is probably due to stubborn adherence to entrenched ways of
thinking along the lines of cue theories. I also think sometimes that evidence-
based lie detection is dismissed as obvious and therefore uninteresting. There
is no fascinating counterintuitive psychology to it. Evidence-based lie detection is the stuff of Sherlock Holmes, not Sigmund Freud. I think some of the
resistance also reflects a wish for a lazy shortcut. Doing the detective work to
properly fact-check is effortful and time consuming. It would be nice to just
read body language or something. I have heard it said that we need nonverbal
lie detection because there are times when evidence is unavailable. This is
partly true. Relevant evidence is not always obtainable. Even then, however, I
still think communication content is more useful than cues. And just because
there are times when evidence cannot be used does not justify failing to use
evidence when it could be used.
Earlier I said that knowing a statement is false does not mean it is deceptive. Maybe the false statement was an honest mistake. Maybe it was delusional. Maybe it’s just bullshit. People often don’t say what they mean but are
still understood. How can we address the false-does-not-equal-deception-and-
vice-versa problem with an evidence-based approach to lie detection? I propose the following four conditions as the inferential process in an evidence-
based approach to lie detection.
1. Does the message in question lack correspondence with the evidence? That is, does the message run afoul of the ground truth as
suggested by the evidence?
2. Is the message likely to mislead reasonable members of the language community?
3. Can it be reasonably inferred that the sender knows that the message is false, knows the message is misleading, or at least should
know that it is false and misleading?
4. Is the truth problematic in some way for the sender, and is the misleading nature of the message functional for the sender? That is,
does the message, by virtue of its misleading nature, help to negate
a problematic truth for the sender and further the sender’s communicative goals?
The first question deals with the truth or falsity of the message. That is the
first step in ascertaining ground truth. Next, we want to know if what was said
is misleading to listeners. This rules out transparently false statements that

IMPROVING ACCURACY

261

fool no one. Third, we consider the likelihood that the sender actually knew
that the message was false and/or was misleading. Here we try to rule out an
honest mistake, delusion, or ignorance. Finally, we consider motive. We know
from chapter 10 that people deceive for a reason. We apply the evidence and
critical thinking to all four steps. If the evidence is sound, and if the answer
to all four questions is “yes,” then it is a good bet the message was deception.
Applying these criteria to the odd statement at the conference show why I
am reluctant to infer deception. The statement was clearly false. It would certainly fool many if not most listeners. The speaker should know what is in his
own published work. But I don’t see anything to be gained by the statement.
The false statement does not seem to solve a problem for the speaker. So, the
statement was false, it was misleading, but its status as deceptive is still murky.
PLAUSIBILITY, SITUATIONAL FAMILIARITY,
AND CONTENT IN CONTEXT
Even in situations where sound evidence is lacking and ground truth is unknown, attention to content allows for the assessment of plausibility. That is,
even when we don’t have direct evidence upon which to objectively and confidently ascertain ground truth, in most communication situations we have
some knowledge about what is typical and usual. We can also project motive.
Having an understanding of the context in which the communication is taking place lets us meaningfully evaluate the content of what is said in light of
sender motivation and typicality.
TDT EXPERIMENT THIRTY-E IGHT:
A FIRST PEEK AT SITUATIONAL FAMILIARITY
This is an experiment I did with Steve McCornack in the early 1990s before I
ever met Hee Sun Park or Pete Blair.10 I was a visiting instructor at Indiana University, and Steve was a young assistant professor at MSU at the time. It took
us ten years to publish the results, but that is a story for elsewhere.11 It took
me twenty years to see the real value of the findings. It is only when looking
back at the results of experiment thirty-eight knowing what I know now that I
realize what I should have seen all along. Having some context matters a lot.
Experiment thirty-eight had 136 judges view videotaped interviews of four
senders discussing their opinions on currently controversial issues on campus.
Some of the issues were local (e.g., current issues at MSU, where the judges
were also students), and some issues were hot topics at a different, out-of-state
university (Indiana U.). The senders lied on some issues, giving opinions opposite to their own while providing honest opinions on other issues. Each sender

262

CHAPTER FOURTEEN

was asked a follow-up probing question about each expressed opinion, but the
probing questions (but not the answers) were edited out of half the messages.
In sum, judges rated each of several taped interviews as honest or deceptive
where the interviewees expressed true or false opinions about issues at a familiar or an unfamiliar campus, and where probing follow-up questions were
heard or were not heard.
Hearing the probing questions or not made little difference in accuracy
(probes 57%, no-probes 59%), and overall accuracy was 58%. Subjects, however, were much better ascertaining the true opinions of senders on issues at
their own school (69%) than on the other-school issues (48%). It seems having some knowledge about what the senders were talking about provided a
big boost to accuracy.12 Our situational familiarity findings have subsequently
been independently replicated in a series of studies by Marc-Andre Reinhard
and Sigi Sporer.13
At the time, Steve and I were much more interested in probing than topic
familiarity. We had included familiarity as a proxy for cognitive processing,
not as an important factor in its own right. But the 69% accuracy for familiar
topics stayed in the back of our heads. It was not until I met Pete Blair and we
did our content-in-context experiments (TDT experiments thirty-nine through
forty-four) that I realized the full implications.
Now is a good time to introduce Pete Blair. Pete and I met at Michigan
State University when he was a PhD student studying criminal justice. Before graduate school Pete had worked for John Reid and Associates14 as both
a trainer and an investigator. He was interested in deception detection, and
when he learned about me, he made an appointment and we met. Pete ended
up taking statistics and persuasion classes in communication from my mentor Frank Boster. He worked on some of Judee Burgoon’s grants. He became
friends with many of the communication graduate students at the time. And
we got to know each other.
Pete brought a real-world, applied flavor to my research, which previously
had been decidedly more academically oriented. Pete thought that Hee Sun’s
How Lies study (TDT experiment thirty-seven) was awesome. He was also a big
fan of my cheating experiments. We began to collaborate on research, mostly
after he graduated and became a professor. Over time he has become one of my
most valued coauthors. These days, Pete researches active-shooter situations.
Our first project together was the projected-motive experiments described
in chapter 10 (see TDT experiments ten, eleven, and twelve). Shortly thereafter, we began working on two ideas that we called content in context and di-

IMPROVING ACCURACY

263

agnostic utility questioning, each of which became modules in TDT. Content
in context is covered first.
Content in Context—Understanding communication requires listening
to what is said and taking that in context. Knowing about the context in
which the communication occurs can help detect lies.
Content in context is nearly synonymous with situational familiarity. They
are different labels for very similar ideas. These days, I prefer “content in context” because I want to emphasize the role of communication content and taking what is said in context. Of course, taking what is said in context requires
being familiar with a situation.
TDT EXPERIMENTS THIRTY-N INE THROUGH FORTY-F OUR:
CONTENT IN CONTEXT
We are all familiar with the idea of taking a statement out of context. When
we only hear words and lack an understanding of the context in which the
words were said, some of the meaning gets lost or misunderstood. Taking
statements in context minimizes lost meanings and reduces misinterpretation. The basic idea here is that the more statements can be understood in
context, the more potentially valuable the communication content becomes in
making sense of something that is said. On the flip side, stripping context reduces the utility of content.
Author Edward T. Hall popularized the idea of high-and low-context communication.15 In low-context communication, statements have a literal meaning and can be understood by anyone who knows the language. If you know
what the words mean, you can understand what is said. If I say, “I am going
to the gym in a half hour to get some exercise,” you know what I mean. High-
context communication, in contrast, requires understanding of the context,
human relationships, situation, culture, and the like in which the communication is embedded. In other words, high-context communication requires
background knowledge to understand what is meant. Hearing or reading just
the words is not enough. If Hee Sun yells at me, “Tim, Ami is barking at me,”
you can probably guess that Ami is a dog. Knowing that this means that she
wants me to take Ami out requires more context. When I teach this idea, I use
the example of the TV show South Park. Getting the humor in South Park requires knowing current American pop culture. The humor involved in parody
and satire requires an understanding of what is being parodied in order to

264

CHAPTER FOURTEEN

“get it.” That is the context. Hall’s thesis was that part of what makes intercultural communication so difficult is that culture provides a context, and when
communicating with people from different cultures, we often lack context.
Pete and I thought that accurate lie detection is often a high-context activity. Typical lie detection experiments deny judges the context to understand
a sender’s statements. This makes content less useful. But, we thought, outside the lab much communication happens in situations where listeners have
context. When we are talking to a person we know, for example, we have relational history with that person. In our professional lives, we have occupational
experience. As members of a culture, we know the norms and the culture. A
little bit of contextual understanding, we thought, might go a long way in improving the usefulness of communication content in lie detection.
We designed a series of six experiments to test our hypothesis that a little
context information could dramatically improve accuracy.16 TDT experiments
thirty-nine through forty-four all involved the same basic comparison. Subjects
were randomly assigned to a control group or a little-bit-of-context-provided
group. In the control condition in each experiment, subjects did a typical deception detection task and were scored for accuracy. In the context-added group,
subjects were given a little bit of useful background (context) information.
TDT experiments thirty-nine through forty-four differed from one another
in two ways. Experiments thirty-nine, forty, and forty-one used students as
judges, while experiments forty-two, forty-three, and forty-four used experienced government agents-interrogators. The experiments also involved one
of three different lie detection tasks. Experiments thirty-nine and forty-t wo involved the cheating-experiment interviews from my NSF cheating tapes. Experiments forty and forty-three involved suspects in a mock crime Pete staged
at the Texas State University library. The mock-crime senders all confessed,
some falsely, some honestly. Finally, experiments forty-one and forty-four involved interviews of real suspects (some guilty, some innocent) in a bank embezzlement case. In this way, experiments thirty-nine through forty-four involved all of the six possible combinations of judges and tasks. We wanted to
show that our point about content in context was robust and not specific to a
particular lie detection task or set of judges.
The added contextual information, by necessity, varied from task to task.
For the cheating interviews, the useful contextual information was that the
cheating questions were difficult and that few people knew more than one or
two answers without cheating. That is, interviewees who got more questions
correct winning more money were likely to be cheaters. In the mock-crime
task, the context information included basic crime data such as a description

265

IMPROVING ACCURACY

Table 14.1. Results of TDT content-in-context experiments
thirty-nine through forty-four
Accuracy in
Control Group

Accuracy when
Context Provided

cheating

61%

77%

students

mock crime

33%

80%

41

students

real crime

42%

69%

42

experts

cheating

69%

72%

43

experts

mock crime

34%

81%

44

experts

real crime

68%

75%

51%

76%

Experiment

Judges

Task

39

students

40

Average across experiments 39–44

of the victim, the location of the crime, a list of what was stolen, and information about the weapon used. In the real crime task, the context information
included information about the crime that the actual investigators had before
the suspects were interviewed.
The results for our six content-in-context experiments are provided in table
14.1. As you can see, performance in the control group varied quite a bit from
task to task and from students to expert agents. The range in the controls was
from 33% to 69%. Absent context, everyone did poorly on the mock-crime
false-confession task (students = 33%, experts = 34%). Everyone did pretty
well on the cheating interviews (students = 61%, experts = 69%). The experts
did much better on the real crime interviews (68%) than the students (42%).
With the addition of a little bit of context information, however, accuracy
was both more uniform and higher. With context provided, accuracy ranged
from a low of 69% up to 81%. Averaging across the six experiments, accuracy
was 51% in the controls compared to 76% with context. That’s a 25% bump!17
Across the three different tasks and the two groups of judges, judges who had
some context in which to put the potentially deceptive statements consistently
performed better than the slightly-better-than-chance norm in deception detection experiments.
The content in context experiments together with the experiments under the
label situation familiarity provide good scientific evidence for two important
and robust conclusions. First, having some background knowledge to situate
or contextualize what is said leads to detection accuracy rates substantially
above the slightly-better-than-chance rate typical of the deception detection lit-

266

CHAPTER FOURTEEN

erature. Second, experiments involving lie detection tasks in which communication is evaluated in a way that deprives judges of content in context artificially depress accuracy. Slightly-better-than-chance accuracy findings apply to
experimentally decontextualized communication.
CONTEXT AND MOTIVES
One of the most important aspects of context relates to deception motives.
TDT experiments six, seven, and eight (chapter 10) showed that lying and deception are not random acts. Lies and deception happen when the truth poses
a problem for a communicator. When the truth works in our favor, unless we
are pathological, we are 100% likely to be honest. Not everyone lies when the
truth is problematic, but that is when lies occur. It follows that if we have a
feel for the likelihood of a deception motive, we should be able to improve our
odds substantially over just fifty-fifty or 54%.
Here is an example of the power of context from my cheating experiments.
Across all the versions, we have more than five hundred taped interviews.
Let’s round down to an even five hundred, for simplicity. The rates of cheating, lying, and confessions vary from version to version, but saying that about
one-third of subjects cheat, while two-thirds don’t cheat, is probably close to
the mark. Of those who do cheat, approximately one-third confess (although
this depends a lot on how they are questioned; rates vary from 20% to 100%).
So, of the 500, let’s say 335 don’t cheat and honestly deny cheating. Of the 165
who do cheat, 110 lie while 55 honestly confess. We never had a false confession in any version of the cheating experiment. Combining honest denials and
honest confessions, 390 (78%) of interviews are honest, and 110 (22%) are lies.
If we have no context at all, and we just make unbiased guesses, we would
expect 50% accuracy. If we instead base our predictions on the research reviewed in chapter 3, then we would expect a little better than that, about 54%.
These provide points of comparison.
What if we add a little context information? For the first bit of context, let’s
say we know, based on a prior cheating experiment, that the majority of subjects in the cheating experiments don’t cheat and lie. That is, we know the approximate base-rates. What if we just guess honest 75% of the time? We would
expect to get 75% of the 390 honest senders right (that’s 293/390) and 25%
of the 110 lies right (28/110).18 That gives us 321 correct out of 500, which
is 64%! We have improved fourteen points on chance and ten points on the
meta-analytic average with a bit of informed guessing.
Now let’s add in motive. Confessors have no reason to lie, so we believe them
100%. Based on base-rates, we are just going to guess on the denials (as before,

IMPROVING ACCURACY

267

75% honest). For the 335 non-cheating denials, we expect to be right 75% of
the time (251/335). For the 110 lying cheaters, we are correct only 25% of the
time (28/110). We get all 55 confessions right (55/55). This gives us 334/500,
which is 67% right. Not a big improvement, but our informed guesses are now
a little better. Further, if we could improve our questioning to increase the honest confession rate, then the bump for motive would be bigger.
Now let’s add one final important bit of context. We know the trivia questions are really hard. Most (but not all) noncheaters know only one or two questions. Most (but not all) cheaters get three or more questions right. We adopt
a decision rule where a trivia game score of 20% or less is an honest denial,
while a score of 30% or more lies. In the cheating experiments, this decision
rule produces a correct assessment about 80% of the time. If we use that on
the 445 senders who deny cheating, we would expect to get 356/445 correct.
Then we add in the 55/55 correctly judged confessions. Now we get 411/500,
which is 82%! Not bad for informed guessing.
In the preceding example from the cheating experiments, contextual information regarding base-rates and task difficulty proved relatively more useful
than projecting motive. But consider an example that happened just yesterday
in my role as a department chair. A professor in my department witnessed two
students cheating on an exam, and all three ended up in my office telling their
sides of the story. The professor said she was 100% confident that the students
were cheating. According to the professor, a book was open on one student’s
lap while the test was in progress, and the two students were communicating.
The students denied cheating, the existence of the open book, etc. Obviously,
the professor’s account and the students’ statements cannot both be true. The
question is, are the students lying, is the professor lying, or might there have
been a misunderstanding?
My opinion is that it is very unlikely that the professor was lying. This has
nothing to do with cues or demeanor. Instead, the professor has nothing to
gain from a false accusation. If the students were in fact cheating, the truth
works against the interests of the professor. The situation was not pleasant.
The easy way out would have been to give the students a warning and leave it
at that. Further, this is an experienced professor who is well liked and widely
respected by students. In my professional experience, malicious false accusations of student cheating by professors are exceeding rare. When they do happen, the professor is generally a known jerk or crackpot, not a widely liked,
popular professor.
Misunderstandings are probably more common, but still unusual. In ambiguous cheating situations, experienced professors usually will express sus-

268

CHAPTER FOURTEEN

picion, not confidence and certainty. Students will often initially deny cheating
but often honestly confess cheating under questioning. However, persistent,
tearful, sincere-appearing denials are not unusual. Confessions are more likely
when there is hard evidence, such as in plagiarism cases. In this situation there
was no hard evidence (at least none that the students knew about). I know all
this, having been a professor for twenty-five years. I therefore have a pretty
good idea about the relative odds of various scenarios.
I think it is possible that there might have been an honest misunderstanding
(i.e., the professor misperceived), but I think it was much more likely that the
students were cheating and just denied it. I’m not 100% sure about my assessment, but my informed estimation is that if the truth were known for sure, I
have a much better than 54% chance of being correct in my assessment. Maybe
the probability that I am correct is in the 85% to 95% range?
None of this matters in deception detection experiments with instructed
lies and random assignment to truth and lie conditions. The way lie detection
is usually researched, lying is literally random, and motive is irrelevant. But,
if TDT is right, lying is not random outside the lab. Motives predict dishonest
behavior, and understanding the situation in which communication occurs allows us to project motive and the potential for deceptive communication. Projecting motive can improve our odds considerably.
ACTIVE QUESTIONING AND DIAGNOSTIC UTILITY
Besides evidence and content in context, a third factor in improved lie detection is diagnostic questioning. Two relevant modules in TDT are Question Effects and Diagnostic Utility.
Question Effects: Question effects involves asking the right questions
to yield diagnostically useful information that improves deception detection accuracy.
Diagnostic Utility: Some aspects of communication are more useful than
others in detecting deception, and some aspects of communication can
be misleading. Diagnostic utility involves prompting and using useful
information while avoiding useless and misleading behaviors.
ACTIVE QUESTIONING DOESN’T ALWAYS HELP
I want to be clear. Merely asking questions of a sender makes little difference.
It is not merely that questions are asked that is important, but what questions
are asked and how they are asked. Further, there is no one, ideal way to question a potential liar. The diagnostic utility of various questioning strategies

IMPROVING ACCURACY

269

is highly contextual. Active questioning can be very helpful. Before we move
to the evidence that active questioning can improve accuracy, however, I will
document my claim that questioning does not necessarily improve detection
accuracy. Neither mere interactivity nor the tone of questioning seems to matter much for accuracy.
As we have seen, meta-analysis19 reports little impact for mere interactivity
on deception detection accuracy findings. In the meta-analysis, there were
eighty-five experiments involving no interaction at all—just judges evaluating
sender statements. Average accuracy in these no-interaction experiments was
53%. There were 189 experiments in which passive observers viewed an interaction between an interviewer and an interviewee. Average accuracy in these
observed-interaction studies was 54%. There were also eighteen experiments
involving direct interaction between a sender and a judge who questioned the
sender. Average accuracy in these interactive questioning experiments was 52%.
Clearly, slightly-better-than-chance prevails regardless of mere interactivity.20
Mere question asking or passive observation of questioned senders has little
impact on deception detection accuracy. We get the same old results regardless.
As was briefly mentioned, in the early 1990s Steve McCornack and I were
interested in a finding that came to be known as the probing effect. The probing effect refers to the empirical finding that probing (questioning) a sender
does not impact accuracy but does impact believability. Probed senders are believed at higher rates than senders who are not questioned. As research on the
probing effect progressed, it became clear that the probing effect was a small
but robust effect that held for honest and deceptive senders, active questioners
and passive observers, friends and strangers, and whether or not the judges
were primed to be suspicious.21 The point here is the probing effect applied
to perceived honesty, not accuracy, and it provides more evidence that mere
interaction matters little for improving lie detection.
In our probing effect experiments, Steve and I also found that tone of questioning mattered little.22 By “tone,” I mean whether the questions are supportive, implying acceptance, or accusatory, expressing skepticism. The mere
act of questioning, regardless of tone, affected truth-bias but not accuracy.23
The supportive or skeptical tone of the questioning didn’t matter for either
truth-bias or accuracy. This was surprising, because if I didn’t know the research findings, I would guess that harsh, accusatory questioning might make
a sender look more deceptive or guilty in the eyes of third parties.
In 2007 Aldert Vrij and his colleagues revisited the issue of questioning
styles and tone.24 They compared information gathering, the behavior analy
sis interview (BAI), and accusatory styles.25 There were no differences in accu-

270

CHAPTER FOURTEEN

racy. Accuracy was similar across conditions (52% for information gathering,
51% for BAI, and 48% for accusatory).26
TDT EXPERIMENT FORTY-F IVE:
A (FAILED?) QUESTIONING STYLE EXPERIMENT
In one version of the cheating experiments, my research team and I tried head-
to-head experimental testing of four different question styles.27 Questioning
style should matter, right?
We ran 104 new participants through the cheating game. Postgame, based
on random assignment, each sender was interviewed with one of four different
sets of questions, each reflecting a different style or strategy. All the questioning started the same way, with three open-ended questions generally asking
about the trivia game. In the nonaccusatory questioning, senders were asked
what they did when the experimenter was out of the room. This was followed
up by “Is there anything you haven’t told me?” In the accusatory questioning,
the interviewer said, “To get such a high score, you must have cheated, just
admit it.” That was followed by “Why should I believe you?” The bait question came from the BAI. For the final question, the interviewer asks, “As you
know, we have already interviewed your partner. Is there any reason why (s)he
would have answered the previous question differently?” It was basically a bluff.
The partner was not really interviewed. In the false-evidence questioning, the
interviewer said, “As you know, we have already interviewed your partner. (S)he
said that you cheated. Just admit it.” Again, the partner had not really been
interviewed, much less incriminated the interviewee. Once all the tapes were
made, we showed the tapes to a sample of 157 student judges, who made truth–
lie assessments, and we also coded each interview for BQ (see chapter thirteen).
The results are in table 14.2. There were no statistically significant differences. Questioning style had little impact on accuracy, truth-bias, or sender
BQ. Accuracy was high across the board, ranging from a low of 68% to a high
of 72%. Truth-bias was also high. There was no evidence that accusatory questioning or false evidence lowered sender credibility.
Confession rates varied quite a bit. Only 20% of the cheaters confessed
under nonaccusatory questioning. I find it surprising that even one cheater
in that condition confessed. In contrast, 80% of cheaters questioned with the
false-evidence ploy confessed. In a later experiment (see TDT experiment fifty-
two), false evidence and the bait question together yielded 100% confessions
from cheaters with an expert interrogator. So, it looks like question style and
strategy might impact confession rates. But since the differences weren’t significant, we just can’t rule out chance in experiment forty-five.

271

IMPROVING ACCURACY

Table 14.2. Results of TDT experiment forty-five
Variable

Accusatory

Nonaccusatory

False Evidence

Bait

Number cheating

6 of 26

5 of 26

5 of 26

6 of 26

Number confessing

3 of 6

1 of 5

4 of 5

3 of 6

50

20

80

50

0

0

0

0

Accuracy (excluding confessions)

72.2%

67.5%

70.4%

70.1%

Truth-bias (excluding confessions)

73.2%

64.2%

70.4%

71.4%

BQ score* (excluding confessions)

2.02

1.55

1.87

2.13

Percentage of cheaters confessing
False confessions

* BQ (believability quotient) was scored from −6 to +6 with 0 as the midpoint and positive values indicating higher believability.

Here is my thinking. We know so far that mere question asking and mere
interactivity are insufficient for improved accuracy. We know that the tone
questioning is not critical either. We also know, however, that sender content,
when taken in context, is highly useful. Therefore, maybe the key to effective
questioning is to elicit useful communication content.
MY QUEST FOR HIGHER ACCURACY
THROUGH DIAGNOSTIC QUESTIONING
We just reviewed boatloads of negative evidence showing that questioning
doesn’t matter much. But here’s the thing about negative evidence. The failure
to find effects shows only that questioning sometimes doesn’t matter. The nuanced conclusion is that questioning does not matter much in research with
certain research designs and certain aspects of questioning. The research does
not show that questioning never matters. Catch the difference? When we don’t
find something, it’s possible we are just looking in the wrong place.
Does questioning always matter? Clearly the answer is no. But let me now
show you the TDT evidence that questioning can matter quite a bit.
Let’s look back at the results of TDT experiment forty-five and table 14.2
again. The four questioning styles yield similar levels of accuracy. The standard
knee-jerk decontextualized interpretation of the findings is that questioning
style doesn’t matter much. The findings for the four questioning styles were,
after all, not significantly different.28 But while the four styles did not differ
from one another, accuracy was 72.2% overall (including confessions, 70% ex-

272

CHAPTER FOURTEEN

cluding confessions), with a 95% margin of error of plus or minus 3.4%. This
is hugely better than the 54% typical finding. Maybe the correct interpretation
is not that questioning makes no difference. Maybe the takeaway is that all four
sets of questions made a (albeit relatively equal) big difference!
As I mentioned previously, to date, more than five hundred subjects have
been run through various versions of my cheating experiments first introduced in chapter 10, TDT experiment eight. Over time my team changed up
the scripts used by the interviewers. Experiment eight was actually the second
version. Experiment forty-five was the third version. In all, there have been
six versions to date.
Judee Burgoon and I wrote the first two question sets.29 In the third and
subsequent versions, Pete Blair and I tried with each successive version to
improve the interview questions in a way that would yield higher accuracy.
As we better understood lie detection in the context of the cheating game, we
were able to come up with better questions that solicited answers containing
more useful content.
The evolution of the questioning scripts is provided in table 14.3. The wording and order of questions are provided along with the TDT experiments that
used the materials produced in each version. Table 14.4 lists accuracy results
from the various TDT experiments involving the different versions of the cheating interviews. Tables 14.3 and 14.4 list some experiments we have yet to cover.
Worry not. We will get there soon.
Looking at table 14.4, here is what we see. The most noticeable thing is how
accuracy improved progressively over the versions. With the one exception of
experiment forty-nine, students viewing the first set of interviews performed
pretty much as expected from the literature. Accuracies of 51%, 53%, 54%,
and 55% were obtained. However, various samples of experts did really poorly
on these interviews (29%, 31%, 42%). Something is going on that makes experts worse than chance. More on this in a bit.
Accuracy is quite a bit better when using interviews from the second set.
Experiments using the second set of interviews yield accuracy ranging from
56% to 72%, with most of the studies finding accuracy of 60-something percent. Experts no longer do worse and might even do slightly better. It is clear
that the second set of findings is systematically better than either the TDT experiments using the first set of interviews or the hundreds of non-TDT studies
constituting the meta-analyses. By the third time around, accuracy was up to
72%. This was just one experiment, but it was at the upper limit of what was
possible given prior research. I was thinking we were on to something. More
data were needed to be sure.

Table 14.3. Question sets in the cheating experiments*
First Question Set (Used in TDT Experiments 10–12, 14, 18, and 49–51)
1. Did you find the trivia questions difficult?
2. Was teamwork much of a factor? If so, please explain.
3. In looking at your score, you did better than other groups. Does this surprise you?
4. How would you explain your success?
5. Did cheating occur when I left the room?
6. Why should I believe you?

Second Question Set (Used in TDT Experiments 8, 22–23, 26–27, 29–35, 39, 42, and 49–51)
1. How much experience have you had working in teams?
2. Have you played trivia games before?
3. How often do you play? [follow-up] Why do you/ don’t you play them much?
4. In the current game, did you find the questions difficult? [follow-up] If you were going to
scale it on a scale of 1 to 10, what would you say they were in terms of difficulty?
5. Was teamwork much of a factor in your performance?
6. What would you say if I told you that you did better than other groups?
7. How would you explain your performance?
8. Did you cheat when the experimenter left the room?
9. Are you telling me the truth?
10. What would your partner say if I asked them the same question?

Fourth Question Set (Used in TDT Experiments 46–51)
1. Tell me, in as much detail as you can, what happened during the trivia game.
2. How well did you do on the trivia game?
3. Which questions did you and your partner get right?
4. For the answers you got right, explain how you knew the right answer.
5. In detail, what happened when the experimenter left the room?
6. Did any cheating occur?
7. When I interview your partner, what will they say about cheating?
8. Did you and your partner discuss cheating?
9. If someone did cheat, what should happen to them?

Continued on the next page

274

CHAPTER FOURTEEN

Table 14.3. Continued
Fifth Question Set (Used in TDT Experiments 52 and 53)
1. Tell me, in as much detail as you can, what happened in the trivia game.
2. How well did you do on the trivia game?
3. Which were the ones you got right?
4. How did you know the answers to those questions?
5. The Trivia Master told me he left the room at one point. Can you tell me what happened
when the Trivia Master left the room?
6. We have some concern that cheating might have happened while the Trivia Master was out
of the room because he left the answers in there. Did any cheating occur?
7. If somebody did cheat, what do you think should happen to them?
8. Under any circumstances would you cut them a break?
9. Obviously, we are going to talk to your partner. When we talk to your partner, what will
they say?
[Interviewer excuses self, leaves room “to check on a few things.”] [Upon Return . . .]
10. I can’t blame you, because the Trivia Master left the room, and they weren’t supposed to
do that. They weren’t supposed to do that, and they definitely weren’t supposed to leave
the answers. But we know you cheated. I’m not worried about the cheating part. What is
important is the data. I don’t want to have data in my study that is invalid. I want you to
be upfront with me.
11. Are you sure about that? Your partner says you did.
12. Is he lying, or are you lying? One of you must be.
* The third iteration of the cheating experiment used four sets of questions (non-accusatory, accusatory, bait, and false-evidence). These questions were briefly described under TDT experiment
forty-five. The full scripts are provided in Timothy R. Levine, Hillary C. Shulman, Christopher J. Carpenter, David C. DeAndrea, and J. Pete Blair, “The Impact of Accusatory, Non-Accusatory, Bait, and
False Evidence Questioning on Deception Detection,” Communication Research Reports 30, no. 2
(April 2013): 169–74. https://doi.org/10.1080/08824096.2012.76290. No scripts were used in the sixth
and final version (see TDT experiments fifty-four and fifty-five).

TDT STUDIES FORTY-S IX THROUGH FORTY-E IGHT:
IMPROVED ACCURACY WITH THE FOURTH QUESTION SET
Pete and I wrote the questions listed in table 14.3 under question set four.30
The questioning starts with requesting an open-ended narrative from the interviewee. Specific questions regarding who got how many questions correct
are asked to elicit content-in-context information. The next question asked
how the subject knew the correct answer. The idea for this question came
from my student Hillary Shulman, who played the trivia master in an ear-

Table 14.4. Improving accuracy using different question sets
in the cheating interviews
Question Set
First

TDT Experiment

Sample

Accuracy

TDT experiment 10 (denials only)

students

54%

TDT experiment 11 (denials only)

students

55%

TDT experiment 12 (denials only)

experts

31%

TDT experiment 14

students

53%

TDT experiment 18 (control group)

students

51%

TDT experiment 49

students

39%

TDT experiment 50

experts

29%

TDT experiment 51

experts

42%

Average for question set one
Second

44%

TDT experiment 24

students

56%

TDT experiment 26 (control group)

students

65%

TDT experiment 39 (control group)

students

61%

TDT experiment 42 (control group)

experts

69%

TDT experiment 49

students

67%

TDT experiment 50

experts

64%

TDT experiment 51

experts

72%

Average for question set two

66%

Third

TDT experiment 45

students

72%

Fourth

TDT experiment 46

students

71%

TDT experiment 47

students

78%

TDT experiment 48

experts

75%

TDT experiment 49

students

72%

TDT experiment 50

experts

73%

TDT experiment 51

experts

78%

Average for question set four
Fifth

75%

TDT experiment 52

experts

100%

TDT experiment 53

students

79%

Average for question set five
Sixth

90%

TDT experiment 54

experts

98%

TDT experiment 55

students

94%

Average for question set six

96%

276

CHAPTER FOURTEEN

lier version. She noticed that cheaters often spontaneously offered up explanations for how information was known. It was as if cheaters provided justifications, while people who actually knew the answers didn’t seem to have
that need. When asked, it is common for noncheaters not to know how they
knew the answer. They often say, “I just knew it.” Next, subjects were eventually asked outright if they cheated. This was followed by the bait question we
had used previously. Then we asked if they had discussed cheating. Our experience was that noncheaters were more likely to admit thinking about it. Finally, we asked if cheaters should be punished. In retrospect, the final question did not seem to add much.
Armed with these questions, we ran thirty-eight new students through the
cheating experiment using the new and improved questions during the interview. Of the thirty-eight, fifteen (about 40%) cheated. Of those fifteen, just
over half (eight) confessed under questioning. Once again, there were no false
confessions.
David Clare took the seven cheating liars and picked out seven honest noncheaters, matching as best he could on sex, race, and physical attractiveness.
This gave us a set of fourteen interviews for use in experiments forty-six, forty-
seven, and forty-eight. The point was to see whether we could obtain improved
accuracy, and if the accuracy would replicate with different samples of judges.
The judges in study forty-six were thirty-six college students recruited from
a standard subject pool. Study forty-seven was populated by twenty students
enrolled in a class I was teaching on deception at the time. They had read the
content-in-context research for class and had seen many previous versions of
the cheating interviews. The judges in study forty-eight were forty-seven elite
government agents.
The accuracies obtained by the three sets of judges were 71%, 78%, and
75% in studies forty-six, forty-seven, and forty-eight. All three were well above
the usual 54% accuracy. These results were very encouraging. It seemed that
questioning can matter quite a bit.
TDT EXPERIMENTS FORTY-N INE, FIFTY, AND FIFTY-O NE:
HEAD-TO-H EAD COMPARISONS
The next obvious step was to compare various question sets from the different versions of the cheating experiments head-to-head.31 I sampled two honest and two cheating liar interviews from the first, second, and fourth versions
of the cheating experiments. This gave me twelve interviews, half honest, half
deceptive, for use in experiments forty-nine, fifty, and fifty-one. The predictions were, of course, that accuracy would improve from set one to set two

277

IMPROVING ACCURACY

Table 14.5. Improving accuracy using different question
sets in experiments forty-nine, fifty, and fifty-one
Question Set
First

TDT Experiment

Sample

Accuracy

TDT experiment 49

students

39%

TDT experiment 50

experts

29%

TDT experiment 51

experts

42%

average for question set one
Second

37%

TDT experiment 49

students

67%

TDT experiment 50

experts

64%

TDT experiment 51

experts

72%

average for question set two
Fourth

68%

TDT experiment 49

students

72%

TDT experiment 50

experts

73%

TDT experiment 51

experts

78%

average for question set four

74%

and from set two to set four, and that the pattern of improvement would hold
across different samples of judges.
The subjects were ninety-three students in experiment forty-nine, 207 law
enforcement professionals in experiment fifty, and thirty-nine elite US Customs agents in experiment fifty-one. Experiments forty-nine and fifty used the
same twelve interviews, while experiment fifty-one used different interviews
culled from the same three sets.
The results are summarized in Table 14.5. Experiments forty-nine, fifty, and
fifty-one replicated the accuracy findings of over 70% for question set four.
The pattern of improvement over question sets is clear. The findings of studies forty-six through fifty-one lead to two firm conclusions. First, the type of
questions asked can make a big difference in accuracy, and second, asking better questions provides one path to improved accuracy in detecting deception.
A BRIEF WORD ABOUT NEGATIVE UTILITY
Check out the low accuracies for the first question set in experiments forty-
nine, fifty, and fifty-one. Not only are the accuracies below those obtained in any
TDT experiment using any of the other question sets (see table 14.4), they are

278

CHAPTER FOURTEEN

also significantly lower than both chance and the 54% meta-analysis average.
What should we make of that? Should we dismiss the findings as merely aberrant, or might something important be at play?
The below-chance accuracy in the experiments involving question set one
does not appear to be a fluke. Below-chance accuracy has been observed in four
of the eight experiments using those tapes, and in all three experiments using
those tapes in conjunction with expert judges. Below-chance accuracy, in contrast, has never been observed in any TDT experiment using any of the other
question sets. In table 14.4, eighteen of eighteen experiments using cheating
tapes with questions other than set one produced accuracy that is significantly
greater than chance. There is something different about the tapes using the
first set of questions.
One feature that separates both TDT and IDT from most other theories of
deception is that these are the only two deception theories specifying a mechanism for predicting and explaining below-chance accuracy. If you think about
it for a moment, below-chance accuracy is an odd thing. Truth-biased judges,
weak cues, focusing on the wrong cues, and individual differences in sender
demeanor all tend to push accuracy down toward chance. But these mechanisms should not create negative transparency. Below-chance accuracy requires
that honest senders are seen as less honest than liars.
IDT predicts below-chance accuracy when liars but not truth tellers pick
up on judge suspicion and strategically adapt their behavior to appear more
honest. That does not seem to be what is going on in TDT results, because
all iterations of the cheating interviews involve questioning that reveals that
both honest and lying senders are under suspicion. In fact, question set one
is less accusatory than some of the other question sets where accuracy does
not dip below chance. Thus, inconsistent with the IDT mechanism, suspicion-
implying questioning does not appear to be the culprit.
TDT’s proposition thirteen specifies that poorly conceived questioning can
backfire and produce below-chance accuracy.
TDT Proposition 13: Both confessions and diagnostically informative
communication content can be produced by effective context-sensitive
questioning of a potentially deceptive sender. Ill-conceived questioning,
however, can backfire and produce below-chance accuracy.
I suspect that this is what is going on in some of the question set one interviews. Specifically, I think the final question in that set (Why should I believe
you?) throws some of the honest noncheaters off. Cheaters know they cheated.

IMPROVING ACCURACY

279

Being under suspicion comes as less of a surprise. Noncheaters, however, have
little reason to anticipate skeptical questioning. To compound matters, honest
senders have no real way in the cheating experiment to exonerate themselves.
When asked why they should be believed, several noncheaters stumbled over
the answer and ended up with a perfectly honest but very unconvincing “I
don’t know.” Some cheaters, however, appeared to see the skeptical question
coming and answered convincingly but deceptively, “Because I am an honest
person.” Expert judges who are attuned to demeanor, and especially demeanor
shifts, get fooled. They attribute the shift in demeanor and lack of conviction
to deception, not its actual cause, which was an unanticipated question that
cannot be answered in a way that is both convincing and honest.32 The upshot is that questioning can make honest people look deceptive. This makes
me very skeptical of adding cognitive load through questioning or about asking unanticipated questions in general. The trick is asking questions that are
hard for liars but easy for honest folk.
PERSUASION AS LIE DETECTION
TDT proposition twelve lists a subsequent confession by a deceiver as a path
to improved lie detection, and TDT proposition thirteen adds that effective
context-sensitive questioning can be used to facilitate honest confessions. Combined, these propositions suggest that persuasion is another path to improved
lie detection. If you think you might be being lied to, you can persuade the
sender to come clean, confess his or her lies, and tell the truth. The results of
TDT experiment forty-five were certainly suggestive regarding the power of
persuasion as a useful deception detection strategy, and the How Lies study
(TDT study thirty-seven) found that liar confessions were a common method
of lie detection outside the lab. However, the true power of persuasion as a lie
detection method is demonstrated in the final four TDT experiments investigating expert questioning.
EXPERT QUESTIONING
If readers will recall chapter 3, it was mentioned that college students and experts typically perform about the same in deception detection experiments.
These conclusions come from two different meta-analyses, both published in
2006.33 In the first, accuracy involving students was 54%, compared to 55%
for police, 51% for detectives, 55% for federal officers, and 54% for customs
agents. Neither age nor education nor professional experience was correlated
with accuracy. In the second meta-analysis, the average accuracy in 250 studies
involving nonexperts was 53.3%, compared to 53.8% in forty-two prior studies

280

CHAPTER FOURTEEN

involving expert judges. In an additional twenty experiments testing experts
and nonexperts head-to-head, the difference was a tiny d = -0.03. Thus, both
meta-analyses lead to the same conclusion: experts and students don’t differ
in terms of accuracy. Experts, however, are a bit less truth-biased (52% compared to 56% for nonexperts).34
The TDT research reviewed up to this point leads to the same conclusion.
Look again at table 14.4. Students do better on question set one, but that reverses on set two, and there is not much difference on set four. In TDT experiments involving the assessment of videotaped interviews, students and
experts yield similar results.
The proper inference from these data, however, is not the obvious conclusion that students and experts just aren’t different. Instead, the findings show
convincingly that students and experts are not much different in the type of
tasks that constitute deception detection experiments. When the task involves
passively watching videotapes, the evidence for little difference is compelling.
But maybe this isn’t how expert lie detection really works.
TDT’s final module and last proposition are about how expert lie detection
really works:
Expert Questioning: Expertise in deception is highly context dependent
and involves knowing how to prompt diagnostically useful information
rather than passively observing deception cues.
TDT Proposition Fourteen: Expertise in deception detection rests on
knowing how to prompt diagnostically useful information rather than
on skill in the passive observation of sender behavior.
According to TDT, what makes expert lie detection expert isn’t knowing how
to read cues and demeanor. Reliance on cues and demeanor pushes accuracy
down toward chance for everyone, expert or not. TDT research (TDT experiments twelve, twenty-nine, fifty, and fifty-one) even shows that cues and demeanor can trip up experts, producing below-chance performance. Instead,
expertise resides in knowing how to effectively question people so that honest
people seem honest, and lies are revealed. Expert questioning increases sender
transparency by prompting diagnostically useful answers.
If TDT is right about expert questioning, then for experts to show their expertise, several preconditions must be met. First, the experts need to either
script their own questions (with the freedom to deviate from the script) or be
free of a script altogether. That is, if expertise lies in knowing what questions to

IMPROVING ACCURACY

281

ask and how to ask them, then imposing researcher-scripted questions should
be an artificial impediment to expertise. Second, experts must have sufficient
context-situation background knowledge. This is because diagnostic questioning is, according to TDT proposition thirteen, context dependent. Third, persuasion and honest-confession solicitation needs to be a viable option. If part
of lie detection expertise is the solicitation of honest confessions, then excluding confessions in the evaluation of expert performance is another artificial
impediment to expertise. A fair test of expert questioning must let experts do
the things that make them expert.
TDT EXPERIMENTS FIFTY-T WO THROUGH FIFTY-F IVE:
SMASHING THE ACCURACY CEILING WITH EXPERT QUESTIONING
When Pete and I tried our hand at the fourth edition of the cheating interviews
used in TDT experiments forty-six through forty-eight, we each got 86% (twelve
out of fourteen) correct.35 We compared notes. We thought the questioning
made ten out of the fourteen interviews pretty clear-cut. It turned out that we
were both right on each of those ten interviews that we thought were pretty
easy. There were four, however, that we thought were difficult. As it turned
out, we both got only two of the four ambiguous ones correct. The thing was,
we knew those four were the questionable ones before we scored ourselves.
We knew we were guessing. Because they were taped interviews, we couldn’t
go back and ask more questions and try to reduce the ambiguity. What if we’d
had that option? Might we have done even better than 86%?
In a related experience, I was at a conference, and in attendance was an
expert who had been an interviewer in an expert deception detection experiment run at a different lab by different researchers. The expert told me that
he thought he could have done much better than he did. He did not think the
study he had participated in adequately assessed his ability. The research design, he said, thwarted him. He was given scripted questions and told to stay
on script. A few times, he said, he went off script and got the sender to confess. In such cases the researchers had scolded him, said that he wasn’t following instructions, and that the data had to be tossed. He said he needed
the freedom to resolve ambiguities and to persuade liars to be honest. Sticking to the script and excluding confessions meant that the results did not reflect his ability.
Pete and I put our experiences together with the expert’s story of being
thwarted by the researchers, and we decided to put our expert questioning
ideas to the test. TDT experiment fifty-two provided initial proof of concept.
We thought that if we could produce some strong preliminary evidence, we

282

CHAPTER FOURTEEN

could leverage that to get the funding we would need for a larger-scale experiment. We ran thirty-three more subjects through the cheating experiment.
This time Pete did the questioning. As a base script, Pete crafted the twelve
questions listed in table 14.3, fifth set. However, he was free to improvise as
needed. After each interview, he recorded his assessment as to who he thought
cheated and who didn’t.
Only four of the thirty-three participants (12%) cheated. All four honestly
confessed. Even though Pete used a false-evidence ploy (he told them that their
partner had said they cheated), there were no false confessions. When we tallied Pete’s judgments, he got all thirty-three correct. 100%! Wow!
Both Pete and I were surprised by the results. We both thought he would do
well. Based on TDT experiments forty-six through fifty-one and our content-in-
context experiments, we knew we could reliably get accuracy levels in the 70s.
We were pretty sure we could beat that and hit accuracy in the 80s or maybe
even 90%. But, thirty-three of thirty-three exceeded our expectations. Pete, in
particular, was aware that he was assigning honesty at an unusually high rate.
He thought that he was probably making too many honesty judgments. The
cheating base-rates in the prior versions were always higher. He wondered if
he was failing to catch some cheaters. He wasn’t. The base-rates were just unusual in that run, and yet he still performed to perfection.
For TDT experiment fifty-three, we showed Pete’s thirty-three interviews to
136 college students. The students got 79%. Students passively viewing Pete’s
interviews did not do as well as Pete, but they did pretty darn well.
The most obvious limitation with TDT experiments fifty-t wo and fifty-three
was that we had just one expert (Pete), just thirty-three interviews, and only
four cheaters to catch. Going thirty-three for thirty-three is much too long a
string to be a statistical fluke. Pete’s performance was not just chance. But the
generality of the findings was a big question. So, I put together a dream team
of deception researchers (Pete Blair, Steve McCornack, Kelly Morrison, and
Hee Sun Park), and we applied to the FBI for funding to replicate TDT experiments fifty-two and fifty-three with different experts.
The FBI gave us the funding, but they would not provide their own interrogators, so I used my connections and got experts from a different government
agency. We flew in five experts, one at a time, for a couple days each, and gave
them the mission of questioning students after the cheating game to ascertain
whether cheating occurred or not, and if so, who cheated.
In the final version (to date) of the cheating experiment (TDT experiment
fifty-four), the five experts collectively conducted eighty-nine unscripted interviews. At the end of each interview, we asked the experts whether they thought

IMPROVING ACCURACY

283

cheating had occurred, and if yes, who cheated, both the subject and their partner or just the partner.36
Cheating happened in forty of eight-nine (45%) of the trivia games. Of
the forty instances of cheating, the partner but not the subject cheated thirteen times, and both the partner and the subject cheated twenty-seven times.
The experts obtained confessions in thirty-four of the forty (85%) sessions involving cheating. The experts correctly identified whether cheating occurred
in eighty-seven of the eighty-nine (98%) interviews. Three of the five experts
were perfect (fourteen of fourteen, sixteen of sixteen, and twenty for twenty),
while the other two had a single error each. On the more specific question of
who cheated, the experts were 96% accurate. Both additional errors involved
the expert thinking that only the partner had cheated when in fact the subject had cheated too.
For TDT experiment fifty-five, we showed a random sample of thirty-six of
the taped interviews from TDT experiment fifty-four to thirty-four students
in my deception class. The students were correct on 94% of the interviews.
There are three especially important aspects of TDT experiments fifty-one
to fifty-five to highlight. First, the expert questioning results show what is
possible but do not provide much insight into what is typical. Here is what I
mean: The point of the research was to see whether higher accuracy could be
obtained by experts if they were free of imposed scripts and where honest and
believed confessions were counted as correct judgments. The results showed
that high accuracy could be obtained. What the results do not show is whether
those high levels of accuracy extend to other experts or to other situations. The
point of the research was testing a generalization, not making a generalization.
I do not claim that experts are invariably good lie detectors, only that they can
be much better than prior research had suggested.
Second, showing the videotaped expert interviews to student samples adds
critical information regarding the causal forces at play. Specifically, however
the experts did it, what they did was make the senders more transparent. They
interviewed the senders in such a way that even untrained passive observers
could tell who was lying. This provides key evidence for the TDT claim that expertise in deception detection rests on knowing how to prompt diagnostically
useful information rather than on skill in reading sender behavior.
Third, the question arises as to what the experts did in order to increase
transparency. I have not formally analyzed the expert interviews yet, but I have
watched them several times and have noticed several things. First, there is
not just a single way to effectively question a sender. Each of the six experts
(including Pete) had their own styles. A couple were hard-nosed, serious, in-

284

CHAPTER FOURTEEN

tense, and accusatory. One guy had a style much like that of Columbo (from
the 1970s TV series). One had a very friendly and disarming style.
Mostly, they started off conversationally. All tried to get a feel for the type
of person they were talking to. They wanted to know the sender’s major in
school, how good a student he or she was, and the sender’s motivation for participating in the research (just for research credit, interested in the monetary
incentive, etc.). They all asked about critical information, such as the number
of correct answers, who got which questions right, and how they knew the
answers they did. They also asked questions about integrity and the sender’s
views on cheating and lying.
The experts were very good listeners and had a good memory for what had
been previously said. They circled back with questions asking the same thing
again. They all paid close attention to logical consistency (coherence). But when
they caught an inconsistency or noticed a sender reacting to a question, they
did not automatically presume guilt. Instead, they asked more focused questions, trying to figure out what was up.
Another thing they all did was try to work what might be called “themes.”
Themes are lines of persuasion aimed at honest confessions. For example, one
common theme was that a sender who claims to be an honest person needs
to admit to cheating if he or she did. They talked to the senders about how
they would feel about themselves if they cheated. They provided justification
for cheating, while stressing the importance of being honest at that moment.
If one theme wasn’t working, they shifted to another.
RESPONDING TO CRITICISM
Not long after the article version of our expert experiments was published, three
psychologists authored an essay disputing our research and conclusions.37
Vrij, Meissner, and Kassin noted that our findings were inconsistent with the
old 54% findings. Rather than interpreting our results as showing evidence
that improved accuracy might be possible, they concluded that our findings
simply should not be believed. They argued that our methods were flawed.
They even asserted that our findings were dangerous because we did not get
any false confessions. They said that expert questioning like that in our study
causes false confessions and that all experts lack the skill to be good lie detectors or to solicit only honest confessions. In essence, because our findings
were different from what they have found in their research, our research just
can’t be scientific.
According to these critics, our method was invalid because the cheating
experiment provided a super-easy lie detection task in which any interviewer

IMPROVING ACCURACY

285

or judge could do well. Lying convincingly about cheating, they claimed, was
just too hard. That is, high accuracy occurred because of our experimental
task. The task, not the experts, prevented cheaters from telling believable lies.
The number of questions interviewees got correct, they argued, was too diagnostic.38 Anyone could do as well as the experts by simply asking two scripted
questions: How many did you get right? And how did you know the answers?
Further, they asserted, it was easy for honest noncheaters to prove their innocence, and this prevented false confessions. Near-perfect accuracy, they said,
was methodological deck-stacking, not the result of expert skill in questioning.
I do not believe that Vrij, Meissner, and Kassin’s criticism is either logically
sound or good-faith academic argument. Their critique seems to me to be a
brazen series of straw man arguments that mischaracterize TDT research. The
flaws they point out simply do not exist in our research and thus cannot possibly explain our results.
The idea that detecting lies in the cheating experiments is a simple matter
and that anyone can do as well as our experts did by just asking a couple of
simple questions is clearly disproven by the research presented in this chapter. Look back at table 14.4. All those findings are from the cheating experiments. If our critics were right, and lie detection in the cheating game is just
an easy task, then accuracy would always be as high as it was for the experts in
experiments fifty-two and fifty-four. It is not. Twenty-two (out of 22) prior TDT
experiments with the cheating tapes refute Vrij, Meissner, and Kassin’s assertion. Six prior TDT studies (forty-six through fifty-one) directly asked the two
questions that our critics argue are sufficient. Those questions improve accuracy (nine points over the second version and three points over the third), but
not enough to explain the experts’ performance (which is more than twenty
points higher than question set four). But maybe the most extreme disproof
of the lying-was-too-hard claim is reviewed in the last chapter. In TDT experiment thirty-three, experts (from the same agency) watched scripted videotaped
interviews of mismatched liars. The most experienced experts incorrectly believed the five mismatched liars more than 85% of the time. Clearly, the demeanor experiments show that convincing lies are absolutely possible in the
cheating experiments.
Earlier in the chapter, I show how three bits of context information (proportion cheating, confession or denial, and number of questions right) could improve accuracy in the cheating experiments probabilistically to 82%. There are
three things to keep in mind. First, using automatic decision rules and probability as I did in the examples guarantees errors in the long run. Eighty-two
percent is quite good, but it means settling for being wrong 18% of the time.

286

CHAPTER FOURTEEN

Second, the 82% is a best-case scenario based on past trends applied in retrospect. The actual rates in new data will usually be at least a little different. And
the more idiosyncratic the new situation, the less useful a decision rule optimized on old data. For example, in the second iteration of the cheating experiment (see TDT experiment eight), the cheating rate was 24%, and the honest
confession rate was 40%. In the next version (TDT experiment forty-five), the
cheating rate was 21%, and the confession rate was 50%. In the expert studies, however, the rates were different. In TDT experiment fifty-two, the cheating rate was unusually low, at 12% with 100% honest confessions, while in
TDT experiment fifty-four the cheating rate was unusually high, at 45% with
an 85% confession rate. We know these rates in hindsight, but experts could
not know what the rates would turn out to be. Third, even if the rates did hold
perfectly to form, simple decision rules still can’t explain the experts’ performance, because they were statistically better than 82%.
Our critics’ claim that false confessions did not happen because noncheaters could prove their innocence is also false. The question set used in the first-
run cheating experiment ended with “Why should I believe you?” Our critics
contend that this was an easy question for honest senders. That was not the
case. In fact, detection experiments with that question set all found poor accuracy (TDT experiments ten, eleven, twelve, fourteen, eighteen, forty-nine, fifty,
and fifty-one). Further, two TDT experiments (forty-five and fifty-t wo) used a
false-evidence ploy on honest interviewees. There were no false confessions in
those experiments either. The premises of the critics’ arguments are just false.
Earlier in this chapter, I suggested that fact-checking is the best way to detect lies and falsehoods. I don’t know why the editor and peer reviewers at Psychology, Crime, & Law did not fact-check Vrij, Meissner, and Kassin’s essay before publication. The premises of their arguments can easily be fact-checked
(just as I did the previous paragraphs). Using the correspondence method, we
know their claims to be false. Their assertions do not pass fact-checking. Applying the criteria for moving from false to deception listed earlier, we might ask
whether their essay would mislead reasonable readers who were not familiar
with the TDT program of research. Did Vrij, Meissner, and Kassin know their
claims were false? For example, we could see if they cited the work disproving
their claims in the past. Or should they have known? For example, if they read
the work they were criticizing, they might be expected to have known about the
false-evidence question being included. Did our finding create a problematic
truth for them, and did their critique serve a purpose by its falsity? Defending
TDT experiments fifty-one through fifty-five provides a clear example of how
the TDT approach described in this chapter can be applied.

IMPROVING ACCURACY

287

CONCLUSION
This chapter reviews the TDT evidence for improved lie detection. A chief
claim of TDT is that cue-and demeanor-based lie detection hover just above
chance. Improved lie detection is possible, but improvement requires moving away from cues and demeanor to focus on contextualized communication
content, evidence, and persuasion.
This chapter reviews research documenting five paths to improved lie detection:
• Using evidence to establish ground truth and assessing the correspondence between communication content and ground truth.
• Using situational familiarity and contextualized communication content to assess plausibility.
• Using situational familiarity and contextualized communication content to assess motives for deception.
• Strategically questioning senders to elicit diagnostically useful communication content.
• Persuading liars to be honest and tell the truth.
While TDT research shows that all these paths lead to improved lie detection, choosing one path does not preclude taking the others too. There is not
just one way to detect lies, and all five methods work well in combination.
Watching the experts in TDT experiment fifty-four at work, I came to appreciate that they had a big toolbox and were not tied to any one or two tools.
The upshot of this chapter is that slightly-better-than-chance accuracy is no
longer inevitable. The accuracy ceiling has been smashed. Claims that humans
can’t detect lies have been disproven. TDT provides much-needed guidance as
to what works and what does not work under what conditions.
