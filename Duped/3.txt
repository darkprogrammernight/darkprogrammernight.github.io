
3
Deception Detection Accuracy
In 1941 Fay and Middleton conducted what might be the first modern
deception detection experiment.1 Six speakers were each asked ten personal
questions about factual matters. Right before each answer, the speaker was
instructed with a flash card to lie or tell the truth. All answers were delivered
over a public address system. Forty-seven students listened to each answer and
were instructed to make a determination of truth or lie for each answer. Over
the sixty answers and forty-seven listener-judges, 55.6% were judged correctly.
The just slightly-better-than-fifty-fifty finding was the case for both male and
female speakers, both male and female listeners, and same-and opposite-sex
combinations of speakers and listeners.
Deception detection research really didn’t take off until the 1970s, but Fay
and Middleton’s experiment foreshadowed the hundreds of experiments that
would follow. For one thing, their results were only a couple of percentage
points off the eventual across-study average, and their conclusion of only
slightly-better-than-chance accuracy would be replicated over and over and
over for the next seventy-five years.
Another important aspect of the Fay–Middleton study is that their method
would become a prototype for how to study deception detection accuracy. In
most of the studies that were to follow, there would be senders who created
truthful and/or deceptive communications that would be received by people
(I’ll call them judges) who evaluated the messages as honest or lies. Typically,
an equal number of truths and lies were presented, and accuracy was calculated as the percentage of correct judgments across truths and lies. Variations
on this basic theme were accomplished by changing the nature of truths and
lies being judged (e.g., topic of the lie, sender motivation, spontaneous lies vs.
planned lies, etc.), changing the mode of communication (e.g., face-to-face,

DECEPTION DETECTION ACCURACY

37

videotape, text messaging), or changing the judges (e.g., students, police, psychiatrists). The sender and the judges might be strangers or friends, and the
sender might or might not be directly questioned by either the judge or an interviewer, and so forth. As the research progressed, more and more variations
were tried, with the goal of finding what might lead to improved accuracy. The
range of variations was vast, but most deception detection experiments can
be understood as variants of the same theme, and most studies found results
remarkably similar to those of Fay and Middleton. Improving upon slightly-
better-than-chance accuracy proved elusive, at least until a few years ago. And
even now that a handful of approaches to improved accuracy have been documented, most findings still have just slightly-better-than-chance accuracy.
In this chapter, my goal is simply to demonstrate the remarkable pervasiveness, tenacity, and robustness of the slightly-better-than-chance accuracy
findings. I also point out several features of slightly-better-than-chance accuracy that I find provocative. I ask the reader to keep in mind that a viable scientific theory of deception must provide a coherent account of the findings
described here and that a viable theory cannot be contradicted by reliable empirical findings. Thus, the findings described here provide standards for assessing the empirical adequacy of the theories and approaches explained in
subsequent chapters.
It is helpful to be explicit up front about how outcomes are typically scored
in most deception detection experiments: messages are either honest or outright lies, and judges are asked to judge each message as either honest or lie.
The task and the scoring are very much like a true-false test. Honest messages
judged as honest, and lies judged as lies, are scored as correct. Honest messages scored as lies, and lies scored as honest, are incorrect. Overall raw accuracy, or just “accuracy,” for short, is the percentage of judgments that are correct averaged across all messages. That is, accuracy is the percentage of correct
truth–lie classifications. Truth accuracy and lie accuracy refer to scores for just
the truthful messages or only the lies, respectively. Finally, truth-bias is the
percent of messages judged as honest.
There are several meta-analyses of deception detection accuracy.2 Of these,
the one I find most useful is the 2006 Bond and DePaulo meta-analysis. It summarized almost three hundred findings cumulated from 206 works involving
almost 25,000 judgments of over four thousand senders. It is very thorough,
and its findings are widely accepted. It offers a clear picture of the sixty-four
years of research from the publication of Fay and Middleton’s study through
2006. Other meta-analyses shed additional light on some specific issues, but the
Bond and DePaulo 2006 analysis gives us the clearest view of the big picture.

38

CHAPTER THREE

Bond and DePaulo document and summarize strong evidence for the robust nature of the slightly-better-than-chance accuracy finding in deception detection experiments. The mean percentage of truth–lie judgments that were
correct was 53.98% unweighted and 53.46% when weighted by the number of
judgments constituting each sample. The population standard deviation was
estimated to be 4.52%, and the 95% confidence interval around the weighted
mean was 53.3% to 53.6%. In terms of the spread of findings, the first and
third quartiles were 50.0% and 58.0%, meaning that half of all findings fell
within this narrow range. Ninety percent of all findings fell within 45% and
64%, and findings below 39% and above 67% were in the bottom and top 1%,
respectively. The distribution of these findings is graphed in figure 1.1 in the
first chapter. As it illustrates, most prior results fall closely around the across-
study average. “Slightly-better-than-chance” concisely yet accurately describes
the large number of findings summarized by the Bond and DePaulo meta-
analysis.
Although the 54% average clearly reflects poor performance, accuracy is
nevertheless statistically significantly better than chance. The null hypothesis
of fifty-fifty mere chance can be ruled out with a high degree of statistical confidence (p < .0001). People are better than chance in deception detection experiments, and the effect size for the difference between the average accuracy
and chance is about d = 0.40.
The statistical conclusion that accuracy is substantially better than chance
seems incongruous with the substantive conclusion that accuracy is poor. This
apparent paradox causes much confusion in the literature, and it requires explanation. Why this is the case may not be obvious or intuitive. For the rec
ord, here is what is actually going on. The calculation of statistical differences
(both statistical significance and effect size) involves the ratio of the size of the
difference relative to variability. Large effects can be “large” because the differences (in this case, between observed accuracy and chance) are big or because
variability is small (i.e., everyone performs similarly). In terms of statistical significance, the difference between observed accuracy and chance (typically 3%
or 4%) is divided by the standard error of the difference. This standard error
is, in turn, determined by the sample size and the standard deviation. In most
areas of social scientific research, individual variation is substantial, so small
differences can be significant only when the sample size is large.
Deception detection is different.3 There is typically not much individual
variation in deception detection experiments, so the standard deviation of accuracy tends to be small. Because the standard deviation is small, the stan-

DECEPTION DETECTION ACCURACY

39

dard error is small, and the ratio of the difference from chance to the standard
error is large. Consequently, differences of 3% or 4% that are small in an absolute sense are large in the statistical sense of being much larger than the
differences expected by chance given the minuscule variability in individual
performance. Stated differently, individual variability is minimal, so even small
differences are statistically significant even with small samples.
A similar explanation accounts for the effect size associated with the difference between obtained accuracy and chance. Effect sizes are some ratio of “effect variance” to “total variance,” where the total is “effect variance” plus “error
variance.” In deception detection experiments, the lack of “within variance” associated with difference in individual performance makes error variance small
and the effect-to-total ratio relatively large.
The apparent paradox of small absolute differences that are statistically
large creates much confusion. Researchers often rely on statistical significance as the standard for scientific evidence and are quick to make the leap
from a statistically significant difference to a pragmatically useful difference,
even though the second does not necessarily follow from the first. Confusion
is further created by arbitrary rules of thumb where d = 0.4 is a “medium” effect size. The result is incoherent thinking in which 54% accuracy is dismissed
as not much better than a coin flip, while findings that are p < .001 or d = 0.4
are heralded as strong evidence for important effects, even though the 54% is
p < .0001 and d = 0.42. The confusions are compounded when the focus is on
the findings of individual experiments without placing results into the larger
context of the research literature.
As a result, there are a number of variables that make a “statistically significant difference” in terms of accuracy, but few of these make more than
a few percentage points’ difference or produce findings other than slightly-
better-than-chance accuracy. The research literature is full of “significant differences” between trivial gradations of slightly-better-than-chance accuracy.
Table 3.1 lists the accuracy associated with several well-researched potential
moderators of deception detection accuracy.
MEDIUM/MODALITY
Communication medium and the related ideas of cue availability-exposure and
media affordances have probably been the single most often studied class of
predictor variable in the deception detection accuracy literature. Many deception theories specify that deception can be detected based on various types of
deception cues. The key idea is that not all types of cues are available in all

Table 3.1. Slightly-better-than-chance
deception detection accuracy

1

Across-Study Average

53.46%1

Video only

50.4%1

Audio only

53.8%1

Audio-visual

54.0%1

Little sender motivation

53.4%1

Motivated senders

53.3%1

Spontaneous lies

53.1%1

Planned lies

53.8%1

No exposure to baseline honest behaviors

53.0%1

Exposure to an honest baseline

54.6%1

Student and non-expert samples

53.3%1

Students as judges

54.2%2

Expert judges

53.8%1

Expert judges

55.5%2

Charles F. Bond Jr. and Bella M. DePaulo, “Accuracy of Deception Judgments,” Personality and So-

cial Psychology Review 10, no. 3 (August 2006): 214–34, https://doi.org/10.1207/s15327957pspr1003_2.
2

Michael G. Aamodt and Heather Custer, “Who Can Best Catch a Liar?” Forensic Examiner 15, no. 1

(Spring 2006): 6–11.
3

Maria Hartwig, Pär Anders Granhag, Leif A. Strömwall, and Aldert Vrij, “Police Officers’ Lie De-

tection Accuracy: Interrogating Freely versus Observing Video,” Police Quarterly 7, no. 4 (December
2004): 429–456, https://doi.org/10.1177/1098611104264748.
4

Norah E. Dunbar, Matthew L. Jensen, Judee K. Burgoon, Katherine M. Kelley, Kylie J. Harrison,

Bradley J. Adame, and Daniel Rex Bernard, “Effects of Veracity, Modality, and Sanctioning on Credibility Assessment during Mediated and Unmediated Interviews,” Communication Research 42, no. 5
(July 2015): 649–74, https://doi.org/10.1177/0093650213480175.
5

Steven A. McCornack and Malcolm R. Parks, “Deception Detection and Relationship Development:

The Other Side of Trust,” in Communication Yearbook 9, edited by Margaret L. McLaughlin, 377–89
(Beverly Hills: Sage, 1986).
6

Steven A. McCornack and Timothy R. Levine, “When Lovers Become Leery: The Relationship be-

tween Suspicion and Accuracy in Detecting Deception,” Communication Monographs 57, no. 3 (September 1990): 219–30.
7

Timothy R. Levine, David D. Clare, Tracie Green, Kim B. Serota, and Hee Sun Park, “The Effects of

Truth-Lie Base Rate on Interactive Deception Detection Accuracy, Human Communication Research
40, no. 3 (July 2014): 350–372, https://doi.org/10.1111/hcre.12027.

Table 3.1 Continued
Expert questioning in Hartwig et al. (2004)

56.7%3

Expert questioning in Dunbar et al. (2013)

59.0%4

Romantic partners

59.0%5
58.4%6

8

Friends

64.2% to 65.4%7

No interaction

52.6%1

Observed interaction with third party

54.0%1

Mere interaction with judge

52.8%1

Probing questions

49.0% to 57%8

Cognitive load inductions
  reverse order
   keep eye contact
  unanticipated questions

58%9
53.5%10
58.7%11

Nonverbal training to detect lies

58.0%12

Placebo training to detect lies

55.9%13

Timothy R. Levine and Steven A. McCornack, “Behavioral Adaptation, Confidence, and Heuristic-

Based Explanations of the Probing Effect,” Human Communication Research 27, no. 4 (October 2001):
471–502.
9

Aldert Vrij, Samantha A. Mann, Ronald P. Fisher, Rebecca Milne, and Ray Bull, “Increasing Cog-

nitive Load to Facilitate Lie Detection: The Benefit of Recalling an Event in Reverse Order,” Law and
Human Behavior 32, no. 3 (June 2008): 253–65, https://doi.org/10.1007/s10979-007-9103-y.
10

Aldert Vrij, Samantha Mann, Sharon Leal, and Ronald Fisher, “‘Look into My Eyes’: Can an In-

struction to Maintain Eye Contact Facilitate Lie Detection?” Psychology, Crime & Law 16, no. 4 (2010):
327–48, https://doi.org/10.1080/10683160902740633.
11

Meiling Liu, Pär Anders Granhag, Sara Landstrom, Emma Roos af Hjelmsater, Leif Strömwall,

and Aldert Vrij, “‘Can You Remember What Was in Your Pocket When You Were Stung by a Bee?’
Eliciting Cues to Deception by Asking the Unanticipated,” Open Criminology Journal 3, no. 3 (June
2010): 31–36, https://doi:10.2174/1874917801003010031.
12

Mark G. Frank and Thomas Hugh Feeley, “To Catch a Liar: Challenges for Research in Lie Detec-

tion Training,” Journal of Applied Communication Research 31, no. 1 (February 2003): 58–75, https://
doi.org/10.1080/00909880305377.
13

Timothy R. Levine, Thomas Hugh Feeley, Steven A. McCornack, Mikayla Hughes, and Chad M.

Harms, “Testing the Effects of Nonverbal Training on Deception Detection Accuracy with the Inclusion of a Bogus Training Control Group,” Western Journal of Communication 69, no. 3 (July 2005):
203–17, https://doi.org/10.1080/10570310500202355.

42

CHAPTER THREE

types of media. For example, vocal pitch is not available in text-based communication, and body language is not available in audio-only or text-based media.
Thus, many theories predict differences in deception detection accuracy depending on the type of medium because different media have different affordances that provide access to different types of deception cues.
More recently, with the advent of the internet, social networks, smartphones,
and various other new media, interest in media differences has been renewed
and intensified. The presumption seems to be that new media have changed
everything in communication. Each medium has different affordances, and
conclusions from one medium may not apply to other media. The lack of differences observed in various old media has not deterred reexamination with
various new media.4
Historically, Maier and Thurber were among the first to investigate media
differences in deception detection.5 They reported much higher accuracy for
text and audio-only communication (77%) than for audio-visual communication (58%). They proposed that being able to see visual cues distracted wouldbe lie detectors, and advised against direct visual observation in lie detection.
By the time the first meta-analysis was reported in 1981, however, the issue
of media effects and cue exposure became more complex, and the differences
between media were smaller.6 Accuracy was above chance across media with
the exception of video-only, face-only presentations. Across conditions, accuracy was better than chance at d = 0.68.7 Generally, exposure to the face reduced accuracy (with face, d = 0.60; without face, d = 0.75; face-only, d = 0.05).
In contrast, visual exposure to the sender’s body improved accuracy from d =
0.53 to d = 0.82. Exposure to speech (audio and content) had the largest effects,
with access to speech leading to much-improved accuracy (d = 1.14) relative
to visual-only conditions (d = 0.21). Access to tone of voice only (d = .20) and
transcripts-only (d = .70) were also above chance, with transcripts-only yielding accuracy below full-audio (d = 1.14). The highest accuracy was obtained
from speech-and-body-without-face (d = 1.49). The conclusions were that media
made a difference, and that verbal content, vocal and paralinguistic information, and the observation of body language all improved accuracy. Only observation of the face was unhelpful.
Over time, however, those effects mostly dissipated. By the time of the 2006
Bond–DePaulo meta-analysis, only one difference mattered. Visual-only detection was no better than chance, while the slightly-better-than-chance accuracy conclusion held otherwise. As we can see in Table 3.1, when comparing
the 54% across-study average, accuracy is (with rounding) 54% with audio-

DECEPTION DETECTION ACCURACY

43

only, 54% with audio-visual, 53% with face-to-face communication and 53%
with no interaction.
COMMUNICATION DURATION
Conceptually similar to the thinking regarding media/modality effects, arguments are sometimes made for interview duration.8 Just as some media allow
for more cues than others, longer communication duration allows for more
cues to emerge. This line of reasoning holds that poor accuracy is expected
from brief communications, but improved accuracy is possible in longer, more
cue-rich communications. At least in my own research, however, interview duration is not correlated with accuracy.9 Longer exposure to a greater number
of cues does not improve accuracy if those cues have little diagnostic value.10
LIE STAKES
Key theories also predict that sender motivation, or “stakes,” is a critical factor in lie detection. An influential theory specifies stakes as a critical prerequisite for deception detection accuracy. The logic holds that deception cues
arise from lies of consequence. The deception detection research community
has accepted this claim uncritically. Yet accuracy rounds to 53% for both motivated senders and unmotivated senders.11 The existing evidence does not support the idea that lie stakes meaningfully affect accuracy.
PLANNING
One might think that planned lies would be different than spontaneous lies.
Planning and preparation might make lies harder to detect, while spontaneous
lies might be more transparent. Yet this does not seem to be the case either.
Planned lies are detected with 54% accuracy, compared to 53% for spontaneous lies.12 Again, accuracy is slightly better than chance regardless of planning and preparation.
BASELINE BEHAVIORS
Cue-based approaches to deception detection also predict that prior exposure
to honest baseline communication should improve deception detection accuracy. The idea is that individuals have different communication styles, and assessing a person’s behaviors while they’re being honest enables the observation of later changes attributable to the act of lying. Observation of baseline
behaviors significantly improves accuracy (d = 0.24), but in terms of raw accuracy, that is an improvement from 53% to 55%.13 Again, the trend holds.

44

CHAPTER THREE

Accuracy is slightly-better-than-chance regardless of prior exposure to baseline honest behavior.
STUDENTS VS. EXPERTS
I wish I had a dollar for every time I have heard the “used-college-students-
as-research-subjects” criticism applied to some social scientific finding or another. There is often a concern that findings originating from college student
samples will not generalize to the broader population. In deception detection experiments, however, the characteristics of research subjects making the
judgments make little difference in accuracy. I mentioned previously that in
dividual differences in performance were small, leading to small standard errors and highly significant effects. This is true both within and across samples
of judges. One meta-analysis estimates that individual differences in lie detection ability make less than a 1% difference in the percent-correct accuracy
scores across studies.14 A second meta-analysis found that judge age, experience, education, and personality were not associated with deception detection accuracy.15 That same meta-analysis found no sex differences in lie detection ability and that students were not any different from people with careers
in law enforcement. A third meta-analytic test reported mixed findings.16 In
twenty experiments involving head-to-head comparisons between expert and
nonexpert judges, no differences were observed (d = -0.03). However, forty-t wo
samples of expert judges produced slightly but statistically significant (p < .05)
higher accuracy (55%) than 250 samples involving nonexpert judges (53%).
In a couple experiments where experts actually did the questioning, accuracy
was higher still (57% and 59%) but remained in the range of slightly-better-
than-chance accuracy that typifies the larger literature.17
RELATIONSHIP BETWEEN THE LIAR AND THE JUDGE
The issue of the relationship between the sender and the judge has been
studied and presents something of a paradox. Common sense and folk wisdom tell us that knowing a sender should facilitate deception detection accuracy. When we know another person and we know how they usually act, we
may know how they act when they are lying (presuming the existence of idiosyncratic deception cues that would be invisible to meta-analysis), and we have
knowledge about people we know that may make communication content useful.18 But, as McCornack and Parks pointed out, relationships involve trust, and
trust can blind us to lies.19
The goal of their experiment was to assess how relational closeness affected
deception detection accuracy. They predicted that as people became closer,

DECEPTION DETECTION ACCURACY

45

Figure 3.1. McCornack and Parks’s model of relational
closeness and deception detection.

they became more confident in their ability to read their partners. The increase in confidence produced truth-bias, that is, a tendency to believe their
partner. Truth-bias lowered accuracy. This series of causal links has come to be
known as the McCornack and Parks Model of Relationship Deception, which
is depicted in figure 3.1.
McCornack and Parks brought dating partners into the lab, separated them,
and made one the sender. The sender was videotaped making honest and deceptive answers to a series of opinion questions. The other partner, the judge,
then watched the videotaped answers, made a truth–lie judgment for each,
and also rated how confident they were in each judgment. In line with the
model, the closer the relationship, the higher the ratings of confidence; the
more confident, the greater the proportion of messages judged as honest, and
the more truth-bias was negatively associated with the percent of judgments
that were correct. These findings were subsequently replicated,20 and the links
in the model have been supported by meta-analysis.21 McCornack and Parks’s
model is well-documented.
There is, however, a catch. In the McCornack and Parks model, relationship
closeness, albeit indirectly, reduces accuracy. Experiments using dating couples
often report accuracy levels above the meta-analysis average. For example, accuracy was 59% in the McCornack and Parks original, and 58% in a follow-up
McCornack and I did using dating partners as subjects.22 And, in a recent series of experiments involving friends and strangers in a face-to-face lie detection task, friends (64% to 65%) performed substantially better than strangers
(48% to 55%).23 I am not entirely sure what to make of this. Accuracy is still
in the slightly-better-than-chance range, but it seems consistently better than
the 54% average, even though the closeness-trust link is well-documented.24
INTERACTION BETWEEN LIAR AND DUPE
Yet another variable that had been investigated with respect to accuracy is
whether or not there is conversational back-and-forth between the sender and
the judge. Meta-analysis finds little difference in accuracy between no interaction (53%), interaction between sender and judge (53%), and a judge ob-

46

CHAPTER THREE

serving interaction between the sender and some other person (54%).25 Several
studies have investigated the related issue of asking probing questions.26 The
research consistently finds that merely asking probing questions (or hearing
probing questions asked) does not affect accuracy either way, but the probed
senders are more likely to be believed than senders who are not questioned.
Steve McCornack and I call this finding “the probing effect,” and we spent
much time early in our research careers testing various explanations for the
effect.27 For the present discussion, however, it is sufficient to note that mere
interaction and mere question asking have negligible impact on deception detection accuracy.
TRAINING
Various approaches have been used to train people to be better lie detectors,
and three meta-analyses summarize the evidence for the efficacy of deception
detection training.28 All three meta-analyses conclude that training significantly improves accuracy over no-training controls. In the 2003 analysis, accuracy in controls matched the 54% accuracy in the literature as a whole compared to 58% for trained judges. The effect size for the improvement was d =
0.41. In the 2012 meta-analysis, the effect size was slightly larger (d = 0.50).
The evidence, however, might be construed as suggesting that training just
made judges more cynical. Accuracy for lies-only improved from 49% to 55%
with training, but accuracy on truths dipped slightly, from 58% to 56%.29 The
most recent analysis, too, found that gains were limited to accuracy for lies;
there was no improvement for truth accuracy.30 I have found this in my own
experiments.31
Thus, evidence suggests that training improves accuracy by 4% or 5%, but
there are three big caveats. First, if the improvement from training is taken at
face value, accuracy remains well within the slightly-better-than-chance range
that is typical with training or not. This is a difference between getting eleven
out of twenty right compared to twelve out of twenty correct. It is not very impressive. Second, training appears to affect cynicism more than ability, a finding which I find worrisome (more on this point shortly). Third, the gains are
improvements over no-training controls. A more scientifically defensible design might involve a placebo control.
In 2005 I published one of my favorite deception experiments (actually,
a series of experiments) testing my concerns about the deception detection
training evaluation literature.32 The experiments compared three groups of
judges. Some judges received no training, some got valid training, and others

DECEPTION DETECTION ACCURACY

47

were assigned to a placebo training control group. Valid training and placebo
training were created in two ways: either based on the research at the time or
based on coding of the cues that were actually in the specific truths and lies
that composed the test materials. Valid training improved accuracy only when
the training was based on cues actually present in and idiosyncratic to the materials used in the experiment. Training based on prior research findings did
not help. In the case where valid training was effective, it produced an eight-
point gain in accuracy over the no-training control (50% to 58%), but only a
marginal improvement of the placebo control (56% vs. 58%). Further, my suspicions about training producing skepticism were borne out. Both valid training and bogus training produced a greater proportion of lie judgments than
the no-training controls (d = 0.81). Just as in the Frank–Feeley and Hauch
meta-analyses, training improvements emerged only for lies, not for truths.
TRUTH AND LIE ACCURACY
The training literature raises an important issue that I have so far skirted.
When we have been talking about “deception detection accuracy,” we have
been talking about the percentage of correct truth–lie classifications averaged
across truths and lies. This can be quite misleading, because the percentage
correct for truths is often quite different from the percentage correct for lies.
This is because people are typically truth-biased. We will spend much of chapters 11 and 12 on the topic of truth-bias, its causes, and its implications. For
now, I need to mention that any variable that affects truth-bias will likely affect truth accuracy and lie accuracy in the opposite ways. As the proportion of
messages judged as true increases, truth accuracy increases, and accuracy for
lies decreases.33 And, while overall accuracy is pretty stable from study to study,
truth-bias fluctuates more substantially.34 For this reason, when reporting raw
accuracy (percentage correct), I often find it useful to present total accuracy,
truth-bias, accuracy for truths only, and accuracy for lies only.
Recall that in meta-analysis the average accuracy across truths and lies is
54%. For truths only, accuracy improves to 61%; and for lies only, it dips to
47%.35 Note that the average of truth and lie accuracy (61 + 47 = 108; 108 ÷ 2 =
54) is the overall average, and the overall average masks the difference between truths and lies. This observation led to the “veracity effect,” which is
the focus of chapter 12.
SIGNAL DETECTION
Because raw accuracy conflates the ability to correctly discriminate truths and
lies with bias, some researchers prefer to report findings in metrics from signal

48

CHAPTER THREE

detection theory. Deception detection using signal detection measures typically
reports “hits,” “false alarms,” d′ (d-prime, or sensitivity), and bias. Hits are correctly identified lies, which are identical to what I call lie accuracy. False alarms
are erroneously classified truths (or 1 minus truth accuracy). Sensitivity, or d′,
is a statistical measure of accuracy controlling for bias and chance. Bias is a
measure of the extent to which errors tip in the direction of false positives or
false negatives.
In deception detection research, the average d′ is .24, and d′ correlates with
raw percentage correct at r = .99.36 I prefer raw accuracy (qualified by truth-
bias and truth and lie accuracy scored separately) because I think it makes research findings more understandable. If I say accuracy is 54%, that is more
meaningful to most people than saying accuracy is d′ = .24. Further, if I report my results in signal detection metrics only, then readers who don’t know
signal detection math are disenfranchised. But if I report truth and lie accuracy, people who actually know and understand signal detection have all the
information they need to calculate their preferred metrics. Because signal detection sensitivity and raw accuracy are so highly correlated in deception detection literature, I see little to gain from signal detection.
SCALING HONESTY AND DECEIT
Another way accuracy can be obscured is by scaling honesty ratings rather
than using dichotomous truth–lie measures. With dichotomous, forced-choice
truth–lie assessment, overall accuracy, accuracy for truths only, accuracy for
lies only, and truth-bias (percentage true) are easily scored and interpreted (or
converted to signal detection metrics if that is preferred). Some research, however, has people rate messages on scales from honest to deceptive. If people
rate honest messages as more honest than lies, then they are to some extent
accurate. It is hard to know how accurate. So, for example, if ratings are on a
seven-point scale, where 1 is totally lying and 7 is completely honest, the average for honest messages is 5.8, and the average for lies is 5.2, and these two
means are significantly different, then are people good at lie detection? Other
times, researchers will ask senders to rate their own honesty, and the judge’s
ratings will be subtracted from sender ratings to measure accuracy. How accurate is a discrepancy score of 1.8?
The reason I think scaling honesty obfuscates findings is that it allows researchers to pass off statistical differences that might translate into very small
differences in percentage correct without putting the differences into a well-
understood metric or into the context of the larger literature. With percentage
correct, we can compare outcomes not only to chance but also against the 54%

DECEPTION DETECTION ACCURACY

49

across-study average and the 45% to 65% range within which most results fall.
With scaled honesty, we need to calculate effect sizes, and convert effect sizes
into comparable units, and only then can we put findings into perspective.
I have other objections to scaling honesty in deception detection experiments beyond just clarity and transparency. I think scaling honesty tends to
confound several conceptually distinct ideas. I worry that such scaling conflates perceptions of honesty with confidence, and both of these with judgments of moral condemnation. I also worry that scaled honesty measures further conflate message features (e.g., proportion of message content that is false
or misleading) with perceptions of deceptive intent. So, for example, on the 1
to 7 scale (7 honest), if a subject rates a message as “3,” what does that mean?
Does it mean that they think it might be a lie but are not very sure? Does it
mean that it is a lie, but not a big, bad lie? Does it mean that it is partly false
and partly true? It could, I believe, mean any of these things, and research
consumers should be highly skeptical of research claims based on ambiguous
scaling of this sort.
It has been asserted for reasons that have never made sense to me that low
accuracy in deception detection experiments may be a measurement artifact
of dichotomous truth–lie judgments and that using honesty scaling produces
higher accuracy.37 It might be that using scaled honesty lets research find “significant results” without having to acknowledge the 50-something-percent accuracy. But, as a matter of empirical fact, accuracy is actually lower with scales
than dichotomous truth–lie assessments.38 The mean effect size reflecting improvement over chance is .42 for dichotomous measures and .34 for rating
scales, a difference that is statistically significant at p < .05!
UNCONSCIOUS AND INDIRECT LIE DETECTION
A belief similar to the idea that accuracy is higher when scaled is the idea of
implicit, indirect, less-conscious lie detection. So far we have been discussing
explicit or direct lie detection experiments in which people are instructed to
assess messages for honesty, most often with an either/or, truth–lie choice.
As we know, accuracy in such experiments hovers around 54% and is not very
impressive. The idea behind implicit deception is that while people may be
poor lie detectors in direct lie detection tasks, maybe they are better at some
quasi-conscious or unconscious level. In the language of Daniel Kanheman,39
maybe system two (slow, deliberate, analytical) is a poor lie detector, but maybe
system one (fast, intuitive, largely unconscious) picks up on lie clues that sys
tem two misses.
To my knowledge, the first suggestion for the superiority of implicit lie de-

50

CHAPTER THREE

tection was Bella DePaulo’s 1997 meta-analysis of confidence and deception
detection accuracy. That meta-analysis found that people rated themselves as
more confident in their assessments when judging truths than lies. DePaulo
and colleagues describe their findings and conclusions this way:
We also tested the hypothesis that feelings of confidence might function
as measures of indirect deception detection, in that they might differentiate truths from lies. The meta-analysis of eight tests of this hypothesis
produced support for this idea. Judges were substantially more confident
when judging truths than lies. . . . Measures of indirect deception detection hold great promise in this field in which explicit measures of deception detection often yield unimpressive levels of accuracy. Judges who appear totally unable to distinguish truths from lies based on their explicit
judgments may show some evidence of accurate discrimination based
on indirect measures. Such judges may not realize that the discriminations they are making are relevant to deception, or they may simply be
unwilling to explicitly call another person a liar.40
Since then, many authors have asserted the superiority of indirect over direct measures of deception detection.41 Recently, an article in Psychological
Science concluded: “Across two experiments, indirect measures of accuracy in
deception detection were superior to traditional, direct measures. These results provide strong evidence for the idea that although humans cannot consciously discriminate liars from truth tellers, they do have a sense, on some
less-conscious level, of when someone is lying.”42
I am skeptical of the superiority of indirect lie detection. Both quotes above
conclude that direct deception is no better than chance. This is just false. It is
a straw man argument. People can directly and consciously discriminate between truths and lies with a great degree of statistical certainty. What I think is
going on is an apples-and-oranges comparison. Indirect measures of accuracy
are being evaluated based on significance tests. Statistically significant differences are found between truths and lies with a variety of indirect measures.
Direct measures, on the other hand, are evaluated on the basis of percentage
right. Fifty-four percent seems pretty darn poor. People forget that there are
substantial statistical differences between 50% chance and 54%, and accuracy
based on direct assessments gets morphed from the factually accurate “slightly-
better-than-chance” to the empirically false “totally unable.”
Along with Charlie Bond and Maria Hartwig, I recently looked at indirect lie

DECEPTION DETECTION ACCURACY

51

detection effects with meta-analysis.43 Based on 130 different estimates spanning twenty-four different indirect measures, indirect detection was significantly better than chance (d = .23), but it was not better than direct measures
(d = .42). Based on these data, Charlie Bond and I wrote a reply to the Psychological Science article touting the advantage of indirect detection.44
MERE MEASUREMENT RELIABILITY
Here is another of those unappreciated but important findings. In the 2006
Bond–DePaulo meta-analysis, the best predictor by far of the accuracy reported in any given experiment was the number of judgments upon which
the finding was based. The big predictor was not some theoretically important variable like cue availability, extent of interaction, sender stakes, or judge
expertise. The best predictor was a methodological consideration. The greater
the number of judgments in an experiment, the more closely the results approached the across-study mean of 54% accuracy. All the findings that are on
the tails of the distribution in figure 1.1 have one thing in common. They are
all based on a relatively few number of judgments.45
When plotting findings by the number of judgments, as Bond and DePaulo
did, the resulting graph is called a funnel plot. To the extent that study-to-study
differences are just random fluctuations, such plots look like a funnel. The
principle is that there is more random variability in small sample data than in
large sample data. This is related to the idea popularly known as the Law of
Large Numbers.46 Random error, however, does not show a linear decrease as
sample size increases. It is more of a curve (standard error involves the square
root of N, the sample size). Deception detection findings, when graphed in
this way, form a nicely symmetrical funnel. The differences in findings from
study to study are mostly just random noise. This finding as much as any
other poses a huge challenge for deception theory. Why don’t variables such
as cue availability, extent of interaction, sender motivation, or judge expertise
matter much? Why is slightly-better-than-chance accuracy so darn pervasive?
This should also make us skeptical of non-normative accuracy findings based
on small numbers of judgments.47
DISPERSION IN DETAIL
So far, we have been discussing accuracy mostly in terms of central tendency;
that is, the average accuracy is such-and-such under this or that condition or
circumstance. Our discussion of prior accuracy findings closes with thinking
about deception detection in terms of variability or dispersion. In my view,

52

CHAPTER THREE

Figure 3.2. Variability in judgments of honesty and deception.

thinking about sources or components of variation in truth and lie judgments
provides important and underappreciated clues about what is going on in deception detection experiments.
We know that on average people are more likely to believe others than to infer a lie (truth-bias) and that people are correct, on average, about 54% of the
time. If we believe someone when they tell us something, we might believe
them because we tend to believe others, or we might believe them because they
seem honest. When we think someone might be lying, that might be because
of us (how we are seeing things), or it might be because the liar did something that made them seem dishonest. When we correctly distinguish truth
from lie, it could be because we have some skill at lie detection, or it might be
because the person we are observing is a poor liar. In short, both belief and
accuracy can vary due to either the sender or the person judging the sender.
I use the terms “ability” and “truth-bias” to describe differences among
judges, and the terms “transparency” and “honest demeanor” to describe differences among senders. Differences in accuracy could be because of judge
ability or sender transparency. Differences in honest and lie assessments might
be because of judge truth-bias or sender honest demeanor. The four sources
of variability are mapped out in figure 3.2
As it turns out, these four dimensions of variability differ a lot from one
another in the amount they vary.48 Generally, senders vary more than judges.
Also, the proportions of messages judged as honest or deceptive vary more
than the proportions of judgments that are correct or incorrect. So, the most
variance is observed in sender honest demeanor, and the least amount of variance is in judge ability. Variance in truth-bias and transparency falls in between
these two. This means that there are big differences in how believable some
senders are. Some people are almost always believed, others are seen as dis-

DECEPTION DETECTION ACCURACY

53

honest, and these differences have nothing to do with actual honesty. There
are some differences in how gullible judges are and some difference in how
inscrutable senders are. These differences are quite a bit less pronounced than
the differences in honest demeanor, but they are quite a bit larger than the differences in judge ability. As mentioned previously, differences in judge ability
contribute very little to accuracy results. The lack of judge variation in ability
comports with the small standard errors in deception detection research that
were mentioned at the beginning of this chapter.
Understanding differences in variability provides critical insight into how
we might best explain prior findings. As a general scientific rule, where there
is variance, there is the potential for cause and effect. The variance is where
the action is. Since the variance is more in senders than in judges, attention
might be placed there for clues as to what is going on.
SUMMARY
The purpose of chapters 2 and 3 is to summarize findings of prior research on
deception detection. Emphasis is on conclusions drawn from meta-analysis,
so that we might focus on the big picture and trends over time, rather than
getting caught up in idiosyncratic findings. This chapter answered two important questions:
How accurate are people at distinguishing truths from lies? People are
slightly better than chance at distinguishing truths from lies in deception detection experiments. Accuracy is better than chance, but not by much. The
across-study average is about 54% correct truth–lie discrimination.
Under what conditions (if any) are people more accurate or less accurate at
lie detection, and what types of people (if any) are more skilled or less skilled
lie detectors? The slightly-better-than-chance accuracy is remarkably robust
and invariant. Some things make a difference of a few percentage points this
way or that, but the slightly-better-than-chance holds across a wide range of
conditions and methods.
Besides answering these two critical questions, this chapter also highlights
some important but underappreciated findings. One of these is the small standard errors in deception detection experiments involving multiple judgments
per judge. The implication is that even small differences in raw accuracy can
be statistically significant with ample effect sizes. Findings need to be understood in context. Second, the number of judgments strongly impacts the results, making unusual results based on small data untrustworthy. Third, raw
accuracy (i.e., correct truth–lie discrimination) and accuracy for lies are not the
same thing. The implication is that if people are better than chance at truth–

54

CHAPTER THREE

lie discrimination, this does not mean that they are better than chance at recognizing lies per se. Finally, there is much more variability in senders than in
judges. This suggests that viable explanations for findings need to account for
both sender variability and judge constancy.
In chapter 4, various prior theories of deception and deception detection
are reviewed. We can see how well these theories align with the actual findings on cues and accuracy.
