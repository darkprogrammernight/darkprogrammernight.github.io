CHAPTER TWELVE

Investigating Social Dynamics: Power, Conformity, and Obedience

I believe that in all men’s lives at certain periods, and in many men’s lives at all periods between infancy and extreme old age, one of the most dominant elements is the desire to be inside the local Ring and the terror of being left outside . . . . Of all the passions the passion for the Inner Ring is most skilful in making a man who is not yet a very bad man do very bad things.

—C. S. Lewis, “The Inner Ring” (1944)1



Motives and needs that ordinarily serve us well can lead us astray when they are aroused, amplified, or manipulated by situational forces that we fail to recognize as potent. This is why evil is so pervasive. Its temptation is just a small turn away, a slight detour on the path of life, a blur in our sideview mirror, leading to disaster.

In trying to understand the character transformations of the good young men in the Stanford Prison Experiment, I previously outlined a number of psychological processes that were pivotal in perverting their thoughts, feelings, perceptions, and actions. We saw how the basic need to belong, to associate with and be accepted by others, so central to community building and family bonding, was diverted in the SPE into conformity with newly emergent norms that enabled the guards to abuse the prisoners.2 We saw further that the basic motive for consistency between our private attitudes and public behavior allowed for dissonant commitments to be resolved and rationalized in violence against one’s fellows.3

I will argue that the most dramatic instances of directed behavior change and “mind control” are not the consequence of exotic forms of influence, such as hypnosis, psychotropic drugs, or “brainwashing,” but rather the systematic manipulation of the most mundane aspects of human nature over time in confining settings.4

It is in this sense, I believe what the English scholar C. S. Lewis proposed—that a powerful force in transforming human behavior, pushing people across the boundary between good and evil, comes from the basic desire to be “in” and not “out.” If we think of social power as arrayed in a set of concentric circles from the most powerful central or inner ring moving outward to the least socially significant outer ring, we can appreciate his focus on the centripetal pull of that central circle. Lewis’s “Inner Ring” is the elusive Camelot of acceptance into some special group, some privileged association, that confers instant status and enhanced identity. Its lure for most of us is obvious—who does not want to be a member of the “in-group”? Who does not want to know that she or he has been tried and found worthy of inclusion in, of ascendance into, a new, rarified realm of social acceptability?

Peer pressure has been identified as one social force that makes people, especially adolescents, do strange things—anything—to be accepted. However, the quest for the Inner Ring is nurtured from within. There is no peer-pressure power without that push from self-pressure for Them to want You. It makes people willing to suffer through painful, humiliating initiation rites in fraternities, cults, social clubs, or the military. It justifies for many suffering a lifelong existence climbing the corporate ladder.

This motivational force is doubly energized by what Lewis called the “terror of being left outside.” This fear of rejection when one wants acceptance can cripple initiative and negate personal autonomy. It can turn social animals into shy introverts. The imagined threat of being cast into the out-group can lead some people to do virtually anything to avoid their terrifying rejection. Authorities can command total obedience not through punishments or rewards but by means of the double-edged weapon: the lure of acceptance coupled with the threat of rejection. So strong is this human motive that even strangers are empowered when they promise us a special place at their table of shared secrets—“just between you and me.”5

A sordid example of these social dynamics came to light recently when a forty-year-old woman pleaded guilty to having sex with five high school boys and providing them and others with drugs and alcohol at weekly sex parties in her home for a full year. She told police that she had done it because she wanted to be a “cool mom.” In her affidavit, this newly cool mom told investigators that she had never been popular with her classmates in high school, but orchestrating these parties enabled her to begin “feeling like one of the group.”6 Sadly, she caught the wrong Inner Ring.

Lewis goes on to describe the subtle process of initiation, the indoctrination of good people into a private Inner Ring that can have malevolent consequences, turning them into “scoundrels.” I cite this passage at length because it is such an eloquent expression of how this basic human motive can be imperceptibly perverted by those with the power to admit or deny access to their Inner Ring. It will set the stage for our excursion into the experimental laboratories and field settings of social scientists who have investigated such phenomena in considerable depth.



To nine out of ten of you the choice which could lead to scoundrelism will come, when it does come, in no very dramatic colors. Obviously bad men, obviously threatening or bribing, will almost certainly not appear. Over a drink or a cup of coffee, disguised as a triviality and sandwiched between two jokes, from the lips of a man, or woman, whom you have recently been getting to know rather better and whom you hope to know better still—just at the moment when you are most anxious not to appear crude, or naive or a prig—the hint will come. It will be the hint of something, which is not quite in accordance with the technical rules of fair play, something that the public, the ignorant, romantic public, would never understand. Something which even the outsiders in your own profession are apt to make a fuss about, but something, says your new friend, which “we”—and at the word “we” you try not to blush for mere pleasure—something “we always do.” And you will be drawn in, if you are drawn in, not by desire for gain or ease, but simply because at that moment, when the cup was so near your lips, you cannot bear to be thrust back again into the cold outer world. It would be so terrible to see the other man’s face—that genial, confidential, delightfully sophisticated face—turn suddenly cold and contemptuous, to know that you had been tried for the Inner Ring and rejected. And then, if you are drawn in, next week it will be something a little further from the rules, and next year something further still, but all in the jolliest, friendliest spirit. It may end in a crash, a scandal, and penal servitude; it may end in millions, a peerage and giving the prizes at your old school. But you will be a scoundrel.




RESEARCH REVELATIONS OF SITUATIONAL POWER

The Stanford Prison Experiment is a facet of the broad mosaic of research that reveals the power of social situations and the social construction of reality. We have seen how it focused on power relationships among individuals within an institutional setting. A variety of studies that preceded and followed it have illuminated many other aspects of human behavior that are shaped in unexpected ways by situational forces.

Groups can get us to do things we ordinarily might not do on our own, but their influence is often indirect, simply modeling the normative behavior that the group wants us to imitate and practice. In contrast, authority influence is more often direct and without subtlety: “You do what I tell you to do.” But because the demand is so open and bold-faced, one can decide to disobey and not follow the leader. To see what I mean, consider this question: To what extent would a good, ordinary person resist against or comply with the demand of an authority figure that he harm, or even kill, an innocent stranger? This provocative question was put to experimental test in a controversial study on blind obedience to authority. It is a classic experiment about which you have probably heard because of its “shocking” effects, but there is much more of value embedded in its procedures that we will extract to aid in our quest to understand why good people can be induced to behave badly. We will review replications and extensions of this classic study and again ask the question posed of all such research: What is its external validity, what are real-world parallels to the laboratory demonstration of authority power?

Beware: Self-Serving Biases May Be at Work

Before we get into the details of this research, I must warn you of a bias you likely possess that might shield you from drawing the right conclusions from all you are about to read. Most of us construct self-enhancing, self-serving, egocentric biases that make us feel special—never ordinary, and certainly “above average.”7 Such cognitive biases serve a valuable function in boosting our self-esteem and protecting against life’s hard knocks. They enable us to explain away failures, take credit for our successes, and disown responsibility for bad decisions, perceiving our subjective world through rainbow prisms. For example, research shows that 86 percent of Australians rate their job performance as “above average,” and 90 percent of American business managers rate their performance as superior to that of their average peer. (Pity that poor average dude.)

Yet these biases can be maladaptive as well by blinding us to our similarity to others and distancing us from the reality that people just like us behave badly in certain toxic situations. Such biases also mean that we don’t take basic precautions to avoid the undesired consequences of our behavior, assuming it won’t happen to us. So we take sexual risks, driving risks, gambling risks, health risks, and more. In the extreme version of these biases, most people believe that they are less vulnerable to these self-serving biases than other people, even after being taught about them.8

That means when you read about the SPE or the many studies in this next section, you might well conclude that you would not do what the majority has done, that you would, of course, be the exception to the rule. That statistically unreasonable belief (since most of us share it) makes you even more vulnerable to situational forces precisely because you underestimate their power as you overestimate yours. You are convinced that you would be the good guard, the defiant prisoner, the resistor, the dissident, the nonconformist, and, most of all, the Hero. Would that it were so, but heroes are a rare breed—some of whom we will meet in our final chapter.

So I invite you to suspend that bias for now and imagine that what the majority has done in these experiments is a fair base rate for you as well. At the very least, please consider that you can’t be certain of whether or not you could be as readily seduced into doing what the average research participant has done in these studies—if you were in their shoes, under the same circumstances. I ask you to recall what Prisoner Clay-416, the sausage resister, said in his postexperimental interview with his tormenter, the “John Wayne” guard. When taunted with “What kind of guard would you have been if you were in my place?” he replied modestly, “I really don’t know.”

It is only through recognizing that we are all subject to the same dynamic forces in the human condition, that humility takes precedence over unfounded pride, that we can begin to acknowledge our vulnerability to situational forces. In this vein, recall John Donne’s eloquent framing of our common interrelatedness and interdependence:



All mankind is of one author, and is one volume; when one man dies, one chapter is not torn out of the book, but translated into a better language; and every chapter must be so translated . . . . As therefore the bell that rings to a sermon, calls not upon the preacher only, but upon the congregation to come: so this bell calls us all . . . . No man is an island, entire of itself . . . any man’s death diminishes me, because I am involved in mankind; and therefore never send to know for whom the bell tolls; it tolls for thee.


(Meditations 27)



Classic Research on Conforming to Group Norms

One of the earliest studies on conformity, in 1935, was designed by a social psychologist from Turkey, Muzafer Sherif.9 Sherif, a recent immigrant to the United States, believed that Americans in general tended to conform because their democracy emphasized mutually shared agreements. He devised an unusual means of demonstrating conformity of individuals to group standards in a novel setting.

Male college students were individually ushered into a totally dark room in which there was a stationary spot of light. Sherif knew that without any frame of reference, such a light appears to move about erratically, an illusion called the “autokinetic effect.” At first, each of these subjects was asked individually to judge the movement of the light. Their judgments varied widely; some saw movement of a few inches, while others reported that the spot moved many feet. Each person soon established a range within which most of his reports would fall. Next, he was put into a group with several others. They gave estimates that varied widely, but in each group a norm “crystallized” wherein a range of judgments and an average-norm judgment emerged. After many trials, the other participants left, and the individual, now alone, was asked again to make estimates of the movement of the light—the test of his conformity to the new norm established in that group. His judgments now fell in this new group-sanctioned range, “departing significantly from his earlier personal range.”

Sherif also used a confederate who was trained to give estimates that varied in their latitude from a small to a very large range. Sure enough, the naive subject’s autokinetic experience mirrored that of the judgments of this devious confederate rather than sticking to his previously established personal perceptual standard.

Asch’s Conformity Research: Getting into Line

Sherif’s conformity effect was challenged in 1955 by another social psychologist, Solomon Asch,10 who believed that Americans were actually more independent than Sherif’s work had suggested. Asch believed that Americans could act autonomously, even when faced with a majority who saw the world differently from them. The problem with Sherif’s test situation, he argued, was that it was so ambiguous, without any meaningful frame of reference or personal standard. When challenged by the alternative perception of the group, the individual had no real commitment to his original estimates so just went along. Real conformity required the group to challenge the basic perception and beliefs of the individual—to say that X was Y, when clearly that was not true. Under those circumstances, Asch predicted, relatively few would conform; most would be staunchly resistant to this extreme group pressure that was so transparently wrong.

What actually happened to people confronted with a social reality that conflicted with their basic perceptions of the world? To find out, let me put you into the seat of a typical research participant.

You are recruited for a study of visual perception that begins with judging the relative size of lines. You are shown cards with three lines of differing lengths and asked to state out loud which of the three is the same length as a comparison line on another card. One is shorter, one is longer, and one is exactly the same length as the comparison line. The task is a piece of cake for you. You make few mistakes, just like most others (less than 1 percent of the time). But you are not alone in this study; you are flanked by a bunch of peers, seven of them, and you are number eight. At first, your answers are like theirs—all right on. But then unusual things start to happen. On some trials, each of them in turn reports seeing the long line as the same length as the medium line or the short line the same as the medium one. (Unknown to you, the other seven are members of Asch’s research team who have been instructed to give incorrect answers unanimously on specific “critical” trials.) When it is your turn, they all look at you as you look at the card with the three lines. You are clearly seeing something different than they are, but do you say so? Do you stick to your guns and say what you know is right, or do you go along with what everyone else says is right? You face that same group pressure on twelve of the total eighteen trials where the group gives answers that are wrong, but they are accurate on the other six trials interspersed into the mix.

If you are like most of the 123 actual research participants in Asch’s study, you would yield to the group about 70 percent of the time on some of those critical, wrong-judgment trials. Thirty percent of the original subjects conformed on the majority of trials, and only a quarter of them were able to maintain their independence throughout the testing. Some reported being aware of the differences between what they saw and the group consensus, but they felt it was easier to go along with the others. For others the discrepancy created a conflict that was resolved by coming to believe that the group was right and their perception was wrong! All those who yielded underestimated how much they had conformed, recalling yielding much less to the group pressure than had actually been the case. They remained independent—in their minds but not in their actions.

Follow-up studies showed that, when pitted against just one person giving an incorrect judgment, a participant exhibits some uneasiness but maintains independence. However, with a majority of three people opposed to him, errors rose to 32 percent. On a more optimistic note, however, Asch found one powerful way to promote independence. By giving the subject a partner whose views were in line with his, the power of the majority was greatly diminished. Peer support decreased errors to one fourth of what they had been when there was no partner—and this resistance effect endured even after the partner left.

One of the valuable additions to our understanding of why people conform comes from research that highlights two of the basic mechanisms that contribute to group conformity.11 We conform first out of informational needs: other people often have ideas, views, perspectives, and knowledge that helps us to better navigate our world, especially through foreign shores and new ports. The second mechanism involves normative needs: other people are more likely to accept us when we agree with them than when we disagree, so we yield to their view of the world, driven by a powerful need to belong, to replace differences with similarities.

Conformity and Independence Light Up the Brain Differently

New technology, not available in Asch’s day, offers intriguing insights into the role of the brain in social conformity. When people conform, are they rationally deciding to go along with the group out of normative needs, or are they actually changing their perceptions and accepting the validity of the new though erroneous information provided by the group? A recent study utilized advanced brain-scanning technology to answer this question.12 Researchers can now peer into the active brain as a person engages in various tasks by using a scanning device that detects which specific brain regions are energized as they carry out various mental tasks. The process is known as functional magnetic resonance imaging (FMRI). Understanding what mental functions various brain regions control tells us what it means when they are activated by any given experimental task.

Here’s how the study worked. Imagine that you are one of thirty-two volunteers recruited for a study of perception. You have to mentally rotate images of three-dimensional objects to determine if the objects are the same as or different from a standard object. In the waiting room, you meet four other volunteers, with whom you begin to bond by practicing games on laptop computers, taking photos of one another, and chatting. (They are really actors—“confederates,” as they are called in psychology—who will soon be faking their answers on the test trials so that they are in agreement with one another but not with the correct responses that you generate.) You are selected as the one to go into the scanner while the others outside look at the objects first as a group and then decide if they are the same or different. As in Asch’s original experiment, the actors unanimously give wrong answers on some trials, correct answers on others, with occasional mixed group answers thrown in to make the test more believable. On each round, when it is your turn at bat, you are shown the answers given by the others. You have to decide if the objects are the same or different—as the group assessed them or as you saw them?

As in Asch’s experiments, you (as the typical subject) would cave in to group pressure, on average giving the group’s wrong answers 41 percent of the time. When you yield to the group’s erroneous judgment, your conformity would be seen in the brain scan as changes in selected regions of the brain’s cortex dedicated to vision and spatial awareness (specifically, activity increases in the right intraparietal sulcus). Surprisingly, there would be no changes in areas of the fore-brain that deal with monitoring conflicts, planning, and other higher-order mental activities. On the other hand, if you make independent judgments that go against the group, your brain would light up in the areas that are associated with emotional salience (the right amygdala and right caudate nucleus regions). This means that resistance creates an emotional burden for those who maintain their independence—autonomy comes at a psychic cost.

The lead author of this research, the neuroscientist Gregory Berns, concluded that “We like to think that seeing is believing, but the study’s findings show that seeing is believing what the group tells you to believe.” This means that other people’s views, when crystallized into a group consensus, can actually affect how we perceive important aspects of the external world, thus calling into question the nature of truth itself. It is only by becoming aware of our vulnerability to social pressure that we can begin to build resistance to conformity when it is not in our best interest to yield to the mentality of the herd.

Minority Power to Impact the Majority

Juries can become “hung” when a dissenter gets support from at least one other person and together they challenge the dominant majority view. But can a small minority turn the majority around to create new norms using the same basic psychological principles that usually help to establish the majority view?

A research team of French psychologists put that question to an experimental test. In a color-naming task, if two confederates among groups of six female students consistently called a blue light “green,” almost a third of the naive majority subjects eventually followed their lead. However, the members of the majority did not give in to the consistent minority when they were gathered together. It was only later, when they were tested individually, that they responded as the minority had done, shifting their judgments by moving the boundary between blue and green toward the green of the color spectrum.13

Researchers have also studied minority influence in the context of simulated jury deliberations, where a disagreeing minority prevents unanimous acceptance of the majority point of view. The minority group was never well liked, and its persuasiveness, when it occurred, worked only gradually, over time. The vocal minority was most influential when it had four qualities: it persisted in affirming a consistent position, appeared confident, avoided seeming rigid and dogmatic, and was skilled in social influence. Eventually, the power of the many may be undercut by the persuasion of the dedicated few.

How do these qualities of a dissident minority—especially its persistence—help to sway the majority? Majority decisions tend to be made without engaging the systematic thought and critical thinking skills of the individuals in the group. Given the force of the group’s normative power to shape the opinions of the followers who conform without thinking things through, they are often taken at face value. The persistent minority forces the others to process the relevant information more mindfully.14 Research shows that the decisions of a group as a whole are more thoughtful and creative when there is minority dissent than when it is absent.15

If a minority can win adherents to their side even when they are wrong, there is hope for a minority with a valid cause. In society, the majority tends to be the defender of the status quo, while the force for innovation and change comes from the minority members or individuals either dissatisfied with the current system or able to visualize new and creative alternative ways of dealing with current problems. According to the French social theorist Serge Moscovici,16 the conflict between the entrenched majority view and the dissident minority perspective is an essential precondition of innovation and revolution that can lead to positive social change. An individual is constantly engaged in a two-way exchange with society—adapting to its norms, roles, and status prescriptions but also acting upon society to reshape those norms.


BLIND OBEDIENCE TO AUTHORITY: MILGRAM’S SHOCKING RESEARCH

“I was trying to think of a way to make Asch’s conformity experiment more humanly significant. I was dissatisfied that the test of conformity was judgments about lines. I wondered whether groups could pressure a person into performing an act whose human import was more readily apparent; perhaps behaving aggressively toward another person, say by administering increasingly severe shocks to him. But to study the group effect . . . you’d have to know how the subject performed without any group pressure. At that instant, my thought shifted, zeroing in on this experimental control. Just how far would a person go under the experimenter’s orders?”

These musings, from a former teaching and research assistant of Solomon Asch, started a remarkable series of studies by a social psychologist, Stanley Milgram, that have come to be known as investigations of “blind obedience to authority.” His interest in the problem of obedience to authority came from deep personal concerns about how readily the Nazis had obediently killed Jews during the Holocaust.

“[My] laboratory paradigm . . . gave scientific expression to a more general concern about authority, a concern forced upon members of my generation, in particular upon Jews such as myself, by the atrocities of World War II . . . . The impact of the Holocaust on my own psyche energized my interest in obedience and shaped the particular form in which it was examined.”17

I would like to re-create for you the situation faced by a typical volunteer in this research project, then go on to summarize the results, outline ten important lessons to be drawn from this research that can be generalized to other situations of behavioral transformations in everyday life, and then review extensions of this paradigm by providing a number of real-world parallels. (See the Notes for a description of my personal relationship with Stanley Milgram.18)

Milgram’s Obedience Paradigm

Imagine that you see the following advertisement in the Sunday newspaper and decide to apply. The original study involved only men, but women were used in a later study, so I invite all readers to participate in this imagined scenario.





A researcher whose serious demeanor and gray laboratory coat convey scientific importance greets you and another applicant at your arrival at a Yale University laboratory in Linsly-Chittenden Hall. You are here to help scientific psychology find ways to improve people’s learning and memory through the use of punishment. He tells you why this new research may have important practical consequences. The task is straightforward: one of you will be the “teacher” who gives the “learner” a set of word pairings to memorize. During the test, the teacher gives each key word, and the learner must respond with the correct association. When right, the teacher gives a verbal reward, such as “Good” or “That’s right.” When wrong, the teacher is to press a lever on an impressive-looking shock apparatus that delivers an immediate shock to punish the error.





The shock generator has thirty switches, starting from a low level of 15 volts and increasing by 15 volts at each higher level. The experimenter tells you that every time the learner makes a mistake, you have to press the next higher voltage switch. The control panel indicates both the voltage level of each of the switches and a corresponding description of the level. The tenth level (150 volts) is “Strong Shock”; the 13th level (195 volts) is “Very Strong Shock”; the 17th level (255 volts) is “Intense Shock”; the 21st level (315 volts) is “Extremely Intense Shock”; the 25th level (375 volts) is “Danger, Severe Shock”; and at the 29th and 30th levels (435 and 450 volts) the control panel is simply marked with an ominous XXX (the pornography of ultimate pain and power).

You and another volunteer draw straws to see who will play each role; you are to be the teacher, and the other volunteer will be the learner. (The drawing is rigged, and the other volunteer is a confederate of the experimenter who always plays the learner.) He is a mild-mannered, middle-aged man whom you help escort to the next chamber. “Okay, now we are going to set up the learner so he can get some punishment,” the researcher tells you both. The learner’s arms are strapped down and an electrode is attached to his right wrist. The shock generator in the next room will deliver the shocks to the learner—if and when he makes any errors. The two of you communicate over the intercom, with the experimenter standing next to you. You get a sample shock of 45 volts, the third level, a slight tingly pain, so you now have a sense of what the shock levels mean. The experimenter then signals the start of your trial of the “memory improvement” study.





Initially, your pupil does well, but soon he begins making errors, and you start pressing the shock switches. He complains that the shocks are starting to hurt. You look at the experimenter, who nods to continue. As the shock levels increase in intensity, so do the learner’s screams, saying he does not think he wants to continue. You hesitate and question whether you should go on, but the experimenter insists that you have no choice but to do so.

Now the learner begins complaining about his heart condition and you dissent, but the experimenter still insists that you continue. Errors galore; you plead with your pupil to concentrate to get the right associations, you don’t want to hurt him with these very-high-level, intense shocks. But your concerns and motivational messages are to no avail. He gets the answers wrong again and again. As the shocks intensify, he shouts out, “I can’t stand the pain, let me out of here!” Then he says to the experimenter, “You have no right to keep me here! Let me out!” Another level up, he screams, “I absolutely refuse to answer any more! Get me out of here! You can’t hold me here! My heart’s bothering me!”

Obviously you want nothing more to do with this experiment. You tell the experimenter that you refuse to continue. You are not the kind of person who harms other people in this way. You want out. But the experimenter continues to insist that you go on. He reminds you of the contract, of your agreement to participate fully. Moreover, he claims responsibility for the consequences of your shocking actions. After you press the 300-volt switch, you read the next keyword, but the learner doesn’t answer. “He’s not responding,” you tell the experimenter. You want him to go into the other room and check on the learner to see if he is all right. The experimenter is impassive; he is not going to check on the learner. Instead he tells you, “If the learner doesn’t answer in a reasonable time, about five seconds, consider it wrong,” since errors of omission must be punished in the same way as errors of commission—that is a rule.

As you continue up to even more dangerous shock levels, there is no sound coming from your pupil’s shock chamber. He may be unconscious or worse! You are really distressed and want to quit, but nothing you say works to get your exit from this unexpectedly distressing situation. You are told to follow the rules and keep posing the test items and shocking the errors.

Now try to imagine fully what your participation as the teacher would be. I am sure you are saying, “No way would I ever go all the way!” Obviously, you would have dissented, then disobeyed and just walked out. You would never sell out your morality for four bucks! But had you actually gone all the way to the last of the thirtieth shock levels, the experimenter would have insisted that you repeat that XXX switch two more times, for good measure! Now, that is really rubbing it in your face. Forget it, no sir, no way; you are out of there, right? So how far up the scale do you predict that you would you go before exiting? How far would the average person from this small city go in this situation?

The Outcome Predicted by Expert Judges

Milgram described his experiment to a group of forty psychiatrists and then asked them to estimate the percentage of American citizens who would go to each of the thirty levels in the experiment. On average, they predicted that less than 1 percent would go all the way to the end, that only sadists would engage in such sadistic behavior, and that most people would drop out at the tenth level of 150 volts. They could not have been more wrong! These experts on human behavior were totally wrong because, first, they ignored the situational determinants of behavior in the procedural description of the experiment. Second, their training in traditional psychiatry led them to rely too heavily on the dispositional perspective to understand unusual behavior and to disregard situational factors. They were guilty of making the fundamental attribution error (FAE)!

The Shocking Truth

In fact, in Milgram’s experiment, two of every three (65 percent) of the volunteers went all the way up the maximum shock level of 450 volts. The vast majority of people, the “teachers,” shocked their “learner-victim” over and over again despite his increasingly desperate pleas to stop.

And now I invite you to venture another guess: What was the dropout rate after the shock level reached 330 volts—with only silence coming from the shock chamber, where the learner could reasonably be presumed to be unconscious? Who would go on at that point? Wouldn’t every sensible person quit, drop out, refuse the experimenter’s demands to go on shocking him?

Here is what one “teacher” reported about his reaction: “I didn’t know what the hell was going on. I think, you know, maybe I’m killing this guy. I told the experimenter that I was not taking responsibility for going further. That’s it.” But when the experimenter reassured him that he would take the responsibility, the worried teacher obeyed and continued to the very end.19

And almost everyone who got that far did the same as this man. How is that possible? If they got that far, why did they continue on to the bitter end? One reason for this startling level of obedience may be related to the teacher’s not knowing how to exit from the situation, rather than just blind obedience. Most participants dissented from time to time, saying they did not want to go on, but the experimenter did not let them out, continually coming up with reasons why they had to stay and prodding them to continue testing their suffering learner. Usually protests work and you can get out of unpleasant situations, but nothing you say affects this impervious experimenter, who insists that you must stay and continue to shock errors. You look at the shock panel and realize that the easiest exit lies at the end of the last shock lever. A few more lever presses is the fast way out, with no hassles from the experimenter and no further moans from the now-silent learner. Voilà! 450 volts is the easy way out—achieving your freedom without directly confronting the authority figure or having to reconcile the suffering you have already caused with this additional pain to the victim. It is a simple matter of up and then out.

Variations on an Obedience Theme

Over the course of a year, Milgram carried out nineteen different experiments, each one a different variation of the basic paradigm of: experimenter/teacher/learner/memory testing/errors shocked. In each of these studies he varied one social psychological variable and observed its impact on the extent of obedience to the unjust authority’s pressure to continue to shock the “learner-victim.” In one study, he added women; in others he varied the physical proximity or remoteness of either the experimenter-teacher link or the teacher-learner link; had peers rebel or obey before the teacher had the chance to begin; and more.

In one set of experiments, Milgram wanted to show that his results were not due to the authority power of Yale University—which is what New Haven is all about. So he transplanted his laboratory to a run-down office building in downtown Bridgeport, Connecticut, and repeated the experiment as a project, ostensibly of a private research firm with no apparent connection to Yale. It made no difference; the participants fell under the same spell of this situational power.

The data clearly revealed the extreme pliability of human nature: almost everyone could be totally obedient or almost everyone could resist authority pressures. It all depended on the situational variables they experienced. Milgram was able to demonstrate that compliance rates could soar to over 90 percent of people continuing the 450-volt maximum or be reduced to less than 10 percent—by introducing just one crucial variable into the compliance recipe.

Want maximum obedience? Make the subject a member of a “teaching team,” in which the job of pulling the shock lever to punish the victim is given to another person (a confederate), while the subject assists with other parts of the procedure. Want people to resist authority pressures? Provide social models of peers who rebelled. Participants also refused to deliver the shocks if the learner said he wanted to be shocked; that’s masochistic, and they are not sadists. They were also reluctant to give high levels of shock when the experimenter filled in as the learner. They were more likely to shock when the learner was remote than in proximity. In each of the other variations on this diverse range of ordinary Ameri can citizens, of widely varying ages and occupations and of both genders, it was possible to elicit low, medium, or high levels of compliant obedience with a flick of the situational switch—as if one were simply turning a “human nature dial” within their psyches. This large sample of a thousand ordinary citizens from such varied backgrounds makes the results of the Milgram obedience studies among the most generalizable in all the social sciences.



When you think of the long and gloomy history of man, you will find far more hideous crimes have been committed in the name of obedience than have been committed in the name of rebellion.


—C. P. Snow, “Either-Or” (1961)



Ten Lessons from the Milgram Studies: Creating Evil Traps for Good People

Let’s outline some of the procedures in this research paradigm that seduced many ordinary citizens to engage in this apparently harmful behavior. In doing so, I want to draw parallels to compliance strategies used by “influence professionals” in real-world settings, such as salespeople, cult and military recruiters, media advertisers, and others.20 There are ten methods we can extract from Milgram’s paradigm for this purpose:



1. Prearranging some form of contractual obligation, verbal or written, to control the individual’s behavior in pseudolegal fashion. (In Milgram’s experiment, this was done by publicly agreeing to accept the tasks and the procedures.)

2. Giving participants meaningful roles to play (“teacher,” “learner”) that carry with them previously learned positive values and automatically activate response scripts.

3. Presenting basic rules to be followed that seem to make sense before their actual use but can then be used arbitrarily and impersonally to justify mindless compliance. Also, systems control people by making their rules vague and changing them as necessary but insisting that “rules are rules” and thus must be followed (as the researcher in the lab coat did in Milgram’s experiment or the SPE guards did to force prisoner Clay-416 to eat the sausages).

4. Altering the semantics of the act, the actor, and the action (from “hurting victims” to “helping the experimenter,” punishing the former for the lofty goal of scientific discovery)—replacing unpleasant reality with desirable rhetoric, gilding the frame so that the real picture is disguised. (We can see the same semantic framing at work in advertising, where, for example, bad-tasting mouthwash is framed as good for you because it kills germs and tastes like medicine is expected to taste.)

5. Creating opportunities for the diffusion of responsibility or abdication of responsibility for negative outcomes; others will be responsible, or the actor won’t be held liable. (In Milgram’s experiment, the authority figure said, when questioned by any “teacher,” that he would take responsibility for anything that happened to the “learner.”)

6. Starting the path toward the ultimate evil act with a small, seemingly insignificant first step, the easy “foot in the door” that swings open subsequent greater compliance pressures, and leads down a slippery slope.21 (In the obedience study, the initial shock was only a mild 15 volts.) This is also the operative principle in turning good kids into drug addicts, with that first little hit or sniff.

7. Having successively increasing steps on the pathway that are gradual, so that they are hardly noticeably different from one’s most recent prior action. “Just a little bit more.” (By increasing each level of aggression in gradual steps of only 15-volt increments, over the thirty switches, no new level of harm seemed like a noticeable difference from the prior level to Milgram’s participants.)

8. Gradually changing the nature of the authority figure (the researcher, in Milgram’s study) from initially “just” and reasonable to “unjust” and demanding, even irrational. This tactic elicits initial compliance and later confusion, since we expect consistency from authorities and friends. Not acknowledging that this transformation has occurred leads to mindless obedience (and it is part of many “date rape” scenarios and a reason why abused women stay with their abusing spouses).

9. Making the “exit costs” high and making the process of exiting difficult by allowing verbal dissent (which makes people feel better about themselves) while insisting on behavioral compliance.

10. Offering an ideology, or a big lie, to justify the use of any means to achieve the seemingly desirable, essential goal. (In Milgram’s research this came in the form of providing an acceptable justification, or rationale, for engaging in the undesirable action, such as that science wants to help people improve their memory by judicious use of reward and punishment.) In social psychology experiments, this tactic is known as the “cover story” because it is a cover-up for the procedures that follow, which might be challenged because they do not make sense on their own. The real-world equivalent is known as an “ideology.” Most nations rely on an ideology, typically, “threats to national security,” before going to war or to suppress dissident political opposition. When citizens fear that their national security is being threatened, they become willing to surrender their basic freedoms to a government that offers them that exchange. Erich Fromm’s classic analysis in Escape from Freedom made us aware of this trade-off, which Hitler and other dictators have long used to gain and maintain power: namely, the claim that they will be able to provide security in exchange for citizens giving up their freedoms, which will give them the ability to control things better.22



Such procedures are utilized in varied influence situations where those in authority want others to do their bidding but know that few would engage in the “end game” without first being properly prepared psychologically to do the “unthinkable.” In the future, when you are in a compromising position where your compliance is at stake, thinking back to these stepping-stones to mindless obedience may enable you to step back and not go all the way down the path—their path. A good way to avoid crimes of obedience is to assert one’s personal authority and always take full responsibility for one’s actions.23

Replications and Extensions of the Milgram Obedience Model

Because of its structural design and its detailed protocol, the basic Milgram obedience experiment encouraged replication by independent investigators in many countries. A recent comparative analysis was made of the rates of obedience in eight studies conducted in the United States and nine replications in European, African, and Asian countries. There were comparably high levels of compliance by research volunteers in these different studies and nations. The majority obedience effect of a mean 61 percent found in the U.S. replications was matched by the 66 percent obedience rate found across all the other national samples. The range of obedience went from a low of 31 percent to a high of 91 percent in the U.S. studies, and from a low of 28 percent (Australia) to a high of 88 percent (South Africa) in the cross-national replications. There was also stability of obedience over decades of time as well as over place. There was no association between when a study was done (between 1963 and 1985) and degree of obedience.24

Obedience to a Powerful Legitimate Authority

In the original obedience studies, the subjects conferred authority status on the person conducting the experiment because he was in an institutional setting and was dressed and acted like a serious scientist, even though he was only a high school biology teacher paid to play that role. His power came from being perceived as a representative of an authority system. (In Milgram’s Bridgeport replication described earlier, the absence of the prestigious institutional setting of Yale reduced the obedience rate to 47.5 percent compared to 65 percent at Yale, although this drop was not a statistically significant one.) Several later studies showed how powerful the obedience effect can be when legitimate authorities exercise their power within their power domains.

When a college professor was the authority figure telling college student volunteers that their task was to train a puppy by conditioning its behavior using electric shocks, he elicited 75 percent obedience from them. In this experiment, both the “experimenter-teacher” and the “learner” were “authentic.” That is, college students acted as the teacher, attempting to condition a cuddly little puppy, the learner, in an electrified apparatus. The puppy was supposed to learn a task, and shocks were given when it failed to respond correctly in a given time interval. As in Milgram’s experiments, they had to deliver a series of thirty graded shocks, up to 450 volts in the training process. Each of the thirteen male and thirteen female subjects individually saw and heard the puppy squealing and jumping around the electrified grid as they pressed lever after lever. There was no doubt that they were hurting the puppy with each shock they administered. (Although the shock intensities were much lower than indicated by the voltage labels appearing on the shock box, they were still powerful enough to evoke clearly distressed reactions from the puppy with each successive press of the shock switches.)

As you might imagine, the students were clearly upset during the experiment. Some of the females cried, and the male students also expressed a lot of distress. Did they refuse to continue once they could see the suffering they were causing right before their eyes? For all too many, their personal distress did not lead to behavioral disobedience. About half of the males (54 percent) went all the way to 450 volts. The big surprise came from the women’s high level of obedience. Despite their dissent and weeping, 100 percent of the female college students obeyed to the full extent possible in shocking the puppy as it tried to solve an insoluble task! A similar result was found in an unpublished study with adolescent high school girls. (The typical finding with human “victims,” including Milgram’s own findings, is that there are no male-female gender differences in obedience.25)

Some critics of the obedience experiments tried to invalidate Milgram’s findings by arguing that subjects quickly discover that the shocks are fake, and that is why they continue to give them to the very end.26 This study, conducted back in 1972 (by psychologists Charles Sheridan and Richard King), removes any doubt that Milgram’s high obedience rates could have resulted from subjects’ disbelief that they were actually hurting the learner-victim. Sheridan and King showed that there was an obvious visual connection between a subject’s obedience reactions and a puppy’s pain. Of further interest is the finding that half of the males who disobeyed lied to their teacher in reporting that the puppy had learned the insoluble task, a deceptive form of disobedience. When students in a comparable college class were asked to predict how far an average woman would go on this task, they estimated 0 percent—a far cry from 100 percent. (However, this faulty low estimate is reminiscent of the 1 percent figure given by the psychiatrists who assessed the Milgram paradigm.) Again this underscores one of my central arguments, that it is difficult for people to appreciate fully the power of situational forces acting on individual behavior when they are viewed outside the behavioral context.

Physicians’ Power over Nurses to Mistreat Patients
