CHAPTER THIRTEEN

Investigating Social Dynamics: Deindividuation, Dehumanization, and the Evil of Inaction

The historical account of humans is a heap of conspiracies, rebellions, murders, massacres, revolutions, banishments, the very worst effects that avarice, faction, hypocrisy, perfidiousness, cruelty, rage, madness, hatred, envy, lust, malice, and ambition could produce . . . . I cannot but conclude the bulk of your natives to be the most pernicious race of little odious vermin that nature ever suffered to crawl upon the surface of the earth.

—Jonathan Swift, Gulliver’s Travels (1727)1



Perhaps Jonathan Swift’s total condemnation of our human race—of us Yahoos—is a bit extreme, but consider that he wrote this critique several hundred years before the advent of genocides throughout the modern world, before the Holocaust. His views reflect a basic theme in Western literature that “Mankind” has suffered a great fall from its original state of perfection, starting with Adam’s act of disobedience against God when he succumbed to Satan’s temptation.

The social philosopher Jean-Jacques Rousseau elaborated this theme of the corrupting influence of social forces by envisioning human beings as “noble, primitive savages” whose virtues were diminished by contact with corrupting society. In stark opposition to this conception of human beings as the innocent victims of an all-powerful, malignant society is the view that people are born evil—genetic bad seeds. Our species is driven by wanton desires, unlimited appetites, and hostile impulses unless people are transformed into rational, reasonable, compassionate human beings by education, religion, and family, or controlled by the discipline imposed upon them by the authority of the State.

Where do you stand in this ages-old debate? Are we born good and then corrupted by an evil society or born evil and redeemed by a good society? Before casting your ballot, consider an alternative perspective. Maybe each of us has the capacity to be a saint or a sinner, altruistic or selfish, gentle or cruel, dominant or submissive, perpetrator or victim, prisoner or guard. Maybe it is our social circumstances that determine which of our many mental templates, our potentials, we develop. Scientists are discovering that embryonic stem cells are capable of becoming virtually any kind of cell or tissue and ordinary skin cells can be turned into embryonic stem cells. It is tempting to expand these biological concepts and what is now known about the developmental plasticity of the human brain to the “plasticity” of human nature.2

What we are is shaped both by the broad systems that govern our lives—wealth and poverty, geography and climate, historical epoch, cultural, political and religious dominance—and by the specific situations we deal with daily. Those forces in turn interact with our basic biology and personality. I have argued earlier that the potential for perversion is inherent in the complexity of the human mind. The impulse toward evil and the impulse toward good together comprise the most fundamental duality in human nature. This conception offers a complex, richer portrait of the pride and puzzles in human actions.

We have examined the power of group conformity and obedience to authority that can dominate and subvert individual initiative. Next, we add insights from research into the domains of deindividuation, dehumanization, and bystander apathy, or the “evil of inaction.” This information will complete the foundation for us to fully appreciate how ordinary, good individuals—perhaps even you, gentle reader—can be led at times to do bad things to others, even bad deeds that violate any sense of common decency or morality.


DEINDIVIDUATION: ANONYMITY AND DESTRUCTIVENESS

William Golding’s novel Lord of the Flies asks how a simple change in one’s external appearance can trigger dramatic changes in overt behavior. Good British choirboys are transformed into murderous little beasts by simply painting their faces. When food runs out on their desert island, a group of boys, led by Jack Merridew, try to kill a pig—but they can’t complete the act because killing has been inhibited by their Christian morality. Then Jack decides to paint his face into a mask, and as he does, a frightening metamorphosis occurs as he sees his reflection in the water:



He looked in astonishment, no longer at himself but at an awesome stranger. He spilt the water and leapt to his feet, laughing excitedly. Beside the pool his sinewy body held up a mask that drew their eyes and appalled them [the other boys]. He began to dance and his laughter became a blood-thirsty snarling. He capered toward Bill, and the mask was a thing on its own, behind which Jack hid, liberated from shame and self-consciousness.



After the other boys in Jack’s gang also disguise themselves with painted masks, they are readily able to “Kill the pig. Cut her throat. Spill her blood.”3 Once that alien deed of killing another creature is accomplished, they then relish the fun of killing both animals and their human enemies, notably the intellectual boy nicknamed “Piggy.” Might makes right, and all hell breaks loose as Ralph, the good-boy leader, is hunted down by the herd.

Is there any psychological validity to the notion that disguising one’s external appearance can drastically infect behavioral processes? I attempted to answer that question with a set of studies that helped stimulate a new field of inquiry on the psychology of deindividuation and antisocial behavior.4

The Shocking Behavior of Anonymous Women

The basic procedure in this first experiment involved having female college students believe they were delivering a series of painful electric shocks to other women, under the guise of a believable “cover story.” They would have multiple opportunities to shock each of two other young women whom they saw and heard from behind a one-way mirror. Half of the student volunteers were randomly assigned to a condition of anonymity, or deindividuation, half to a condition where their identity was made salient, or individuation. The four college student subjects, in each of the ten separately tested deindividuation groups, had their appearance concealed by hoods and loose, oversized lab coats, their names replaced by numbers, one to four. The experimenter treated them as an anonymous group, not as individuals. These procedures were performed allegedly to mask their nonverbal behavior so that others could not detect their reactions. The comparison group, by contrast, was given name tags that helped to make them feel unique, but everything else was the same for them as for those in the deindividuated groups. Both the deindividuated and the comparison subjects were in four-woman groups, and both were asked to repeatedly shock each of two women “victims” over the course of twenty trials.

The cover story was that the target women were subjects in a study of creativity under stress. The job of our participants was to stress them by administering painful electric shocks while I, as the experimenter in the next room, also behind the mirror, administered the creativity test.

Unlike in the Milgram paradigm, there was no authority directly pressuring them to act aggressively by delivering these painful shocks because I did not interact with them at all during the shock episodes. The students could see me through the observation window along with each of the two women in the alleged creativity study. In addition, there was no group conformity pressure exerted on them because they did not interact with one another, being in separate cubicles adjacent to one another. Finally, there was not even pressure on them to perform the task in order not to violate the rationale for the study. If any one of the four group members delivered a shock, the target woman would act hurt, so there would be sufficient stress induced to impact her creativity. Thus, not shocking would be neither disobedient nor confrontational—one could be merely a passive observer of teammates’ shocking instead of an active perpetrator.

The behavior of interest was the duration of shock administered—how long each student continued to hold down the shock button after the shock-ready signal appeared. Their decision was not whether to press the shock switch but how quickly to pull their finger off the switch or to keep pressing it down and deliver longer shocks to the victims. Of course, they could choose not to follow the instructions and refuse to even press the shock switch at all (no one ever did that).

One of the complicit women had been portrayed as very nice, the other as “bitchy,” in their tape-recorded interviews. This was done to create divergent attitudes toward them in the minds of their “stress inducers.” Actually, they were both research confederates, trained to act as though they were in pain when a shock light lit up and to give a convincing performance that the students in the other room could witness.

Before the experiment began, each of the students in the study was given a sample of the painful shock (of 75 volts) so they knew that it would hurt—but of course, no shock was actually given in the experiment. For each of twenty trials, the women had to decide whether and for how long they would administer this standard level of painful shock to a woman who was seemingly suffering in pain from their actions. Then, after the first target woman left, she was replaced by the second target woman for yet another round of twenty shock trials. Because each member of the four-person experimental and comparison groups was tested in adjacent individual cubicles, their “teammates” could not directly influence their behavior. Nevertheless, they had a sense of being in a group from having been together initially while they listened together to the tape recordings. It is important to note that except for the manipulation of anonymity versus identifiability, everything in the study was identical for both groups.

The results were clear: The women in the deindividuation condition delivered twice as much shock to both victims as did the comparison women who had been randomly assigned to the individuated condition. Moreover, the women who were anonymous shocked both victims equally, the one they had previously rated as pleasant and the other they had rated as unpleasant. It did not matter what they had previously felt about them once they had their finger on the trigger. They also increased shock time for both over the course of the twenty trials, holding their finger down ever longer on the shock switch as their victims twisted and moaned right before them. In contrast, the individuated women discriminated between the likeable and unpleasant targets, shocking the pleasant woman less over time than they did the unpleasant one.

That the anonymous women ignored their previous liking or disliking of the two target women when they had the chance to harm them speaks to a dramatic change in their mentality when in this psychological state of deindividuation. The escalation of shock, with repeated opportunities to administer its painful consequences, appears to be an upward-spiraling effect of the emotional arousal that is being experienced. The agitated behavior becomes self-reinforcing, each action stimulating a stronger, less controlled next reaction. Experientially, it comes not from sadistic motives of wanting to harm others but rather from the energizing sense of one’s domination and control over others at that moment in time.

This basic paradigm has been repeated with comparable results in a host of laboratory and field studies, using deindividuating masks, administering white noise, or throwing Styrofoam balls at the target victims, and with military personnel from the Belgian Army as well as with schoolchildren and a variety of college students. Similar escalations of shock over time were also found in a study where teacher-shockers were supposed to be educating their pupil-victims—they too delivered increasing levels of shock across training sessions.5

The Stanford Prison Experiment, as you recall, relied on the deindividuating silver reflecting sunglasses for the guards and staff along with standard military-style uniforms. One important conclusion flows from this body of research: anything, or any situation, that makes people feel anonymous, as though no one knows who they are or cares to know, reduces their sense of personal accountability, thereby creating the potential for evil action. This becomes especially true when a second factor is added: if the situation or some agency gives them permission to engage in antisocial or violent action against others, as in these research settings, people are ready to go to war. If, instead, the situation conveys merely a reduction of self-centeredness with anonymity and encourages prosocial behavior, people are ready to make love. (Anonymity in party settings often makes for more socially engaging parties.) So William Golding’s insight about anonymity and aggression was psychologically valid—but in more complex and interesting ways than he depicted.



Sure, this robe of mine doth change my disposition.


—William Shakespeare, The Winter’s Tale



Anonymity can be conferred on others not only with masks but also by the way that people are treated in given situations. When others treat you as if you are not a unique individual but just an undifferentiated “other” being processed by the System, or your existence is ignored, you feel anonymous. The sense of a lack of personal identifiability can also induce antisocial behavior. When a researcher treated college student research volunteers either humanely or as “guinea pigs” in an experiment, guess who ripped him off when he wasn’t looking? Later on, these students found themselves alone in the professor-researcher’s office with the opportunity to steal coins and pens from a bowl full of them. Those who were in the anonymity condition stole much more often than did the humanely treated students.6 Kindness can be more than its own reward.

Halloween Aggression by Schoolchildren

What happens when children go to an unusual Halloween party where they put on costumes and are given permission by their teacher to play aggressive games for prizes? Will anonymity plus opportunity to aggress lead children to engage in more aggression over time?

Elementary school children attended a special, experimental Halloween party given by their teacher and supervised by a social psychologist, Scott Fraser.7 There were many games to play, and the children could win tokens for each game they won. These tokens could be exchanged for gifts at the end of the party. The more tokens you won, the better the toys you could get, so the motivation to win as many tokens as possible was high.

Half the games were nonaggressive in nature, and half involved confrontations between two children to reach the goal. For example, a nonaggressive game might have individual students trying to speedily retrieve a beanbag in a tube, while a potentially aggressive game would entail two students competing to be the first one to get that one beanbag out of the tube. The aggression observed typically involved the competitors’ pushing and shoving each other. It was not very extreme but was characteristic of first-stage physical encounters between children.

The experimental design used only one group, in which each child served as his or her own control. This procedure is known as the A-B-A format—pre-baseline/change introduced/post-baseline. The children first played the games without costumes (A), then with costumes (B), then again without costumes (A). Initially, while the games were played, the teacher said the costumes were on the way so they would start the fun while they waited for them to arrive. Then, when the costumes arrived, they were put on in different rooms so the children’s identities were not known to each other, and they played the same games but now in costume. In the third phase, the costumes were removed (allegedly to be given to other children in other parties) and the games continued as in the first phase. Each phase of the games lasted about an hour.

The data are striking testimony to the power of anonymity. Aggression among these young schoolchildren increased significantly as soon as they put the costumes on. The percentage of the total time that these children played the aggressive games more than doubled from their initial base level average, up from 42 percent (in A) to 86 percent (in B). Equally interesting was the second major result: aggression had a high negative payoff. The more time a child spent engaged in the aggressive games, the fewer tokens she or he won during that phase of the party. Being aggressive thus cost the children a loss of tokens. Acting in the aggressive games took more time than the nonaggressive games and only one of two contestants could win, so overall, being aggressive lost valued prizes. However, that did not matter when the children were costumed and anonymous. The smallest number of tokens won was during the second, anonymity B, phase, where aggression was highest; only an average of 31 tokens were won, compared to 58 tokens in the A phase.

A third important finding was that there was no carryover of aggressive behavior from the high level in the B phase to the last A-phase level, which was comparable to the initial A phase. The percentage of aggressive acts dropped to 36 percent, and the number of tokens won soared to 79. Thus, we can conclude that the behavior change brought on by anonymity did not create a dispositional, internal change, but only an outward response change. Change the situation, and behavior changes in lockstep fashion. The use of this A-B-A design also makes apparent that perceived anonymity was sufficient to dramatically alter behavior in each time frame. Anonymity facilitated aggression even though the consequences of that physical aggression were not in the child’s best immediate interest of winning tokens exchangeable for fine prizes. Aggression became its own reward. Goals that were distant took a backseat to “the fun and games” of the present moment. (We will see a similar phenomenon operating in some of the Abu Ghraib abuses.)

In a related field study, Halloween trick-or-treaters visiting local homes in their own costumes were more likely to steal goodies when they were anonymous than when identifiable. Friends of the researchers put out bowls filled with candies and others with coins, each of which was labeled “Take one.” Going beyond that limit constituted a transgression, stealing. Some children arrived alone, others in groups of friends. In the anonymous condition, the homeowner made it evident that he or she could not tell who they were. With their identities concealed by their costumes, the majority of those in groups stole the candy and money (just as did those college students in the study where they were treated as “guinea pigs”). This was in contrast to the nonanonymous condition, wherein the adult host had first asked them to reveal their identity behind their masks.8

Among the more than seven hundred children studied in this natural situation, more transgressions were found when they were in anonymous groups (57 percent) than when anonymous and alone (21 percent). Fewer transgressions occurred when nonanonymous children were alone (8 percent) than when they were in groups of other nonanonymous trick-or-treaters (21 percent). Even when alone and identifiable, the temptation of easy money and delicious treats was too great for some children to pass up. However, adding the full-anonymity dimension turned that singular temptation into an overwhelming passion for most children to take all the goodies they could.

Cultural Wisdom: How to Make Warriors Kill in War but Not at Home

Let’s leave the laboratory and the games at children’s parties to go back to the real world, where these issues of anonymity and violence may take on life-and-death significance. Specifically, let’s look at the differences between societies that go to war without having young male warriors change their appearance and those that always include ritual transformations of appearance by painting faces and bodies or masking the warriors (as in Lord of the Flies). Does a change in external appearance make a significant difference in how warring enemies are then treated?

A cultural anthropologist, R. J. Watson,9 posed that question after reading my earlier work on deindividuation. His data source was the Human Relations Area Files, where information on cultures around the world is archived in the form of reports of anthropologists, missionaries, psychologists, and others. Watson found two pieces of data on societies in which warriors did or did not change their appearance prior to going to war and the extent to which they killed, tortured, or mutilated their victims, a decidedly deadly dependent variable—the ultimate in outcome measures.

The results are striking confirmation of the prediction that anonymity promotes destructive behavior—when permission is also given to behave in aggressive ways that are ordinarily prohibited. War provides the institutionally approved permission to kill or wound one’s adversaries. This investigator found that, of the twenty-three societies for which these two data sets were present, in fifteen warriors changed their appearance. They were the societies that were the most destructive; fully 80 percent of them (twelve of fifteen) brutalized their enemies. By contrast, in seven of eight of the societies in which the warriors did not change their appearance before going into battle, they did not engage in such destructive behavior. Another way to look at this data is that 90 percent of the time when victims of battle were killed, tortured, or mutilated, it was by warriors who had first changed their appearance and deindividuated themselves.

Cultural wisdom dictates that a key ingredient in transforming ordinarily nonaggressive young men into warriors who can kill on command is first to change their external appearance. Most wars are about old men persuading young men to harm and kill other young men like themselves. For the young men, it becomes easier to do so if they first change their appearance, altering their usual external façade by putting on military uniforms or masks or painting their faces. With the anonymity thus provided in place, out go their usual internal compassion and concern for others. When the war is won, the culture then dictates that the warriors return to their peacetime status. This reverse transformation is readily accomplished by making the warriors remove their uniforms, take off their masks, wash away the paint, and return to their former personae and peaceful demeanor. In a sense, it is as though they were in a macabre social ritual, unknowingly using the A-B-A paradigm of Fraser’s Halloween experiment. Peaceful when identifiable, murderous when anonymous, peaceful again when returned to the identifiable condition.

Certain environments convey a sense of transient anonymity in those who live or behave in their midst, without changing their physical appearance. To demonstrate the impact of the anonymity of place in facilitating urban vandalism, my research team did a simple field study. Recall from chapter 1 that we abandoned cars on the streets near the uptown campus of New York University in the Bronx, New York, and near Stanford University’s campus in Palo Alto, California. We photographed and videotaped acts of vandalism against these cars, which were clearly abandoned (license plates removed, hoods raised). In the anonymity of the Bronx setting, several dozen passersby, on the street or in cars, stopped to vandalize the car within forty-eight hours. Most were reasonably well-dressed adults, who stripped the car of any valuable items or simply destroyed it—all in the daytime. By contrast, over a week’s time, not a single passerby engaged in any act of vandalism against the car abandoned in Palo Alto. This demonstration was the only empirical evidence cited in support of the “Broken Windows Theory” of urban crime. Environmental conditions contribute to making some members of society feel that they are anonymous, that no one in the dominant community knows who they are, that no one recognizes their individuality and thus their humanity. When that happens, we contribute to their transformation into potential vandals and assassins. (For full details of this research and Broken Windows Theory, see our Lucifer Effect website.)

Deindividuation Transforms Our Apollonian Nature into a Dionysian Nature

Let’s assume that the “good” side of people is the rationality, order, coherence, and wisdom of Apollo, while the “bad” side is the chaos, disorganization, irrationality, and libidinous core of Dionysus. The Apollonian central trait is constraint and the inhibition of desire; it is pitted against the Dionysian trait of uninhibited release and lust. People can become evil when they are enmeshed in situations where the cognitive controls that usually guide their behavior in socially desirable and personally acceptable ways are blocked, suspended, or distorted. The suspension of cognitive control has multiple consequences, among them the suspension of: conscience, self-awareness, sense of personal responsibility, obligation, commitment, liability, morality, guilt, shame, fear, and analysis of one’s actions in cost-benefit calculations.

The two general strategies for accomplishing this transformation are: (a) reducing the cues of social accountability of the actor (no one knows who I am or cares to) and (b) reducing concern for self-evaluation by the actor. The first cuts out concern for social evaluation, for social approval, doing so by making the actor feel anonymous—the process of deindividuation. It is effective when one is functioning in an environment that conveys anonymity and diffuses personal responsibility. The second strategy stops self-monitoring and consistency monitoring by relying on tactics that alter one’s state of consciousness. This is accomplished by means of taking alcohol or drugs, arousing strong emotions, engaging in hyperintense actions, getting into an expanded present-time orientation where there is no concern for past or future, and projecting responsibility outward onto others rather than inward toward oneself.

Deindividuation creates a unique psychological state in which behavior comes under the control of immediate situational demands and biological, hormonal urges. Action replaces thought, seeking immediate pleasure dominates delaying gratification, and mindfully restrained decisions give way to mindless emotional responses. A state of arousal is often both a precursor to and a consequence of deindividuation. Its effects are amplified in novel or unstructured situations where typical response habits and character traits are nullified. One’s vulnerability to social models and situational cues is heightened; therefore, it becomes as easy to make love as to make war—it all depends on what the situation demands or elicits. In the extreme, there is no sense of right and wrong, no thoughts of culpability for illegal acts or Hell for immoral ones.10 With inner restraints suspended, behavior is totally under external situational control; outer dominates inner. What is possible and available dominates what is right and just. The moral compass of individuals and groups has then lost its polarity.

The transition from Apollonian to Dionysian mentalities can be swift and unexpected, making good people do bad things, as they live temporarily in the expanded present moment without concerns for the future consequences of their actions. Usual constraints on cruelty and libidinal impulses melt away in the excesses of deindividuation. It is as if there were a short circuit in the brain, cutting off the frontal cortex’s planning and decision-making functions, while the more primitive portions of the brain’s limbic system, especially its emotion and aggression center in the amygdala, take over.

The Mardi Gras Effect: Communal Deindividuation as Ecstasy

In ancient Greece, Dionysus was unique among the gods. He was seen as creating a new level of reality that challenged traditional assumptions and ways of living. He represented both a force for the liberation of the human spirit from its staid confinement in rational discourse and orderly planning, and a force of destruction: lust without limits and personal pleasure without societal controls. Dionysus was the god of drunkenness, the god of insanity, the god of sexual frenzy and battle lust. Dionysus’ dominion includes all states of being that entail the loss of self-awareness and rationality, the suspension of linear time, and the abandonment of the self to those urges in human nature that overthrow codes of behavior and public responsibility.

Mardi Gras has its origins as a pagan, pre-Christian ceremony now recognized by the Roman Catholic Church as occurring on the Tuesday (Fat Tuesday, or Shrove Tuesday) just before Ash Wednesday. That holy day marks the start of the Christian liturgical Season of Lent with its personal sacrifices and abstinence leading to Easter Sunday, forty-six days later. Mardi Gras celebrations begin on the Twelfth Night Feast of the Epiphany, when the three kings visited the newborn Jesus Christ.

In practice, Mardi Gras celebrates the excess of libidinous pleasure seeking, of living for the moment, of “wine, women, and song.” Cares and obligations are forgotten while celebrants indulge their sensual nature in communal revelries. It is a Bacchanalian festivity that loosens behavior from its usual constraints and reason-based actions. However, there is always the preconscious awareness that this celebration is transitory, soon to be replaced by even greater than usual limits on personal pleasures and vices with the advent of Lent. “The Mardi Gras effect” involves temporarily giving up the traditional cognitive and moral constraints on personal behavior when part of a group of like-minded revelers bent on having fun now without concern for subsequent consequences and liabilities. It is deindividualization in group action.





DEHUMANIZATION AND MORAL DISENGAGEMENT


Dehumanization is the central construct in our understanding of “man’s inhumanity to man.” Dehumanization occurs whenever some human beings consider other human beings to be excluded from the moral order of being a human person. The objects of this psychological process lose their human status in the eyes of their dehumanizers. By identifying certain individuals or groups as being outside the sphere of humanity, dehumanizing agents suspend the morality that might typically govern reasoned actions toward their fellows.

Dehumanization is a central process in prejudice, racism, and discrimination. Dehumanization stigmatizes others, attributing to them a “spoiled identity.” For example, the sociologist Erving Goffman11 described the process by which those who are disabled are socially discredited. They become not fully human and thus tainted.

Under such conditions, it becomes possible for normal, morally upright, and even usually idealistic people to perform acts of destructive cruelty. Not responding to the human qualities of other persons automatically facilitates inhumane actions. The golden rule then becomes truncated: “Do unto others as you would.” It is easier to be callous or rude toward dehumanized “objects,” to ignore their demands and pleas, to use them for your own purposes, even to destroy them if they are irritating.12

A Japanese general reported that it had been easy for his soldiers to brutally massacre Chinese civilians during Japan’s pre–World War II invasion of China, “because we thought of them as things, not people like us.” This was obviously so during the “Rape of Nanking” in 1937. Recall the description (in chapter 1) of the Tutsis by the woman who orchestrated many of the rapes of them—they were nothing more than “insects,” “cockroaches.” Similarly, the Nazi genocide of the Jews began by first creating through propaganda films and posters a national perception of these fellow human beings as inferior forms of animal life, as vermin, as voracious rats. The many lynchings of black people by mobs of whites in cities throughout the United States were likewise not considered crimes against humanity because of the stigmatization of them as only “niggers.”13

Behind the My Lai massacre of hundreds of innocent Vietnamese civilians by American soldiers was the dehumanizing “gooks” label that GIs had for all of those different-looking Asian people.14 Yesterday’s “gooks” have become today’s “hajjis” and “towel heads” in the Iraq War as a new corps of soldiers derogates these different-looking citizens and soldiers. “You just sort of try to block out the fact that they’re human beings and see them as enemies,” said Sergeant Mejia, who refused to return to action in what he considered an abominable war. “You call them ‘hajis’, you know? You do all the things that make it easier to deal with killing them and mistreating them.”15

That such labels and their associated images can have powerful motivating effects was demonstrated in a fascinating controlled laboratory experiment (mentioned in chapter 1, elaborated here).

Experimental Dehumanization: Animalizing College Students

My Stanford University colleague Albert Bandura and his students designed a powerful experiment that elegantly demonstrates the power of dehumanizing labels to foster harm against others.16

Seventy-two male volunteers from nearby junior colleges were divided into three-member “supervisory teams” whose task was to punish the inadequate decision making of other college students who were allegedly serving as a group of decision makers. The real subjects of the study were, of course, the students playing the role of supervisors.

On each of twenty-five bargaining trials, the supervisors heard the decision-making team (reported to be in an adjacent room) supposedly formulating collective decisions. The supervisors were given information they used to evaluate the adequacy of the decision on each trial. Whenever a bad decision was made, it was the job of this supervisory team to punish the error by administering a shock. They could choose the shock intensity from a mild level of 1 to a maximum level of 10 on any trial, which all the members of the decision-making team would receive.

The supervisors were told that participants from different social backgrounds were included in this project to increase its generality, but each group of decision makers was composed of people with similar attributes. This was done so that the positive or negative labels soon to be applied to them would hold for the entire group.

The researchers varied two features of this basic situation: how the “victims” were labeled and how personally responsible the supervisors were for the shocks they administered. The volunteers were randomly assigned to three conditions of labeling—dehumanized, humanized, or neutral—and two conditions of responsibility—individualized or diffused.

Let’s first consider how the labeling was imposed and its effects. Then we will see how the responsibility variations operated. After settling into the study, each group of participants believed they were overhearing an interchange over the intercom between the research assistant and the experimenter about the questionnaires the decision makers had allegedly completed. The assistant remarked in a brief aside that the personal qualities exhibited by this group confirmed the opinion of the person by whom they had been recruited. In the dehumanized condition, the decision makers were characterized as “an animalistic, rotten bunch.” By contrast, in the humanized condition, they were characterized as a “perceptive, understanding, and otherwise humanized group.” No evaluative references were made about those in the third, neutral condition.

It should be made clear that the participants never interacted with their shock victims and therefore could not make such evaluations personally or evaluate their adequacy. The labels were secondhand attributions made about other young college men, supposedly also volunteers functioning in an assigned role in this situation. So did the labels have any effect on how these college students punished those they were allegedly supervising? (There were, in fact, no actual “others,” only standardized tape feedback.)

Indeed, the labels stuck and had a big impact on the extent to which the students punished their supervisees. Those labeled in the dehumanizing way, as “animals,” were shocked most intensively, and their shock level increased linearly over ten trials. It also climbed higher and higher over trials, up to an average of 7 out of the maximum of 10 for each group of participants. Those labeled “nice” were given the smallest amount of shock, while the unlabeled, neutral group fell in the middle of these two extremes.

Further, during the first trial, there was no difference at all between the three experimental treatments in the level of shock administered—they all administered the same low level of shock. Had the study ended then, the conclusion would have been that the labels made no difference. However, with each successive trial, as the errors of the decision makers allegedly multiplied, the shock levels of the three groups diverged. Those shocking the so-called animals shocked them more intensely over time, a result comparable to the escalating shock level of the deindividuated female college students in my earlier study. That rise in aggressive responding over time, with practice, or with experience illustrates a self-reinforcing effect. Perhaps the pleasure is not so much in inflicting pain as in the sense of power and control one feels in such a situation of dominance—giving others what they deserve to get. The researchers point to the disinhibiting power of labeling to divest other people of their human qualities.

On the plus side in this study, that same arbitrary labeling also resulted in others being treated with greater respect if someone in authority had labeled them positively. Those perceived as “nice” were harmed the least. Thus, the power of humanization to counteract punitiveness is of equal theoretical and social significance as the phenomenon of dehumanization. There is an important message here about the power of words, labels, rhetoric, and stereotyped labeling, to be used for good or evil. We need to refashion the childhood rhyme “Sticks and stones may break my bones, but names will never harm me,” to alter the last phrase to “but bad names can kill me, and good ones can comfort me.”

Finally, what about the variations in responsibility for the level of shock that was being administered? Significantly higher levels of shock were given when participants believed that the shock level was an average response of their team rather than when it was the direct level of each individual’s personal decision. As we have seen before, diffusion of responsibility, in any form it takes, lowers the inhibition against harming others. As one might predict, the very highest levels of shock—and anticipated harm—were administered both when participants felt less personally responsible and when their victims were dehumanized.

When Bandura’s research team evaluated how the participants had justified their performance, they found that dehumanization promoted the use of self-absolving justifications, which in turn were associated with increasing punishment. These findings about how people disengage their usual self-sanctions against behaving in ways that are detrimental to others led Bandura to develop a conceptual model of “moral disengagement.”

Mechanisms of Moral Disengagement

This model begins by assuming that most people adopt moral standards because of undergoing normal socialization processes during their upbringing. Those standards act as guides for prosocial behavior and deterrents of antisocial behavior as defined by their family and social community. Over time, these external moral standards imposed by parents, teachers, and other authorities become internalized as codes of personal conduct. People develop personal controls over their thoughts and actions that become satisfying and provide a sense of self-worth. They learn to sanction themselves to prevent acting inhumanely and to foster humane actions. The self-regulatory mechanisms are not fixed and static in their relation to a person’s moral standards. Rather, they are governed by a dynamic process in which moral self-censure can be selectively activated to engage in acceptable conduct; or, at other times, moral self-censure can be disengaged from reprehensible conduct. Individuals and groups can maintain their sense of moral standards by simply disengaging their usual moral functioning at certain times, in certain situations, for certain purposes. It is as if they shift their morality into neutral gear and coast along without concern for hitting pedestrians until they later shift back to a higher gear, returning to higher moral ground.

Bandura’s model goes further in elucidating the specific psychological mechanisms individuals generate to convert their harmful actions into morally acceptable ones as they selectively disengage the self-sanctions that regulate their behavior. Because this is such a fundamental human process, Bandura argues that it helps to explain not only political, military, and terrorist violence but also “everyday situations in which decent people routinely perform activities that further their interests but have injurious human effects.”17

It becomes possible for any of us to disengage morally from any sort of destructive or evil conduct when we activate one or more of the following four types of cognitive mechanisms.

First, we can redefine our harmful behavior as honorable. Creating moral justification for the action, by adopting moral imperatives that sanctify violence, does this. Creating advantageous comparisons that contrast our righteous behavior to the evil behavior of our enemies also does this. (We only torture them; they behead us.) Using euphemistic language that sanitizes the reality of our cruel actions does this as well. (“Collateral damage” means that civilians have been bombed into dust; “friendly fire” means that a soldier has been murdered by the stupidity or intentional efforts of his buddies.)

Second, we can minimize our sense of a direct link between our actions and its harmful outcomes by diffusing or displacing personal responsibility. We spare ourselves self-condemnation if we do not perceive ourselves as the agents of crimes against humanity.

Third, we can change the way we think about the actual harm done by our actions. We can ignore, distort, minimize, or disbelieve any negative consequences of our conduct.

Finally, we can reconstruct our perception of victims as deserving their punishment, by blaming them for the consequences, and of course, by dehumanizing them, perceiving them to be beneath the righteous concerns we reserve for fellow human beings.

Understanding Dehumanization Is Not Excusing It

It is important once again to add here that such psychological analyses are never intended to excuse or make light of the immoral and illegal behaviors of perpetrators. By making explicit the mental mechanisms people use to disengage their moral standards from their conduct, we are in a better position to reverse the process, reaffirming the need for moral engagement as crucial for promoting empathic humaneness among people.

However, before moving on it is important to make concrete the notion that people in positions of power and authority often reject attempts at causal situational analyses in matters of great national concern. Instead, at least in one recent instance, they have endorsed simplistic dispositional views that would have made Inquisition judges smile.

Secretary of State Condoleezza Rice is a Stanford University professor of political science with a specialization in the Soviet military. Her training should have made her sensitive to systems-level analyses of complex political problems. However, not only was that perspective missing during an interview with Jim Lehrer on his NewsHour (July 28, 2005), but instead she championed a dogmatic, simplistic dispositional view. In response to her interviewer’s question about whether U.S. foreign policy is promoting rather than eliminating terrorism, Rice attacked any such thinking as “excuse mongering,” as she makes it clear that terrorism is simply about “evil people”: “When are we going to stop making excuses for the terrorists and saying that somebody is making them do it? No, these are simply evil people who want to kill. And they want to kill in the name of a perverted ideology that really is not Islam, but they somehow want to claim that mantle to say that this is about some kind of grievance. This isn’t about some kind of grievance. This is an effort to destroy, rather than to build. And until everybody in the world calls it by name—the evil that it is—stops making excuses for them, then I think we’re going to have a problem.”

I Am More Human than You: The Infrahumanization Bias

Beyond perceiving and derogating others in the “out-group” with animallike qualities, people also deny them any “human essence.” Out-group infrahumanization is a newly investigated phenomenon in which people tend to attribute uniquely human emotions and traits to their in-group and deny their existence in out-groups. It is a form of emotional prejudice.18

However, we go further in declaring that the essence of humanness resides primarily in ourselves, more so than in any others, even our in-group members. While we attribute infrahumaness to out-groups, as less than human, we are motivated to see ourselves as more human than others. We deny uniquely human traits and even human nature to others, relative to our own egocentric standard. This self-humanization bias is the complement of the other-infrahumanization bias. These tendencies appear to be rather general and multifaceted. A team of Australian researchers concluded their investigation into the perception of humanness with a variant of the famous quote by the ancient Roman writer Terence. He proudly proclaimed, “Nothing human is alien to me.” Its ironic twist notes, “Nothing human may be alien to me, but something human is alien to you.”19 (It is unlikely that such an imperial “I” exists among members of collectivist cultures, but we await new research to inform us of the limits of such egocentrism.)

Creating Dehumanized Enemies of the State

Among the operational principles we must add to our arsenal of weapons that trigger evil acts by ordinarily good men and women are those developed by nation-states to incite their own citizens. We learn about some of these principles by considering how nations prepare their young men to engage in deadly wars while also preparing citizens to endorse engaging in wars of aggression. A special form of cognitive conditioning through propaganda helps accomplish this difficult transformation. “Images of the enemy” are created by national media propaganda (in complicity with governments) to prepare the minds of soldiers and citizens to hate those who fit the new category “your enemy.” Such mental conditioning is a soldier’s most potent weapon. Without it, he might never put another young man in the crosshairs of his gun sight and fire to kill him. It induces a fear of vulnerability among citizens who can imagine what it would be like to be dominated by that enemy.20 That fear becomes morphed into hatred and a willingness to take hostile action to reduce its threat. It extends its reaches into a willingness to send our children to die or be maimed in battle against that threatening enemy.

In Faces of the Enemy, Sam Keen21 shows how archetypes of the enemy are created by visual propaganda that most nations use against those judged to be the dangerous “them,” “outsiders,” “enemies.” These visual images create a consensual societal paranoia that is focused on the enemy who would do harm to the women, children, homes, and God of that nation’s way of life, destroying its fundamental beliefs and values. Such propaganda has been widely practiced on a worldwide scale. Despite national differences in many dimensions, it is still possible to categorize all such propaganda into a select set utilized by “homo hostilis.” In creating a new evil enemy in the minds of good members of righteous tribes, “the enemy” is: aggressor, faceless, rapist, godless, barbarian, greedy, criminal, torturer, murderer, an abstraction, or a dehumanized animal. Scary images reveal one’s nation being consumed by the animals that are most universally feared: snakes, rats, spiders, insects, lizards, gigantic gorillas, octopi, or even “English pigs.”

A final point on the consequences of adopting a dehumanized conception of selected others is the unthinkable things that we are willing to do to them once they are officially declared different and undesirable. More than 65,000 American citizens were sterilized against their will during an era (1920s–1940s) when eugenics advocates used scientific justifications to purify the human race by ridding it of all those with undesirable traits. We expect that view from Adolf Hitler but not from one of America’s most revered jurists, Oliver Wendell Holmes. He ruled in a majority opinion (1927) that compulsory sterilization laws, far from being unconstitutional, were a social good:



It is better for all the world, if instead of waiting to execute degenerate off-spring for crime, or let them starve for their imbecility, society can prevent those who are manifestly unfit from continuing their kind. Three generations of imbeciles are enough.22



Please recall the research cited in chapter 12 on students at the University of Hawaii who were willing to endorse the “final solution” to eliminate the unfit, even their own family members if necessary.

Both the United States and England have had a long history of involvement in the “war against the weak.” They have had their fair share of vocal, influential proponents of eugenics advocating and scientifically justifying plans to rid their nation of the misfits while enhancing the privileged status of the most fit.23


THE EVIL OF INACTION: PASSIVE BYSTANDERS



The only thing necessary for evil to triumph is for good men to do nothing.


—British statesman Edmund Burke



[W]e must learn that passively to accept an unjust system is to cooperate with that system, and thereby to become a participant in its evil.


—Martin Luther King, Jr.24



Our usual take on evil focuses on the violent, destructive actions of perpetrators, but the failure to act can also be a form of evil, when helping, dissent, disobedience, or whistle-blowing are required. One of the most critical, least acknowledged contributors to evil goes beyond the protagonists of harm to the silent chorus who look but do not see, who hear but do not listen. Their silent presence at the scene of evil doings makes the hazy line between good and evil even fuzzier. We ask next: Why don’t people help? Why don’t people act when their aid is needed? Is their passivity a personal defect of callousness, of indifference? Alternatively, are there identifiable social dynamics once again at play?

The Kitty Genovese Case: Social Psychologists to the Rescue, Belatedly

In a major urban center, such as New York City, London, Tokyo, or Mexico City, one is surrounded by literally tens of thousands of people. We walk beside them on the streets, sit near them in restaurants, movies, buses, and trains, wait in line with them—but remain unconnected, as if they do not really exist. For a young woman in Queens, they did not exist when she most needed them.



For more than half an hour, 38 respectable, law-abiding citizens in Queens [New York] watched a killer stalk and stab a woman in three separate attacks in Kew Gardens. Twice the sound of their voices and the sudden glow of their bedroom lights interrupted him and frightened him off. Each time he returned, sought her out and stabbed her again. Not one person telephoned the police during the assault; one witness called the police after the woman was dead. [The New York Times, March 13, 1964]



A recent reanalysis of the details of this case casts doubt upon how many people actually saw the events unfolding and whether they really comprehended what was happening, given that many were elderly and had awoken suddenly in the middle of the night. Nevertheless, there seems to be no question that many residents of this well-kept, usually quiet, almost suburban neighborhood heard the chilling screams and did not help in any way. Kitty died alone on a staircase, where she could no longer elude her crazed murderer.

Yet only a few months later, there was an even more vivid and chilling depiction of how alienated and passive bystanders can be. An eighteen-year-old secretary had been beaten, choked, stripped, and raped in her office. When she finally broke away from her assailant, naked and bleeding, she ran down the stairs of the building to the doorway screaming “Help me! Help me! He raped me!” A crowd of about forty persons gathered on the busy street and watched as the rapist dragged her back upstairs to continue his abuse. No one came to her aid! Only the chance arrival of passing police prevented her further abuse and possible murder (The New York Times, May 6, 1964).

Researching Bystander Intervention

Social psychologists heeded the alarm by initiating a series of pioneering studies on bystander intervention. They countered the usual slew of dispositional analyses about what is wrong with the callous New York bystanders by trying to understand what in the situation freezes the prosocial actions of ordinary people. At the time, both Bibb Latané and John Darley25 were professors at New York City universities—Columbia and NYU, respectively—so they were close to the heart of the action. Their field studies were done in a variety of New York City venues, such as on subways and street corners, and in laboratories.

Their research generated a counterintuitive conclusion: the more people who witness an emergency, the less likely any of them will intervene to help. Being part of a passively observing group means that each individual assumes that others are available who could or will help, so there is less pressure to initiate action than there is when people are alone or with only one other observer. The mere presence of others diffuses the sense of personal responsibility of any individual to get involved. Personality tests of participants showed no significant relationship between any particular personality characteristics and the speed or likelihood of intervening in staged emergencies.26

New Yorkers, like Londoners or others from big cities around the world, are likely to help and will intervene if they are directly asked or when they are alone or with a few others. The more people present who might help in an emergency situation, the more we assume that someone else will step forward, so we do not have to become energized to take any personal risk. Rather than callousness, failure to intervene is not only because one fears for one’s life in a violent scenario, but also because one denies the seriousness of the situation, fears doing the wrong thing and looking stupid or worries about the costs of getting involved in “someone else’s business.” There is also an emergent group norm of passive non-action.

Want Help? Just Ask for It

A former student of mine, Tom Moriarity, conducted a convincing demonstration that a simple situational feature can facilitate active bystander intervention among New Yorkers.27 In two scenarios, Tom arranged for a confederate to leave her purse on a table in a public, busy restaurant or her radio on a blanket at a crowded beach. Then another member of his research team would pretend to steal the purse or the radio as Tom recorded the actions of those near the scene of the simulated crime. Half the time virtually no one intervened and let the criminal escape with the goods. However, the other half of the time virtually everyone stopped the criminal in his tracks and prevented the crime. What made the difference?

In the first case, the woman merely asked the person nearby for the time, making minimal social contact, before leaving the scene temporarily. However, in the second case, she made a simple request to a nearby person to keep an eye on her purse or her radio until she returned. That direct request created a social obligation to protect this stranger’s property—an obligation that was honored fully. Want help? Ask for it. Chances are good that you will get it, even from allegedly callous New Yorkers or other large-city folks.

The implications of this research also highlight another theme we have been developing, that social situations are created by and can be modified by people. We are not robots acting on situational demand programs but can change any programming by our creative and constructive actions. The problem is that too often we accept others’ definition of the situation and their norms, rather than being willing to take the risk of challenging the norm and opening new channels of behavioral options. One interesting consequence of the line of research on passive and responsive bystanders has been the emergence of a relatively new area of social psychological research on helping and altruism (well summarized in a monograph by David Schroeder and his colleagues).28

How Good Are Good Samaritans in a Hurry?

A team of social psychologists staged a truly powerful demonstration that the failure to help strangers in distress is more likely due to situational variables than to dispositional inadequacies.29 It is one of my favorite studies, so let’s role-play with you once again as a participant.

Imagine you are a student studying for the ministry at Princeton University’s Theological Seminary. You are on your way to deliver a sermon on the Good Samaritan so that it can be videotaped for a psychology experiment on effective communication. You know the passage from the Gospel of Luke, chapter 10, quite well. It is about the only person who stopped to help a victim in distress on the side of the road from Jerusalem to Jericho. The Gospel tells us that he will reap his just rewards in Heaven for having been the Good Samaritan on Earth—a biblical lesson for all of us to heed about the virtues of altruism.

Imagine further that as you are heading from the Psychology Department to the videotaping center, you pass a stranger huddled up in an alley in dire distress, on the ground moaning, clearly in need of some aid. Now, can you imagine any conditions that would make you not stop to be that Good Samaritan, especially when you are mentally rehearsing the Good Samaritan parable at that very moment?

Rewind to the psychology laboratory. You have been told that you are late for the appointed taping session and so should hurry along. Other theology students were randomly assigned to conditions in which they were told that they had a little time or a lot more time to get to the taping center. But why should time pressure on you (or the others) make a difference if you are a good person, a holy person, a person thinking about the virtue of intervening to help strangers in distress, as did that old-time Good Samaritan? I am willing to wager that you would like to believe it would not make a difference, that in that situation you would stop and help, no matter what the circumstances. And so would the other seminary students come to the aid of the victim in distress.

Guess again: if you took the bet, you lost. The conclusion from the point of view of the victim is this: Don’t be a victim in distress when people are late and in a hurry. Almost every one of those seminary students—fully 90 percent of them—passed up the immediately compelling chance to be a Good Samaritan because they were in a hurry to give a sermon about it. They experienced the clash in task demands: to help science or to help a victim. Science won, and the victim was left to suffer. (As you would now expect, the victim was an acting confederate.)

The more time the seminarians believed they had, the more likely they were to stop and help. Thus, the situational variable of time pressure accounted for the major variations in who helped and who were passive bystanders. There was no need to resort to dispositional explanations about theology students being callous, cynical, or indifferent, as the nonhelping New Yorkers were assumed to be in the case of poor Kitty Genovese. When the research was replicated, the same result occurred, but when the seminarians were on their way to fulfill a less important task, the vast majority did stop to help. The lesson from this research is to not ask who does or does not help but rather what the social and psychological features of that situation were when trying to understand situations in which people fail to help those in distress.30

The Institutionalized Evil of Inaction

In situations where evil is being practiced, there are perpetrators, victims, and survivors. However, there are often observers of the ongoing activities or people who know what is going on and do not intervene to help or to challenge the evil and thereby enable evil to persist by their inaction.

It is the good cops who never oppose the brutality of their buddies beating up minorities on the streets or in the back room of the station house. It was the good bishops and cardinals who covered over the sins of their predatory parish priests because of their overriding concern for the image of the Catholic Church. They knew what was wrong and did nothing to really confront that evil, thereby enabling these pederasts to continue sinning for years on end (at the ultimate cost to the Church of billions in reparations and many disillusioned followers).31

Similarly, it was the good workers at Enron, WorldCom, Arthur Andersen, and hosts of similarly corrupt corporations who looked the other way when the books were being cooked. Moreover, as I noted earlier, in the Stanford Prison Experiment it was the good guards who never intervened on behalf of the suffering prisoners to get the bad guards to lighten up, thereby implicitly condoning their continually escalating abuse. It was I, who saw these evils and limited only physical violence by the guards as my intervention while allowing psychological violence to fill our dungeon prison. By trapping myself in the conflicting roles of researcher and prison superintendent, I was overwhelmed with their dual demands, which dimmed my focus on the suffering taking place before my eyes. I too was thus guilty of the evil of inaction.

At the level of nation-states, this inaction, when action is required, allows mass murder and genocide to flourish, as it did in Bosnia and Rwanda and has been doing more recently in Darfur. Nations, like individuals, often don’t want to get involved and also deny the seriousness of the threat and the need for immediate action. They also are ready to believe the propaganda of the rulers over the pleas of the victims. In addition, there often are internal pressures on decision makers from those who “do business there” to wait it out.

One of the saddest cases I know of the institutional evil of inaction occurred in 1939, when the U.S. government and its humanitarian president, Franklin D. Roosevelt, refused to allow a ship loaded with Jewish refugees to embark in any port. The SS St. Louis had come from Hamburg, Germany, to Cuba with 937 Jewish refugees escaping the Holocaust. The Cuban government reversed its earlier agreement to accept them. For twelve days these refugees and the ship’s captain tried desperately to get permission from the U.S. government to enter a port in Miami, which was in clear view. Denied permission to enter this or any other port, the ship turned back across the Atlantic. Some refugees were accepted in Britain and other countries, but many finally died in Nazi concentration camps. Imagine being so close to freedom and then dying as a slave laborer.

When incompetence is wedded to indifference and indecision, the outcome is the failure to act when action is essential for survival. The Katrina hurricane disaster in New Orleans (August 2005) is a classic case study in the total failure of multiple, interlocking systems to mobilize the enormous resources at their disposal to prevent the suffering and deaths of many citizens. Despite advance warnings of the impending disaster of the worst kind imaginable, city, state, and national authorities did not engage in the basic preparations needed for evacuation and for the safety of those who could not leave on their own. In addition to the municipal and state authority systems failing to communicate adequately (because of political differences at the top), the response from the Bush administration was nil, too late, and too little when it did come. Incompetent, inexperienced heads of the Federal Emergency Management Association (FEMA) and of the Department of Homeland Security failed to engage the National Guard, Army reserve units, Red Cross, state police, or Air Force personnel to provide food, water, blankets, medicine, and more for the hundreds of thousands of survivors living in squalor for days and nights on end. A year later, much of the city is still in shambles, with entire neighborhoods decimated and deserted, thousands of homes marked for destruction, but little help has been forthcoming. Touring these desolate areas was heartbreaking for me. Critics contend that the systems’ failed response can be traced to class and racial issues, because most of the survivors who could not evacuate were lower-class African Americans. This evil of inaction has been responsible for the deaths, despair, and disillusion of many citizens of New Orleans. Perhaps as many as half of those who did finally leave may never come home again.32

Et tu, Brute?

Each of us has to wonder if, and hope that when the time comes, we will have the courage of our convictions to be a responsive bystander who sounds the alarm when our countrymen and -women are violating their oath of allegiance to country and to humanity. However, we have seen in these chapters that pressures to conform are enormous, to be a team player, not to rock the boat, and not to risk the sanctions against confronting any system. Those forces are often coupled with the top-down power of authority systems to convey expectations indirectly to employees and underlings that unethical and illegal behavior is appropriate under special circumstances—which they define. Many of the recently uncovered scandals at the highest levels of government, in the military, and in business involve the toxic mix of unverbalized authority expectations conveyed to subordinates who want to be accepted in the “Inner Ring,” with the tacit approval of a horde of knowingly silent partners.

“Toxic leaders cast their spell broadly. Most of us claim we abhor them. Yet we frequently follow—or at least tolerate—them, be they our employers, our CEOs, our senators, our clergy, or our teachers. When toxic leaders don’t appear on their own, we often seek them out. On occasion, we even create them by pushing good leaders over the toxic line.” In Jean Lipman-Blumen’s penetrating analysis of the dynamic relationship between leaders and followers in The Allure of Toxic Leaders, we are reminded that recognizing the early signs of toxicity in our leaders can enable us to take preventive medicine, not passively imbibe their seductive poison.33



Throughout history, it has been the inaction of those who could have acted; the indifference of those who should have known better; the silence of the voice of justice when it mattered most; that has made it possible for evil to triumph.


—Haile Selassie, former emperor of Ethiopia





WHY SITUATIONS AND SYSTEMS MATTER


It is a truism in psychology that personality and situations interact to generate behavior; people are always acting within various behavioral contexts. People are both products of their different environments and producers of the environments they encounter.34 Human beings are not passive objects simply buffeted about by environmental contingencies. People usually select the settings they will enter or avoid and can change the setting by their presence and their actions, influence others in that social sphere, and transform environments in myriad ways. More often than not, we are active agents capable of influencing the course of events that our lives take and also of shaping our destinies.35 Moreover, human behavior and human societies are greatly affected by fundamental biological mechanisms as well as by cultural values and practices.36

The individual is the coin of the operating realm in virtually all of the major Western institutions of medicine, education, law, religion, and psychiatry. These institutions collectively help create the myth that individuals are always in control of their behavior, act from free will and rational choice, and are thus personally responsible for any and all of their actions. Unless insane or of diminished capacity, individuals who do wrong should know that they are doing wrong and be punished accordingly. Situational factors are assumed to be little more than a set of minimally relevant extrinsic circumstances. In evaluating various contributors to any behavior of interest, the dispositionalists put the big chips on the Person and the chintzy chips on the Situation. That view seemingly honors the dignity of individuals, who should have the inner strength and will power to resist all temptations and situational inducements. Those of us from the other side of the conceptual tracks believe that such a perspective denies the reality of our human vulnerability. Recognizing such common frailties in the face of the kinds of situational forces we have reviewed in our journey thus far is the first step in shoring up resistance to such detrimental influences and in developing effective strategies that reinforce the resilience of both people and communities.

The situationist approach should encourage us all to share a profound sense of humility when we are trying to understand “unthinkable,” “unimaginable,” “senseless” acts of evil—violence, vandalism, suicidal terrorism, torture, or rape. Instead of immediately embracing the high moral ground that distances us good folks from those bad ones and gives short shrift to analyses of causal factors in that situation, the situational approach gives those “others” the benefit of “attributional charity.” It preaches the lesson that any deed, for good or evil, that any human being has ever done, you and I could also do—given the same situational forces.

Our system of criminal legal justice over-relies on commonsense views held by the general public about what things cause people to commit crimes—usually only motivational and personality determinants. It is time for the legal justice system to take into account the substantial body of evidence from the behavioral sciences about the power of the social context in influencing behavior, criminal actions as well as moral ones. My colleagues Lee Ross and Donna Shestowsky have offered a penetrating analysis of the challenges that contemporary psychology poses to legal theory and practice. Their conclusion is that the legal system might adopt the model of medical science and practice by taking advantage of current research on what goes wrong, as well as right, in how the mind and body work:



The workings of the criminal justice system should not continue to be guided by illusions about cross-situational consistency in behavior, by erroneous notions about the impact of dispositions versus situations in guiding behavior, or by failures to think through the logic of “person by situation” interactions, or even comforting but largely fanciful notions of free will, any more than it should be guided by once common notions about witchcraft or demonic possession.37



Situated Identities

Our personal identities are socially situated. We are where we live, eat, work, and make love. It is possible to predict a wide range of your attitudes and behavior from knowing any combination of “status” factors—your ethnicity, social class, education, and religion and where you live—more accurately than by knowing your personality traits.

Our sense of identity is in large measure conferred on us by others in the ways they treat or mistreat us, recognize or ignore us, praise us or punish us. Some people make us timid and shy; others elicit our sex appeal and dominance. In some groups we are made leaders, while in others we are reduced to being followers. We come to live up to or down to the expectations others have of us. The expectations of others often become self-fulfilling prophecies. Without realizing it, we often behave in ways that confirm the beliefs others have about us. Those subjective beliefs can create new realities for us. We often become who other people think we are, in their eyes and in our behavior.38

Can You Be Judged Sane in an Insane Place?

Situations confer their social identities on us even when it should be obvious that it is not our true personal identity. Recall in the “mock ward” study at Elgin State Mental Hospital (chapter 12) that hospital staff mistreated the “mental patients” on their ward in a variety of ways; however, they were not actually patients but fellow staff members dressed as and playing the role of patients. Similarly, in the Stanford Prison Experiment, everyone knew that the guards were college kids pretending to be guards and that the prisoners were college kids pretending to be prisoners in that mock prison. Did it matter what their real identity was? Not really, as you saw; not after a day or so. They became their situated identities. In addition, I too became The Prison Superintendent in walk, talk, and distorted thought—when I was in that place.

Some situations “essentialize” the roles people are assigned; each person must be what the role demands when he is on that stage set. Image, if you will, that you are a totally normal person who finds yourself hospitalized in a psychiatric ward in a mental hospital. You are there because a hospital admissions officer mistakenly labeled you as “schizophrenic.” That diagnosis was based on the fact that you complained to him about “hearing voices,” nothing more. You believe that you do not deserve to be there and realize that the way to be released is to act as normal and as pleasant as you can. Obviously the staff will soon realize there has been some mistake, you are not a mentally ill patient, and send you back home. Right?

Don’t count on this happening if you were in that setting. You might never be released, according to a fascinating study conducted by another of my Stanford colleagues, David Rosenhan, with the wonderful title “On Being Sane in Insane Places.”39

David and seven associates each went through the same scenario of making an appointment with a different mental hospital admissions officer and complaining of hearing voices or noises, “thuds,” but giving no other unusual symptoms. Each of them was admitted to their local mental hospital, and as soon as they were dressed in the patient’s pajamas and scuffies, they behaved in a pleasant, apparently normal fashion at all times. The big question was how soon the staff would catch on, realize they were really sane, and bid them adieu.

The simple answer in every one of the eight cases, in each of the eight mental wards, was Never! If you are in an Insane Place, you must be an Insane Person because Sane People are not Patients in Insane Asylums—so the situated-identity reasoning went. To be released took a lot of doing, after several weeks, and only with help from colleagues and lawyers. Finally, after the suitably sane Eight were checked out, written across each of their hospital charts was the same final evaluation: “Patient exhibits schizophrenia in remission.” Meaning that, no matter what, the staff still believed that their madness could erupt again some day—so don’t throw away those hospital scuffies!

Assessing Situational Power

At a subjective level, we can say that you have to be embedded within a situation to appreciate its transformative impact on you and others who are similarly situated. Looking in from the outside won’t do. Abstract knowledge of the situation, even when detailed, does not capture the affective tone of the place, its nonverbal features, its emergent norms, or the ego involvement and arousal of being a participant. It is the difference between being an audience member at a game show and being the contestant onstage. It is one reason that experiential learning can have such potent effects, as in the classroom demonstrations by Ms. Elliott and Ron Jones we visited earlier. Do you recall that when forty psychiatrists were asked to predict the outcome of Milgram’s experimental procedure, they vastly underestimated its powerful authority impact? They said that only 1 percent would go all the way up to the maximum shock level of 450 volts. You have seen just how far off they were. They failed to appreciate fully the impact of the social psychological setting in making ordinary people do what they would not do ordinarily.

How important is situational power? A recent review of 100 years of social psychological research compiled the results of more than 25,000 studies including 8 million people.40 This ambitious compilation used the statistical technique of meta-analysis, which is a quantitative summary of findings across a variety of studies that reveals the size and consistency of such empirical results. Across 322 separate meta-analyses, the overall result was that this large body of social psychological research generated substantial effect sizes—that the power of social situations is a reliable and robust effect.

This data set was reanalyzed to focus only on research relevant for understanding the social context variables and principles that are involved when ordinary people engage in torture. The Princeton University researcher Susan Fiske found 1,500 separate effect sizes that revealed the consistent and reliable impact of situational variables on behavior. She concluded, “Social psychological evidence emphasizes the power of the social context, in other words, the power of the interpersonal situation. Social psychology has accumulated a century of knowledge across a variety of studies about how people influence each other for good or ill.”41


LOOKING AHEAD TO APPLES, BARRELS, WHEELERS, AND DEALERS

Now the time has come to collect our analytical gear and move our journey to the far-off foreign land of Iraq to try to understand an extraordinary phenomenon of our times: the digitally documented abuses of Iraqis detained in the prison at Abu Ghraib. Revelations of these violations against humanity moved out from that secret dungeon in Tier 1A, that little shop of horrors, to reverberate around a shocked world. How could this happen? Who was responsible? Why had photographs been taken that implicated the torturers in the act of committing their crimes? These and more questions filled the media for months on end. The president of the United States vowed “to get to the bottom of this.” A host of politicians and pundits knowingly proclaimed that it was all the work of a few “bad apples.” The abusers were nothing more than a band of sadistic “rogue soldiers.”

Our plan is to reexamine what happened and how it happened. We are now adequately prepared to contrast this standard dispositional analysis of identifying the evil perpetrators, the “bad apples,” in the otherwise presumably good barrel, with our search for situational determinants—the nature of that bad barrel. We will also review some of the conclusions from various independent investigations into these abuses that will take us beyond situational factors to implicate the System—military and political—in our explanatory mix.





