CHAPTER 6


Age of Reason


Aristotle defined man as the rational animal, but he had never heard of the Third Pounder.

In the 1980s, the restaurant chain A&W wanted to create a burger that would compete with McDonald’s popular Quarter Pounder. So they created the Third Pounder, which had more beef, was less expensive, and did better in blind taste tests. It was a failure. Focus groups found that the name was the problem. Customers believed that they were being overcharged, assuming that a third of a pound of beef was less than a quarter of a pound of beef since the 3 in ⅓ is smaller than the 4 in ¼.

In some regards, this tale of mathematical dunderheadedness meshes well with the theme of this book so far. I’ve argued that we rely too much on gut feelings and emotional responses to guide our judgments and behaviors. Doing so isn’t a mistake like a mathematical error, but it’s a mistake nonetheless and leads to needless suffering. We are often irrational animals.

At the same time, though, my antiempathy argument presupposes rationality. To say “This sort of judgment is flawed” and to believe it myself and to expect you to believe it assumes a psychological capacity that isn’t subject to the same flaws. The argument, then, is that while we are influenced by gut feelings such as empathy, we are not slaves to them. We can do better, as when we rely on cost-benefit reasoning when deciding whether to go to war, or when we recognize that a stranger’s life matters just as much as the life of our child, even though we love our child and don’t feel any particular warmth toward the stranger.

The idea that human nature has two opposing facets—emotion versus reason, gut feelings versus careful, rational deliberation—is the oldest and most resilient psychological theory of all. It was there in Plato, and it is now the core of the textbook account of cognitive processes, which assumes a dichotomy between “hot” and “cold” mental processes, between an intuitive “System 1” and a deliberative “System 2.” This contrast is nicely captured in the title of Daniel Kahneman’s best-selling book, Thinking, Fast and Slow.

But there are many who now think that the deliberative part—“cold cognition,” System 2—is largely impotent. To argue for the centrality of deliberative reasoning is seen as philosophically naive, psychologically unsophisticated, and even politically suspect.

I recently wrote a short article in the New York Times summarizing research on how hard it is to appreciate what’s going on in the minds of others and arguing that we’re bad at what’s sometimes called “cognitive empathy.” I figured that people would disagree with me on this, and they did, but what surprised me was the reaction to my last sentence, which was “Our efforts should instead be put toward cultivating the ability to step back and apply an objective and fair morality.”

I had thought of this as a reasonable, actually pretty drab, ending, but many commentators seized on it, asking—often with scorn—exactly what this objective and fair morality was supposed to be. Did such a thing even exist? If so, why would one expect it to be a good thing? In a similar vein, a sociology professor once wrote to me and gently told me that my emphasis on reason expressed a particularly Western white male viewpoint. He didn’t use the phrase, but the gist of his polite letter was that I really should check my privilege.

This sort of response really puzzles me. There are a lot of serious arguments regarding the precise sort of morality we should have—moral philosophy is hard—but I think the case for an objective and fair morality is self-evident. Would one prefer a subjective and unfair morality?

I can easily accept that a fan of empathy might argue (contrary to my own position) that empathy really can be fair and objective or that empathy is a necessary part of a fair and objective morality or that, at the very least, empathy is not incompatible with a fair and objective morality. That is, one might believe that the argument running through this book is mistaken and maintain that empathy is overall a good thing for someone who wants to make wise and fair decisions. One might also believe that some partiality makes sense in a personal context—if my child and a stranger were drowning and I could save just one, I’d save my child, and I don’t feel that this is the wrong choice. So the partiality of empathy and other psychological processes might be morally appropriate at least some of the time. These are concerns worth taking seriously, and I’ve tried to respond to them throughout this book.

But it’s hard for me to take seriously the claim that public policy should be made in an unfair and subjective manner (so that, say, it’s right for white politicians to create laws that favor whites over blacks). As for the sociology professor, the idea that rationality is an especially white male Western pursuit is where the extremes of postmodern ideology circle around to meet with the most retrograde views of a barroom bigot. In fact, there is no reason to believe that those who are not male and not white have any special problems with reason. And with regard to the Western part, I would refer the professor to the earlier discussion of how Buddhist theology provides some exceptionally clear insights into why empathy is overrated.

There is a different critique, though, that deserves a lot more attention. This is the concern that regardless of reason’s virtues, we just aren’t any good at it. An undergraduate taking an Introduction to Psychology class is likely to hear in the first lecture that Aristotle’s definition of man as a rational animal is flat wrong. Rather, we are creatures of intuition, of emotion, of the gut. System 1 dominates; System 2 is, well, a distant second. This is said to have been proved by neuroscience, which finds that the emotional parts of the brain have dominion, and supported by the best work in cognitive and social psychology. Contemporary psychologists are often embarrassed about Freud, but they would agree with him about the centrality of the unconscious.

I want to end this book by responding to these sorts of arguments, making the case that we are not as stupid as many scholars think we are. Then, because everyone loves a surprise ending, I’ll finish off by saying some nice things about empathy.

The first attack on reason is from neuroscience. Some believe that the material basis of mental life—the fact that it all reduces to brain processes—is incompatible with a rationalist perspective on human nature.

These are hard times for anyone who wishes to defend Cartesian dualism—the idea that our minds are somehow separate from the workings of the material world, that our thinking is not done in our brain. There is evidence from neuroscience—both regular neuroscience and its sexier children, cognitive neuroscience, affective neuroscience, and social neuroscience—making it abundantly clear that the brain really is the source of mental life. It’s long been known that damage to certain brain areas can impair capacities such as moral judgment and conscious experience, and over the last few decades we’ve developed the technology to create pretty, multicolored fMRI maps that show the material manifestations of thought. Indeed, we’re getting closer to the point where we can tell what someone is thinking—or dreaming!—through neuroimaging. Someone who wanted to hold on to Cartesian dualism would have to do a lot of wiggling around to account for all this.

Some think that the neural basis of thought entails that the only way, or the best way, to study the mind is through looking at brain processes. But this is a mistake. As an analogy, consider that everything your stomach does is ultimately a physical interaction—nobody is a dualist about the tummy. But it would be crazy to try to explain indigestion in terms of particle physics. Similarly, cars are made of atoms, but understanding how a car works requires appealing to higher-level structures such as engines, transmissions, and brakes, which is why physicists will never replace auto mechanics. Or to take a final analogy closer to psychology, you can best understand how a computer works by looking at the program it implements, not the material stuff the computer is made of.

(Also, if it were really true that the best explanations were at the lowest level, then nobody should be doing neuroscience. After all, categories such as “neuron” and “synapse” are themselves quite high-level descriptions of molecules, atoms, quarks, and so on.)

All this means that you can do psychology without studying the brain, even though the mind is the brain. While we’re at it, one can do psychology without studying evolution, even though the brain has evolved, and one can do psychology without studying child development, even though we were all once children. Of course, a good psychologist should be receptive to evidence concerning the brain, evolution, development, and much else. But the study of psychology does not reduce to any of these things. There are many routes to understanding. And in particular, for a lot of what psychologists are interested in, the fact that the mind is the brain just doesn’t matter.

Some would disagree with this. There are scientists and philosophers who maintain that the neural basis of mental life has a particularly radical consequence. It shows that rational deliberation and free choice must be illusions. It shows that, to use the nice phrase coined by Sam Harris, each of us is little more than “a biochemical puppet.”

David Eagleman makes this argument with a series of striking examples. He tells the story of how, in 2000, an otherwise normal Virginia man started to collect child pornography and make sexual overtures toward his prepubescent stepdaughter. He was sentenced to spend time in a rehabilitation center only to be expelled for making lewd advances toward staff members and patients. The next step was prison, but the night before he was to be incarcerated, severe headaches sent him to the hospital, where doctors discovered a large tumor in his brain. After they removed it, his sexual obsessions disappeared. Months later, his interest in child pornography returned, and a scan showed that the tumor had come back. Once again it was removed, and once again his obsessions disappeared.

Other examples of biochemical puppetry abound. A pill used to treat Parkinson’s disease can lead to pathological gambling; date-rape drugs can induce a robotlike compliance; sleeping pills can lead to sleep-binging and sleep-driving.

It might seem that these examples are interesting just because they are so atypical. Most of the time we are not influenced by factors out of our control. As you read this book, your actions are determined by physical law, but unless you have been drugged, have a gun to your head, or are acting under the influence of a behavior-changing brain tumor, reading it is what you have chosen to do. You have reasons for that choice, and you can decide to stop reading if you want.

Eagleman would argue that this distinction is an illusion. Tumor Man is not a bizarre anomaly; he is just a case where the determined nature of behavior is particularly obvious. Speaking more generally about the implications of psychology and neuroscience, Eagleman muses: “It is not clear how much the conscious you—as opposed to the genetic and neural you—gets to do any deciding at all.”

I disagree. I think there are critical differences between the violent acts of a paranoid schizophrenic and a killer for hire, between Tumor Man and your more mundane sexual harasser.

Now Eagleman is surely right that the difference is not that the reflexive cases involve actions performed by the brain while the actual deliberative cases are performed in some other way. It’s all done by the brain. Even some otherwise sophisticated commentators get confused here. One scholar, for instance, discussing serial killers, gives a musical analogy, asking us to think about a person as akin to a conductor and the brain as the orchestra. From this perspective, a bad performance can be explained as the fault of the conductor or the orchestra or both—and it would be unfair to blame the conductor for the failure of the orchestra. Similarly, “If investigation of a miscreant reveals that his brain is broken, it is likely that brain failure was at least partly responsible for his unacceptable behavior.” Blame the brain, not the person! This leads to the excuse that Michael Gazzaniga has dubbed “My brain made me do it.”

I agree with Eagleman that this is the wrong way of thinking. Unless one is a Cartesian dualist (and one really shouldn’t be), the mind is the brain, and there is no such thing as an immaterial conductor using the brain to accomplish his will.

Rather, I’m making the distinction in a different way. My suggestion is that cases like Tumor Man are special because they involve actions that are disengaged from the normal neural mechanisms of conscious deliberation. One way to see this is that when people in these states are brought back to normal—the tumor is removed, the drug wears off—they feel that their desires and actions were alien to them and fell outside the scope of their will. Accordingly, such individuals in their altered states are less responsive to carrots and sticks: Even the threat of imprisonment did not slow down Tumor Man, because the part of his psyche that motivated his sexual behavior was disengaged from the part of his psyche that computed the long-term consequences of his actions.

In the normal course of affairs, there isn’t such a disengagement. We go through a mental process that is typically called “choice,” where we think about the consequences of our actions. There is nothing magical about this. The neural basis of mental life is fully compatible with the existence of conscious deliberation and rational thought—with neural systems that analyze different options, construct logical chains of argument, reason through examples and analogies, and respond to the anticipated consequences of actions.

To see this, imagine two computers. One behaves randomly and erratically; it doesn’t have a rational bone in its mechanical body. The other is a deliberating cost-benefit analyzer. Plainly, both are machines: no souls here. Yet they are as different as can be. The question that remains for the psychologist is: What kind of computer are we? Or better than that—since the answer here is plainly both—to what extent are we irrational things and to what extent are we reasoning things?

This is an empirical question, to be resolved through experiments and observation. Neuroscience research can be relevant here, of course, but the mere fact that we are physical beings doesn’t bear on the issue one way or the other. There is nothing, then, in the claim that we are rational animals that clashes with findings of neuroscience.

So we could be rational. But many psychologists would argue that they have discovered we are not. This is the second attack on reason.

Let’s start with social psychology. There are countless demonstrations of how we are influenced by factors beyond our conscious control. There are studies that purport to show that our judgments and actions are swayed by how hungry we are, what the room we are in smells like, and whether or not there is a flag in the vicinity. Thinking about Superman makes you more likely to volunteer; thinking like a professor makes you better at Trivial Pursuit; being surrounded by the color blue makes you more creative; and sitting on a rickety chair makes you think that other people’s relationships are more fragile.

College students who fill out a questionnaire about their political opinions when standing next to a dispenser of hand sanitizer become, at least for a moment, more politically conservative than those standing next to an empty wall. Those who fill out a survey in a room that smells bad become more disapproving of gay men. Shoppers walking past a good-smelling bakery are more likely to make change for a stranger. Subjects favor job applicants whose résumés are presented to them on heavy clipboards. Supposedly egalitarian white people who are under time pressure are more likely to misidentify a tool as a gun after being shown a photo of a black male face. People are more likely to vote for sales taxes that will fund education when the polling place is in a school.

Many of these are short-term effects, but others are not. There is evidence, for instance, that our names influence our entire lives. Is it a coincidence that the coauthors of an article in the British Journal of Urology are named Dr. Splatt and Dr. Weedon? Or that another urologist is named Dick Finder? Well, probably it is. But there is some statistical evidence that someone named Larry is more likely to become a lawyer, while someone named Gary is more likely to live in Georgia—that is, the first letter of your name exerts subtle influences on your preferences.

What all these examples show is that our thoughts, actions, and desires can be influenced by factors outside our conscious control and so don’t make any rational sense. The sort of chair you’re sitting on has no actual bearing on the sturdiness of anyone’s relationship; and the fact that my first name is Paul shouldn’t have influenced my choice to become a psychologist. So if it turns out that these considerations really do determine what we think and do, it would be devastating for the position that people are rational and deliberative agents.

Many do see it as devastating in this way. Jonathan Haidt captures a certain consensus when he suggests that social psychology research should motivate us to reject the notion that we are in control of our decisions. We should instead think of the conscious self as a lawyer who, when called upon to defend the actions of a client, provides after-the-fact justifications for decisions that have already been made. We are wrong to see rationality as the dog—it’s actually the tail.

Now I respect the social psychology research I just summarized—I’ve even done some of it myself. But I don’t think it shows what many think it shows.

For one thing, many of these findings are fragile. Over the last several years, the field of social psychology has been rocked by failures of replication, where the same experiment is run by a different group of psychologists and fails to find the predicted results. The issue in “repligate” isn’t academic fraud, though that sometimes does happen, and there has been one prominent case where the psychologist Diederik Stapel, who reported exactly these types of counterintuitive findings (messy environments make people discriminate more), turned out to be making up his data. But the real concern has to do with normal scientific practice in this field; there are concerns that the findings have been enhanced by repeated testing and improper statistical analyses.

I once taught a seminar in which participants could satisfy their final requirement by working together on a research project, and a group of students teamed up to extend and explore a fascinating effect involving purity and morality, one that I had written about in a previous book and that raised all sorts of interesting follow-up questions. But despite numerous attempts, they couldn’t replicate the original findings—and they eventually published this failure to replicate. The atypical thing about the story isn’t the failure to replicate, it’s the publication. Usually the project is just abandoned, though sometimes the word gets out in an informal way—in seminars, lab meetings, conferences—that some findings are vaporware (“Oh, nobody can replicate that one”). Many psychologists now have an attitude that if a finding seems really implausible, just wait a while and it will go away.

Not every result from a psychology lab is like this; some are powerful and robust and easy to replicate. But even for these, there is the question of real-world relevance. Statistically significant doesn’t mean actually significant. Just because something has an effect in a controlled situation doesn’t mean that it’s important in real life. Your impression of a résumé might be subtly affected by its being presented to you on a heavy clipboard, and this tells us something about how we draw inferences from physical experience when making social evaluations. Very interesting stuff. But this doesn’t imply that your real-world judgments of job candidates have much to do with what you’re holding when you make those judgments. What will actually matter much more are such boringly relevant considerations as the candidate’s experience and qualifications. Your assessment of gay people might be influenced by a bad smell in the room, and this supports a certain theory of the relationship of disgust and morality—one that I was interested in and the reason my colleagues and I did the study. But it’s hardly clear that this matters much when people interact with one another in the real world.

Sometimes studies really are worth their press releases. Certain effects, even when they’re small, can make a practical difference. And some effects aren’t small at all. An example of a powerful finding is that people eat less when their food is served on small plates. One could lose weight, then, by changing one’s tableware. (There, now this book contains diet tips.)

Still, even the most robust and impressive demonstrations of unconscious or irrational processes do not in the slightest preclude the existence of conscious and rational processes. To think otherwise would be like concluding that because salt adds flavor to food, nothing else does.

This point is often missed, in part because of the sociology of our field. Everybody loves cool findings, so researchers are motivated to explore the strange and unexpected ways in which the mind works. It’s striking to discover that when assigning punishment to criminals, people are influenced by factors they consciously believe to be irrelevant, such as how attractive the criminals are. This finding will get published in the top journals and might make its way into the popular press. But nobody will care if you discover that people’s feelings about punishments are influenced by the severity of the crimes or the criminal’s past record. This is just common sense.

As an example of this, take a study in which psychologists put baseball cards on sale on eBay with photographs depicting them held either by a dark-skinned hand or a light-skinned hand. People were willing to pay about 20 percent less if they were held by the dark hands. This provides, as the authors note, a sharp demonstration of how effects of racial bias show up in a real-world marketplace—an interesting and socially significant finding. But nobody bothers to do a study looking at whether the scarcity of the card or its quality influences how much it sells for, because it’s obvious that people would take into account these perfectly reasonable considerations. Findings of racial bias shouldn’t lead us to forget that more rational processes exist as well, and are deeply important.

What about certain other well-known demonstrations of human irrationality? One example here is that we often ignore base rates when making decisions. Suppose you are being tested for a fatal disease. The particular test you are given will never miss this disease—if you have it, the test will be positive. But it does have a 5 percent false-positive rate, where it says you have the disease when you actually don’t—that is, for every twenty people who are fine, one of them will test positive.

You test positive. Should you worry? People tend to say yes—95 percent accuracy sounds chilling. But actually, the risk depends on the base rate, on how prevalent the disease is in the population. Suppose you know that the disease is present in one out of one thousand people. Now should you worry? What are your odds of having the disease?

People tend to say the odds remain relatively high, but actually they are only about 2 percent. To see this, imagine that 20,000 people are tested: 20 people will actually have the disease and will test positive, but the test will also yield positive results for one-twentieth of the remaining 19,980 who are healthy, which is about 1,000 people. So there will be 1,020 testing positive for the disease, only 20 of whom (about 2 percent) actually have it. It’s simple math when you work it out, but it doesn’t seem natural.

Or take another example: Which one is more common: words ending with ng or words ending with ing? People often say that there are more words ending with ing because these words come to mind more easily. But if you think about it, this has to be wrong because every word that ends with ing also ends with ng, so there have to be at least as many ending with ng. Here, we used how quickly something comes to mind as evidence for how likely it is. This is a good heuristic but one that can lead us astray.

As a final example, imagine that you had to rule on a custody case. Here is the information about the parents:

● Parent A is average in every way—income, health, working hours—and has a reasonably good rapport with the child and a stable social life.

● Parent B has an above-average income, is very close to the child, has an extremely active social life, travels a lot for work, and has minor health problems.

Who should be awarded custody? Who should be denied custody? There may be no right answers to these questions, but one thing is for sure: The specific framing shouldn’t matter. That is, since there are two individuals, and one is awarded and the other denied, they’re really the same question—if you would respond A to the question about who should be awarded custody, you should respond B to the question about who should be denied, and vice versa.

But this isn’t the way people respond: They show a bias toward Parent B in both cases, for both awarding and denying. One explanation for this is that when we get a question, we tend to look for data that is relevant to precisely what is being asked. So when you are asked about awarding custody, you look for considerations that would warrant getting custody and find them in Parent B (income, closeness to child), and when you are asked about denying custody, you look for considerations that would warrant being denied custody and also find them in Parent B (social life, travel, health). And this leads to irrationality—the sort of irrationality that can make a real difference in the real world.

There are many more such demonstrations. The “heuristics and biases” literature in psychology has many famous cases, and unlike some of the social psychology findings, these are robust. They make for great examples in psychology courses and can be used to liven up a conversation, a psychologist’s version of a bar trick.

The existence of these “mind bugs” should be unsurprising. Some amount of irrationality is inevitable given our physical natures. We are finite beings, so there will be some cases that we get wrong. There is an analogy here with visual illusions—vision is another biological system that has evolved to perform a complex job under certain specific circumstances, so tricky scientists can often figure out how to make the system go awry by exposing people to the sorts of images that never occur in the natural world. By the same token, people often get confused when presented with problems that are expressed in terms of statistical probabilities and abstract scenarios; we are better at reasoning about problems that are expressed in terms of frequencies of events, which is just what we would expect based on the circumstances under which our minds have evolved.

A while ago, John Macnamara pointed out that the discovery of these failures of reason reveal two very different things about our minds. Most obviously, they illustrate irrationality, how things go wrong, how we are limited. But they also illustrate how intelligent we are, how we can override our biases. After all, we know that they are mistakes! Upon reflection, we appreciate the relevance of the base rates, we acknowledge that there cannot be more ing words than ng words, and we appreciate that asking about getting custody and being refused custody are really different ways of asking about the same thing. When we hear the story about the Third Pounder, we shake our heads at how dumb people can be, we wonder if the story was made up, we laugh, and we tweet about it. It turns out that every demonstration of our irrationality is also a demonstration of how smart we are, because without our smarts we wouldn’t be able to appreciate that it’s a demonstration of irrationality.

Much of this book has been observing this dynamic. Just as one example among many, yes, we often favor those who are adorable more than those who are ugly. This is a fact about our minds worth knowing. But we can also recognize that this is the wrong way to make moral decisions. It’s this ability to critically assess our limitations—with regard to our social behavior, our reasoning, and our morality—that makes all sorts of things possible.

I’ve been playing defense up to now. I’ve been arguing that evidence and theory from neuroscience, social psychology, and cognitive psychology don’t prove our everyday irrationality. But I haven’t yet made a positive case for our everyday rationality, for the role of reasoning and intelligence in our lives. I’ll do this now.

Think about the most mundane activities that you engage in. When you’re thirsty, you don’t just squirm in your seat at the mercy of unconscious impulses and environmental inputs. You make a plan and execute it. You get up, find a glass, walk to the sink, turn on the tap. This sort of seemingly mundane planning is beyond the capacity of any computer, which is why we don’t yet have robot servants. Making it through a day requires the formulation and initiation of complex, multistage plans, in a world that’s unforgiving of mistakes (try driving your car on an empty tank or going to work without clothes). And the broader project of holding together relationships and managing a job or career requires extraordinary cognitive skills.

If you doubt the power of reason in everyday life, consider those who have less of it. We take care of people with intellectual disabilities and brain damage because they cannot take care of themselves. Think for a minute of how much you would give up so that you or those you love wouldn’t get Alzheimer’s. Think about how reliant such individuals are on the help of others. Even if one is unscathed by neurological problems, there are periods of one’s life where reason is diminished, such as when we are young or when we are drunk. During these periods, individuals are blocked from making significant decisions and rightfully so.

Then there are more subtle gradients of the capacity for reason. Like many other countries, the United States has age restrictions for driving, military service, voting, and drinking, and even higher age restrictions for becoming president, all under the assumption that certain core capacities, including wisdom, take time to mature.

Now some would argue that there is a threshold effect here: Once you pass an average level, you’re fine. This argument is sometimes made by academics, which, as Steven Pinker points out, is rather ironic, given that academics “are obsessed with intelligence. They discuss it endlessly in considering student admissions, in hiring faculty and staff, and especially in their gossip about one another.” Some fields are deeply invested in the concept of genius, revering those special individuals like Albert Einstein and Paul Erdős who are of such great intelligence that everything comes easy to them.

But when it comes to intelligence, there is a law of diminishing returns. The difference between an IQ of 120 and an IQ of 100 (average) is going to be more important than the difference between 140 and 120. And once you pass a certain minimum, other capacities might be more important than intelligence. As David Brooks writes, social psychology “reminds us of the relative importance of emotion over pure reason, social connections over individual choice, character over IQ.” Malcolm Gladwell, for his part, argues for the irrelevance of a high IQ. “If I had magical powers,” he says, “and offered to raise your IQ by 30 points, you’d say yes—right?” But then he goes on to say that you shouldn’t bother, because after you pass a certain basic threshold, IQ really doesn’t make any difference.

Brooks and Gladwell are interested in the determinants of success, and their goal isn’t to bash intelligence but to promote other factors. Brooks focuses on emotional and social skills and Gladwell on the role of contingent factors such as who your family is and where and when you were born. Both are right in assuming these other factors to be significant. To claim that the capacity for reasoning is centrally important to our lives isn’t to claim that it is all that matters.

Still, IQ is critically important at any level. If you had to give a child one psychometric test to predict his or her fate in life, you couldn’t go wrong with an IQ test. Scores on the test are correlated with all sorts of good things, such as steady job performance, staying out of prison, good mental health, being in stable and fulfilling relationships, and even living longer. A long time ago people said things like “IQ tests just measure how good you are at doing IQ tests,” but nobody takes this seriously anymore.

A cynic might object that IQ is meaningful only because our society is obsessed with it. In the United States, after all, getting into a good university depends to a large extent on how well you do on the SAT, which is basically an IQ test. (The correlation between a person’s score on the SAT and on the standard IQ test is very high.) A critic could point out that if we gave slots at top universities to candidates with red hair, we would quickly live in a world in which being a redhead correlated with high income, elevated status, and other positive outcomes . . . and then psychologists would go on about how important it is to have red hair.

But the relationship between IQ and success is hardly arbitrary, and it’s no accident that universities take the tests so seriously. They reveal abilities such as mental speed and the capacity for abstract thought, and it’s not hard to see how these abilities aid intellectual pursuits, how they are good traits to have, and how they can have broader consequences in one’s life.

Indeed, high intelligence is not only related to success; it’s also related to good behavior. Highly intelligent people commit fewer violent crimes (holding other things, such as income, constant), and the difference in IQ between people in prison and those in the outside world is not a subtle one. There is also evidence that highly intelligent people are more cooperative, perhaps because intelligence allows one to appreciate the benefits of long-term coordination and to consider the perspectives of others.

It’s important to emphasize that this is an “on average” thing. Certainly intellectual giftedness is no guarantee of good behavior. Eric Schwitzgebel and Joshua Rust have done a series of impressive (and entertaining) studies finding that professional moral philosophers, the people who think about right and wrong more than just about anyone else, are no better morally than other academics, at least in their everyday lives. They don’t call their mothers more, they don’t give more to charity, they are not more likely to return library books, and so on.

And there really are evil geniuses. When someone has evil on his or her mind, intelligence can be a valuable tool, and a dangerous one. This is a point I’ve made earlier regarding social intelligence—or cognitive empathy, if you want—but one can make it again regarding smarts in general. Intelligence is an instrument that can be used to achieve certain ends. If these ends are positive ones, as they are for most of us, more intelligence can make you a better person. But goodness requires some motivation; you have to care about others and value their fates.

Reason and rationality, then, are not sufficient for being a good and capable person. But my argument is that they are necessary, and on average, the more the better.

It’s not just intelligence, however. I said that if you were curious about what sort of person a child would grow up to be, an intelligence test would be a great measure. But there’s something even better. Self-control can be seen as the purest embodiment of rationality in that it reflects the working of a brain system (embedded in the frontal lobe, the part of the brain that lies behind the forehead) that restrains our impulsive, irrational, or emotive desires. In a series of classic studies, Walter Mischel investigated whether children could refrain from eating one marshmallow now to get two later. He found that the children who waited for two marshmallows did better in school and on their SATs as adolescents and ended up with better mental health, relationship quality, and income as adults. We’ve seen from studies of psychopaths that violent criminal behavior is associated with low self-control; it’s interesting as well that studies of exceptional altruists, such as those who donate their kidneys to strangers, find that they have unusually high self-control.

Steven Pinker has argued that just as a high level of self-control benefits individuals, cultural values that prize self-control are good for a society. Europe, he writes, witnessed a thirtyfold drop in its homicide rate between the medieval and modern periods, and this, he argues, had much to do with the change from a culture of honor to a culture of dignity, which prizes restraint.

Once again, none of this is to deny the importance of traits such as compassion and kindness. We want to nurture these traits in our children and work to establish a culture that prizes and rewards them. But they are not enough. To make the world a better place, we would also want to bless people with more smarts and more self-control. These are central to leading a successful and happy life—and a good and moral one.

This is not a novel insight. It’s been pages since I cited Adam Smith’s Theory of Moral Sentiments, so consider a section where Smith discusses the qualities that are most useful to a person. There are two, and neither of them directly has to do with feelings or sentiments, moral or otherwise. Those are “superior reason and understanding” and “self-command.”

The first is important because it enables us to appreciate the consequences of our actions in the future: You can’t act to make the world better if you aren’t smart enough to know which action will achieve that goal. The second—which we would now call self-control—is critical as well, as it allows us to abstain from our immediate appetites to focus on long-term consequences.

There are areas of life where we certainly seem stupid. Take politics. Social psychologists often use political irrationality as an illustration of our broader psychological limitations.

The case for political irrationality seems pretty strong. For one thing, politics is associated with certain weird factual beliefs, such as the view that Barack Obama was born in Kenya or that George Bush was directly complicit in the 9/11 attacks. My wife recently saw a Facebook post by a high school friend, warning that the president was going to remove “In God We Trust” from all paper money, a claim originally posted in a satirical online magazine, which was uncritically accepted by this person and many of her friends. This is not an isolated incident.

Rationality in political domains often does seem to be in short supply. One striking example of this is a series of studies run by Geoffrey Cohen. Subjects were told about a proposed welfare program, which was described as being endorsed by either Republicans or Democrats, and were asked whether they approved of it. Some subjects were told about an extremely generous program, others about an extremely stingy program, but this made little difference. What mattered was which party was said to support the program: Democrats approved of the Democratic program; Republicans, the Republican program. Subjects were unaware of their bias: When asked to justify their decision, they insisted that party considerations were irrelevant; they felt they were responding to the program’s objective merits.

Other studies have found that when people are called upon to justify their political positions, even those that they feel strongly about, many are flummoxed. For instance, many people who claim to believe deeply in cap and trade or a flat tax have little idea what these policies actually entail.

This sure does look stupid. But there is another way to think about these findings. Yes, certain political attitudes and beliefs might not be the products of careful reasoning, but perhaps they’re not supposed to be. Think about sports fans. When people root for the Red Sox or the Yankees, it’s not an exercise in rational deliberation, nor should it be. Rather, people are expressing loyalty to their team. Perhaps people’s views on health care, global warming, and the like should be viewed in a similar light, not as articulated conclusions, but rather as “Yay, team!” and “Boo, the other guys!” To complain that someone’s views on global warming aren’t grounded in the facts, then, is to miss the point. It would be like complaining that a Red Sox fan’s love of her team doesn’t reflect a realistic appraisal of the Sox’s performance in the last few seasons.

Political views share an interesting property with views about sports teams—they don’t really matter. If I have the wrong theory of how to make scrambled eggs, they will come out too dry; if I have the wrong everyday morality, I will hurt those I love. But suppose I think that the leader of the opposing party has sex with pigs, or has thoroughly botched the arms deal with Iran. Unless I’m a member of a tiny powerful community, my beliefs have no effect on the world. This is certainly true as well for my views about the flat tax, global warming, and evolution. They don’t have to be grounded in truth, because the truth value doesn’t have any effect on my life.

I am unhappy making this argument, because my own moral commitments lean me toward the perspective that it’s important to try to be right about issues even if they don’t matter in a practical sense. I would be horrified if one of my sons thought that our ancestors rode dinosaurs, even though I can’t think of a view that matters less for everyday life. I would feel similarly if he supported ridiculous claims as true just because they fit his political ideology. We should try to believe true things.

But that’s just me. Others see things differently. My point here is just that the failure of people to attend to data in the political domain does not reflect a limitation in their capacity for reason. It reflects how most people make sense of politics. They don’t care about truth because, for them, it’s not really about truth.

We do much better, after all, when the stakes become high, when being rational really matters. If our thought processes in the political realm reflected how our minds generally work, we wouldn’t even make it out of bed each morning. So if you’re curious about people’s capacity for reasoning, don’t look at cases where being right doesn’t matter and where it’s all about affiliation. Rather, look at how people cope in everyday life. Look at the discussions that adults have over whether to buy a house, what jobs to take, where to send their kids to school, what they should do about an elderly parent. Look at the social negotiations that occur among friends deciding where to go for dinner, planning a hike, figuring out how to help someone who just had a baby. Or even look at a different sort of politics— the type of politics where individuals might actually make a difference, such as a town hall meeting where people discuss zoning regulations and where to put a stop sign.

My own experience is that the level of rational discourse here is high. People know that they are involved in real decision processes, so they work to exercise their rational capacities: They make arguments, express ideas, and are receptive to the ideas of others. They sometimes even change their minds.

Let’s consider again the effective altruists. Peter Singer points out that when some of these altruists talk about why they act as they do, they use language more suggestive of rational thought than of strong feelings or emotional impulse. We saw that Zell Kravinsky, for example, said that the reason many people didn’t understand his desire to donate a kidney is that “they don’t understand math.” Another effective altruist wrote, “Numbers turned me into an altruist. When I learned that I could spend my exorbitant monthly gym membership (I don’t even want to tell you how much it cost) on curing blindness instead, the only thought I had was, ‘Why haven’t I been doing this all along?’ ”

The effective altruists are unusual people, but the capacity to engage in such reasoning exists in all of us. Social psychologists are correct that some moral intuitions are impossible to justify. But as I argue in my book Just Babies, these are the exceptions. People are not at a loss when asked why drunk driving is wrong, or why a company shouldn’t pay a woman less than a man for the same job, or why you should hold the door open for someone on crutches. We can easily justify these views by referring to fundamental concerns about harm, equity, and kindness.

Moreover, when faced with more difficult problems, we think about them—we mull, deliberate, argue. This is manifest in the discussions we have with friends and families over the moral issues that arise in everyday life. Is it right to cross a picket line? Should I give money to the homeless man in front of the bookstore? Was it appropriate for our friend to start dating so soon after her husband died? What do I do about the colleague who is apparently not intending to pay me back the money she owes me?

I’ve argued elsewhere that this capacity for moral reason has had dramatic consequences. As scholars like Steven Pinker, Robert Wright, and Peter Singer have noted, our moral circle has expanded over history: Our attitudes about the rights of women, homosexuals, and racial minorities have all shifted toward inclusiveness. Most recently, there has been a profound difference in how people in my own community treat trans individuals—we are watching moral progress happen in real time.

But this is not because our hearts have opened up over the course of history. We are not more empathic than our great-grandparents. We really don’t think of humanity as our family and we never will. Rather, our concern for others reflects a more abstract appreciation that regardless of our feelings, their lives have the same value as the lives of those we love. Steven Pinker put this nicely:

The Old Testament tells us to love our neighbors, the New Testament to love our enemies. The moral rationale seems to be: Love your neighbors and enemies; that way you won’t kill them. But frankly, I don’t love my neighbors, to say nothing of my enemies. Better, then, is the following idea: Don’t kill your neighbors or enemies, even if you don’t love them. . . . What really has expanded is not so much a circle of empathy as a circle of rights—a commitment that other living things, no matter how distant or dissimilar, be safe from harm and exploitation.

And Adam Smith put it even better. He asks why we would ever care about strangers when our own affairs feel so much more important, and his answer is this: “It is not the soft power of humanity, it is not that feeble spark of benevolence which Nature has lighted up in the human heart, that is thus capable of counteracting the strongest impulses of self-love. It is a stronger power, a more forcible motive, which exerts itself upon such occasions. It is reason, principle, conscience, the inhabitant of the breast, the man within, the great judge and arbiter of our conduct.”

As this book comes to an end, I worry that I have given the impression that I’m against empathy.

Well, I am—but only in the moral domain. And even here I don’t deny that empathy can sometimes have good results. As I conceded from the start, empathy can motivate kindness to individuals that makes the world better. Even when empathy motivates violence and war, it might be a good thing—there are worse things than violence and war; sometimes the reprisal motivated by empathy makes the world a better place. The concern about empathy is not that its consequences are always bad, then. It’s that its negatives outweigh its positives—and that there are better alternatives.

Also, there is more to life than morality.

Empathy can be an immense source of pleasure. Most obviously, we feel joy at the joy of others. I’ve noted elsewhere that here lies one of the joys of having children: You can have experiences that you’ve long become used to—eating ice cream, watching Hitchcock movies, riding a roller coaster—for the first time all over again. Empathy amplifies the pleasures of friendship and community, of sports and games, and of sex and romance. And it’s not just empathy for positive feelings that engages us. There is a fascination we have with seeing the world through the eyes of another, even when the other is suffering. Most of us are intensely curious about the lives of other people and find the act of trying to simulate these lives to be engaging and transformative.

There is much to be said about our appetite for empathic engagement and about the appeal of stories more generally. But that would be a topic for another book.
